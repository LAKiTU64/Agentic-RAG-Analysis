#!/usr/bin/env python3
"""
NVIDIA æ€§èƒ½åˆ†æé›†æˆå·¥å…·
å…ˆç”¨ nsys è¯†åˆ«çƒ­ç‚¹kernelsï¼Œå†ç”¨ ncu æ·±åº¦åˆ†æ

å·¥ä½œæµç¨‹ï¼š
1. nsys profile -> è·å–å…¨å±€æ€§èƒ½overview  
2. æå–çƒ­ç‚¹kernelåç§°
3. ncu profile -> é’ˆå¯¹çƒ­ç‚¹kernelsæ·±åº¦åˆ†æ
4. ç»¼åˆåˆ†ææŠ¥å‘Š

ä½œè€…: AIåŠ©æ‰‹
ç‰ˆæœ¬: 1.0
"""

import os
import sys
import json
import subprocess
import argparse
import re
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime

# å¯¼å…¥æˆ‘ä»¬çš„åˆ†æå·¥å…·
sys.path.append(str(Path(__file__).parent))
from nsys_parser import NsysParser, NsysAnalyzer
from ncu_parser import NCUParser, NCUAnalyzer, NCUVisualizer, NCUReporter

class NSysToNCUAnalyzer:
    """é›†æˆ nsys å’Œ ncu çš„åˆ†æå·¥å…·"""
    
    def __init__(self, output_dir: str = "integrated_analysis"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.hot_kernels = []
        self.nsys_stats = {}
        self.ncu_results = {}
        
    def step1_nsys_analysis(self, target_command: List[str], profile_name: str = "overview") -> str:
        """ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨nsysè¿›è¡Œå…¨å±€æ€§èƒ½åˆ†æ"""
        
        nsys_profile = self.output_dir / f"{profile_name}.nsys-rep"
        
        # æ„å»ºnsyså‘½ä»¤
        nsys_cmd = [
            'nsys', 'profile',
            '-o', str(nsys_profile.with_suffix('')),  # nsysä¼šè‡ªåŠ¨æ·»åŠ .nsys-rep
            '-t', 'cuda,nvtx,osrt',
            '--cuda-memory-usage=true',
            '--force-overwrite=true'
        ] + target_command
        
        print("ğŸš€ æ­¥éª¤1: è¿è¡Œnsyså…¨å±€æ€§èƒ½åˆ†æ...")
        print(f"å‘½ä»¤: {' '.join(nsys_cmd)}")
        
        try:
            result = subprocess.run(nsys_cmd, capture_output=True, text=True, check=True)
            print(f"âœ… nsysåˆ†æå®Œæˆ: {nsys_profile}")
            return str(nsys_profile)
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ nsysåˆ†æå¤±è´¥: {e.stderr}")
            raise
    
    def step2_extract_hot_kernels(self, nsys_file: str, 
                                  top_k: int = 10, 
                                  min_duration_ms: float = 0.1) -> List[Dict]:
        """ç¬¬äºŒæ­¥ï¼šä»nsysç»“æœä¸­æå–çƒ­ç‚¹kernels"""
        
        print("ğŸ” æ­¥éª¤2: ä»nsysç»“æœæå–çƒ­ç‚¹kernels...")
        
        # ä½¿ç”¨æˆ‘ä»¬çš„nsysè§£æå™¨
        parser = NsysParser(nsys_file)
        parser.parse()
        
        if not parser.kernels:
            print("âš ï¸  æœªå‘ç°CUDA kernels")
            return []
        
        # åˆ†ækernels
        analyzer = NsysAnalyzer(parser)
        self.nsys_stats = analyzer.analyze()
        
        # æå–çƒ­ç‚¹kernels
        hot_kernels = []
        
        # æ–¹æ³•1: æŒ‰æ‰§è¡Œæ—¶é—´æ’åº
        kernels_by_time = sorted(parser.kernels, 
                               key=lambda k: k.duration, reverse=True)
        
        # æ–¹æ³•2: æŒ‰è°ƒç”¨æ¬¡æ•°ç»Ÿè®¡
        kernel_stats = {}
        for kernel in parser.kernels:
            name = kernel.name
            if name not in kernel_stats:
                kernel_stats[name] = {
                    'count': 0,
                    'total_time': 0,
                    'max_time': 0,
                    'avg_time': 0
                }
            
            kernel_stats[name]['count'] += 1
            kernel_stats[name]['total_time'] += kernel.duration * 1000  # è½¬ä¸ºms
            kernel_stats[name]['max_time'] = max(kernel_stats[name]['max_time'], 
                                                kernel.duration * 1000)
        
        # è®¡ç®—å¹³å‡æ—¶é—´
        for name, stats in kernel_stats.items():
            stats['avg_time'] = stats['total_time'] / stats['count']
        
        # æŒ‰æ€»æ‰§è¡Œæ—¶é—´æ’åºï¼Œæå–top-k
        sorted_kernels = sorted(kernel_stats.items(), 
                              key=lambda x: x[1]['total_time'], reverse=True)
        
        for kernel_name, stats in sorted_kernels[:top_k]:
            # è¿‡æ»¤æ‰æ‰§è¡Œæ—¶é—´å¤ªçŸ­çš„kernels
            if stats['avg_time'] >= min_duration_ms:
                hot_kernels.append({
                    'name': kernel_name,
                    'total_time_ms': stats['total_time'],
                    'avg_time_ms': stats['avg_time'],
                    'count': stats['count'],
                    'max_time_ms': stats['max_time']
                })
        
        self.hot_kernels = hot_kernels
        
        print(f"ğŸ”¥ è¯†åˆ«åˆ° {len(hot_kernels)} ä¸ªçƒ­ç‚¹kernels:")
        for i, kernel in enumerate(hot_kernels[:5], 1):
            print(f"  {i}. {kernel['name'][:60]}... "
                  f"(æ€»è®¡: {kernel['total_time_ms']:.2f}ms, "
                  f"å¹³å‡: {kernel['avg_time_ms']:.3f}ms, "
                  f"è°ƒç”¨: {kernel['count']}æ¬¡)")
        
        # ä¿å­˜çƒ­ç‚¹kernelsåˆ°æ–‡ä»¶
        hot_kernels_file = self.output_dir / "hot_kernels.json"
        with open(hot_kernels_file, 'w', encoding='utf-8') as f:
            json.dump(hot_kernels, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ çƒ­ç‚¹kernelsåˆ—è¡¨å·²ä¿å­˜: {hot_kernels_file}")
        return hot_kernels
    
    def step3_ncu_targeted_analysis(self, target_command: List[str], 
                                   kernels_to_analyze: List[Dict],
                                   max_kernels: int = 5) -> List[str]:
        """ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨ncuå¯¹çƒ­ç‚¹kernelsè¿›è¡Œæ·±åº¦åˆ†æ"""
        
        print("âš¡ æ­¥éª¤3: ä½¿ç”¨ncuæ·±åº¦åˆ†æçƒ­ç‚¹kernels...")
        
        ncu_results = []
        
        # é™åˆ¶åˆ†æçš„kernelæ•°é‡ï¼ˆncuåˆ†æå¾ˆè€—æ—¶ï¼‰
        kernels_to_analyze = kernels_to_analyze[:max_kernels]
        
        for i, kernel_info in enumerate(kernels_to_analyze):
            kernel_name = kernel_info['name']
            
            # æ¸…ç†kernelåç§°ï¼Œç”¨äºæ–‡ä»¶å
            safe_name = re.sub(r'[^\w\-_]', '_', kernel_name)[:50]
            ncu_profile = self.output_dir / f"ncu_kernel_{i}_{safe_name}"
            
            # æ„å»ºncuå‘½ä»¤
            ncu_cmd = [
                'ncu',
                '--kernel-name', kernel_name,
                '--set', 'full',  # æ”¶é›†å®Œæ•´æŒ‡æ ‡é›†
                '-o', str(ncu_profile),
                '--force-overwrite'
            ] + target_command
            
            print(f"ğŸ¯ æ­£åœ¨åˆ†ækernel {i+1}/{len(kernels_to_analyze)}: {kernel_name[:60]}...")
            
            try:
                # è¿è¡Œncu
                result = subprocess.run(ncu_cmd, capture_output=True, text=True, 
                                      timeout=300, check=True)  # 5åˆ†é’Ÿè¶…æ—¶
                
                ncu_file = str(ncu_profile) + ".ncu-rep"
                if Path(ncu_file).exists():
                    print(f"âœ… kernelåˆ†æå®Œæˆ: {ncu_file}")
                    ncu_results.append(ncu_file)
                    
                    # ç«‹å³å¯¼å‡ºä¸ºCSVä»¥ä¾¿åˆ†æ
                    self._export_ncu_to_csv(ncu_file)
                else:
                    print(f"âš ï¸  NCUæ–‡ä»¶æœªç”Ÿæˆ: {ncu_file}")
                    
            except subprocess.TimeoutExpired:
                print(f"â° kernelåˆ†æè¶…æ—¶: {kernel_name[:60]}")
            except subprocess.CalledProcessError as e:
                print(f"âŒ kernelåˆ†æå¤±è´¥: {kernel_name[:60]} - {e.stderr}")
            except Exception as e:
                print(f"âŒ æ„å¤–é”™è¯¯: {e}")
        
        return ncu_results
    
    def _export_ncu_to_csv(self, ncu_file: str) -> Optional[str]:
        """å¯¼å‡ºncuç»“æœä¸ºCSVæ ¼å¼"""
        csv_file = ncu_file.replace('.ncu-rep', '.csv')
        
        export_cmd = [
            'ncu', '--csv',
            '--log-file', csv_file,
            '--import', ncu_file
        ]
        
        try:
            subprocess.run(export_cmd, capture_output=True, text=True, check=True)
            if Path(csv_file).exists():
                return csv_file
        except:
            pass
        
        return None
    
    def step4_comprehensive_analysis(self, ncu_files: List[str]) -> Dict:
        """ç¬¬å››æ­¥ï¼šç»¼åˆåˆ†ænsyså’Œncuç»“æœ"""
        
        print("ğŸ“Š æ­¥éª¤4: ç»¼åˆåˆ†æç»“æœ...")
        
        comprehensive_results = {
            'timestamp': datetime.now().isoformat(),
            'nsys_overview': self.nsys_stats,
            'hot_kernels_count': len(self.hot_kernels),
            'ncu_detailed_analysis': {}
        }
        
        # åˆ†ææ¯ä¸ªncuç»“æœ
        for ncu_file in ncu_files:
            csv_file = ncu_file.replace('.ncu-rep', '.csv')
            
            if Path(csv_file).exists():
                try:
                    # ä½¿ç”¨æˆ‘ä»¬çš„ncuåˆ†æå™¨
                    parser = NCUParser(csv_file)
                    parser.parse()
                    
                    analyzer = NCUAnalyzer(parser)
                    stats = analyzer.analyze()
                    
                    kernel_name = Path(ncu_file).stem
                    comprehensive_results['ncu_detailed_analysis'][kernel_name] = {
                        'kernels_analyzed': len(parser.kernels),
                        'bottlenecks_found': len(analyzer.bottlenecks),
                        'gpu_utilization': stats.get('gpu_utilization', {}),
                        'memory_analysis': stats.get('memory_analysis', {}),
                        'bottleneck_summary': [
                            {
                                'type': b.type,
                                'severity': b.severity,
                                'description': b.description
                            }
                            for b in analyzer.bottlenecks[:3]  # åªå–å‰3ä¸ª
                        ]
                    }
                    
                    # ç”Ÿæˆè¯¦ç»†çš„å¯è§†åŒ–æŠ¥å‘Š
                    visualizer = NCUVisualizer(parser, analyzer)
                    vis_output_dir = self.output_dir / f"visualization_{kernel_name}"
                    visualizer.output_dir = vis_output_dir
                    vis_output_dir.mkdir(exist_ok=True)
                    visualizer.create_visualizations()
                    
                except Exception as e:
                    print(f"âš ï¸  åˆ†æ {ncu_file} å¤±è´¥: {e}")
        
        # ä¿å­˜ç»¼åˆåˆ†æç»“æœ
        results_file = self.output_dir / "comprehensive_analysis.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“‹ ç»¼åˆåˆ†æç»“æœå·²ä¿å­˜: {results_file}")
        return comprehensive_results
    
    def generate_final_report(self, comprehensive_results: Dict) -> str:
        """ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š"""
        
        report_file = self.output_dir / "integrated_performance_report.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# é›†æˆæ€§èƒ½åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # nsysæ¦‚è§ˆ
            f.write("## ğŸ” Nsys å…¨å±€æ€§èƒ½æ¦‚è§ˆ\n\n")
            nsys_overview = comprehensive_results.get('nsys_overview', {})
            
            if 'kernel_analysis' in nsys_overview:
                kernel_stats = nsys_overview['kernel_analysis']
                f.write(f"- **æ€»kernelsæ•°é‡**: {kernel_stats.get('total_kernels', 0)}\n")
                f.write(f"- **æ€»kernelæ‰§è¡Œæ—¶é—´**: {kernel_stats.get('total_kernel_time', 0):.2f} ms\n")
                f.write(f"- **å¹³å‡kernelæ‰§è¡Œæ—¶é—´**: {kernel_stats.get('avg_kernel_time', 0):.3f} ms\n")
            
            # çƒ­ç‚¹kernels
            f.write(f"\n## ğŸ”¥ è¯†åˆ«çš„çƒ­ç‚¹Kernels ({comprehensive_results.get('hot_kernels_count', 0)}ä¸ª)\n\n")
            for i, kernel in enumerate(self.hot_kernels[:10], 1):
                f.write(f"{i}. **{kernel['name'][:80]}**\n")
                f.write(f"   - æ€»æ‰§è¡Œæ—¶é—´: {kernel['total_time_ms']:.2f} ms\n")
                f.write(f"   - å¹³å‡æ‰§è¡Œæ—¶é—´: {kernel['avg_time_ms']:.3f} ms\n") 
                f.write(f"   - è°ƒç”¨æ¬¡æ•°: {kernel['count']}\n\n")
            
            # ncuæ·±åº¦åˆ†æ
            f.write("## âš¡ NCU æ·±åº¦åˆ†æç»“æœ\n\n")
            ncu_analysis = comprehensive_results.get('ncu_detailed_analysis', {})
            
            for kernel_name, analysis in ncu_analysis.items():
                f.write(f"### {kernel_name}\n\n")
                
                gpu_util = analysis.get('gpu_utilization', {})
                if gpu_util:
                    f.write(f"- **å¹³å‡SMæ•ˆç‡**: {gpu_util.get('average_sm_efficiency', 0):.1f}%\n")
                
                memory_analysis = analysis.get('memory_analysis', {})
                if 'bandwidth_stats' in memory_analysis:
                    bw = memory_analysis['bandwidth_stats']
                    f.write(f"- **å¹³å‡å†…å­˜å¸¦å®½**: {bw.get('average_bandwidth', 0):.1f} GB/s\n")
                
                # ç“¶é¢ˆåˆ†æ
                bottlenecks = analysis.get('bottleneck_summary', [])
                if bottlenecks:
                    f.write(f"- **ä¸»è¦æ€§èƒ½ç“¶é¢ˆ**:\n")
                    for bottleneck in bottlenecks:
                        f.write(f"  - {bottleneck['description']} ({bottleneck['severity']})\n")
                
                f.write("\n")
            
            # ä¼˜åŒ–å»ºè®®
            f.write("## ğŸ’¡ ä¼˜åŒ–å»ºè®®\n\n")
            f.write("### åŸºäºnsysåˆ†æ:\n")
            f.write("- å…³æ³¨ä¸Šè¿°çƒ­ç‚¹kernelsçš„ä¼˜åŒ–\n")
            f.write("- æ£€æŸ¥kernelè°ƒç”¨çš„æ—¶é—´é—´éš™ï¼Œä¼˜åŒ–overlap\n\n")
            
            f.write("### åŸºäºncuåˆ†æ:\n")
            f.write("- å¯¹SMæ•ˆç‡ä½çš„kernelsè¿›è¡Œç®—æ³•ä¼˜åŒ–\n")
            f.write("- å¯¹å†…å­˜å¸¦å®½ä½çš„kernelsä¼˜åŒ–è®¿é—®æ¨¡å¼\n")
            f.write("- æ ¹æ®å…·ä½“ç“¶é¢ˆç±»å‹é‡‡å–é’ˆå¯¹æ€§ä¼˜åŒ–æªæ–½\n")
        
        print(f"ğŸ“„ æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return str(report_file)

def create_sglang_analysis_workflow():
    """åˆ›å»ºSGlangä¸“ç”¨çš„åˆ†æå·¥ä½œæµ"""
    
    def run_sglang_integrated_analysis(model_path: str, 
                                      batch_size: int = 8,
                                      input_len: int = 512, 
                                      output_len: int = 64):
        """è¿è¡ŒSGlangçš„é›†æˆåˆ†æ"""
        
        # æ„å»ºSGlangå‘½ä»¤
        sglang_cmd = [
            'python', '-m', 'sglang.bench_one_batch',
            '--model-path', model_path,
            '--batch-size', str(batch_size),
            '--input-len', str(input_len),
            '--output-len', str(output_len),
            '--load-format', 'dummy'
        ]
        
        # åˆ›å»ºåˆ†æå™¨
        analyzer = NSysToNCUAnalyzer(f"sglang_analysis_b{batch_size}_i{input_len}_o{output_len}")
        
        # æ­¥éª¤1: nsyså…¨å±€åˆ†æ
        nsys_file = analyzer.step1_nsys_analysis(sglang_cmd, "sglang_overview")
        
        # æ­¥éª¤2: æå–çƒ­ç‚¹kernels
        hot_kernels = analyzer.step2_extract_hot_kernels(nsys_file, top_k=8)
        
        if not hot_kernels:
            print("âŒ æœªå‘ç°çƒ­ç‚¹kernelsï¼Œåˆ†æç»ˆæ­¢")
            return
        
        # æ­¥éª¤3: ncuæ·±åº¦åˆ†æï¼ˆé™åˆ¶åˆ†ææ•°é‡ï¼‰
        ncu_files = analyzer.step3_ncu_targeted_analysis(sglang_cmd, hot_kernels, max_kernels=3)
        
        # æ­¥éª¤4: ç»¼åˆåˆ†æ
        results = analyzer.step4_comprehensive_analysis(ncu_files)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        report_file = analyzer.generate_final_report(results)
        
        print(f"\nğŸ‰ SGlangé›†æˆåˆ†æå®Œæˆ!")
        print(f"ğŸ“ åˆ†æç»“æœç›®å½•: {analyzer.output_dir}")
        print(f"ğŸ“„ åˆ†ææŠ¥å‘Š: {report_file}")
        
        return analyzer.output_dir
    
    return run_sglang_integrated_analysis

def main():
    parser = argparse.ArgumentParser(description='é›†æˆ nsys å’Œ ncu çš„æ€§èƒ½åˆ†æå·¥å…·')
    parser.add_argument('command', nargs='+', help='è¦åˆ†æçš„å‘½ä»¤')
    parser.add_argument('--output-dir', default='integrated_analysis', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--top-k', type=int, default=10, help='æå–çš„çƒ­ç‚¹kernelæ•°é‡')
    parser.add_argument('--max-ncu-kernels', type=int, default=5, help='ncuåˆ†æçš„æœ€å¤§kernelæ•°é‡')
    parser.add_argument('--min-duration', type=float, default=0.1, help='æœ€å°kernelæ‰§è¡Œæ—¶é—´(ms)')
    
    # SGlangç‰¹æ®Šå‚æ•°
    parser.add_argument('--sglang-model', type=str, help='SGlangæ¨¡å‹è·¯å¾„')
    parser.add_argument('--sglang-batch', type=int, default=8, help='SGlangæ‰¹æ¬¡å¤§å°')
    parser.add_argument('--sglang-input-len', type=int, default=512, help='SGlangè¾“å…¥é•¿åº¦')
    parser.add_argument('--sglang-output-len', type=int, default=64, help='SGlangè¾“å‡ºé•¿åº¦')
    
    args = parser.parse_args()
    
    try:
        if args.sglang_model:
            # SGlangä¸“ç”¨åˆ†æ
            sglang_workflow = create_sglang_analysis_workflow()
            sglang_workflow(
                args.sglang_model,
                args.sglang_batch,
                args.sglang_input_len, 
                args.sglang_output_len
            )
        else:
            # é€šç”¨åˆ†æ
            analyzer = NSysToNCUAnalyzer(args.output_dir)
            
            # æ­¥éª¤1-4
            nsys_file = analyzer.step1_nsys_analysis(args.command)
            hot_kernels = analyzer.step2_extract_hot_kernels(nsys_file, args.top_k, args.min_duration)
            
            if hot_kernels:
                ncu_files = analyzer.step3_ncu_targeted_analysis(args.command, hot_kernels, args.max_ncu_kernels)
                results = analyzer.step4_comprehensive_analysis(ncu_files)
                analyzer.generate_final_report(results)
            else:
                print("âŒ æœªå‘ç°ç¬¦åˆæ¡ä»¶çš„çƒ­ç‚¹kernels")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­åˆ†æ")
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

