#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Agentæ ¸å¿ƒæ¨¡å— - é›†æˆNSyså’ŒNCUæ€§èƒ½åˆ†æ
"""

import re
import os
import sys
import json
import asyncio
import subprocess
from functools import partial
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime
from math import gcd, lcm
from fractions import Fraction

# å¯¼å…¥åˆ†æå·¥å…·
from utils.nsys_to_ncu_analyzer import NSysToNCUAnalyzer, create_sglang_analysis_workflow
from .offline_llm import get_offline_qwen_client

try:
    from .utils.roofline_estimator import compute_roofline  # type: ignore
except Exception:
    try:
        from utils.roofline_estimator import compute_roofline  # type: ignore
    except Exception:
        compute_roofline = None  # type: ignore

OFFLINE_QWEN_PATH = Path(os.getenv("QWEN_LOCAL_MODEL_PATH", "/workspace/Qwen3-32B"))

class AIAgent:
    """AI Agentæ ¸å¿ƒç±» - è‡ªåŠ¨åŒ–æ€§èƒ½åˆ†æ"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.sglang_path = Path(config.get('sglang_path', 'SGlang'))
        self.models_path = Path(config.get('models_path', 'models'))
        self.model_mappings = config.get('model_mappings', {})
        self.results_dir = Path(config.get('output', {}).get('results_dir', 'analysis_results'))
        self.results_dir.mkdir(exist_ok=True)
        
        # åˆ†æå·¥å…·é…ç½®
        self.profiling_config = config.get('profiling_tools', {})
        self.analysis_defaults = config.get('analysis_defaults', {})

        # ç¼“å­˜æœ€è¿‘ä¸€æ¬¡åˆ†æçš„å…³é”®ä¿¡æ¯ï¼Œä¾¿äºå¯¹å¤–æ¥å£å¤ç”¨
        self.last_analysis_dir: Optional[str] = None
        self.last_analysis_dirs: List[str] = []
        self.last_analysis_reports: List[str] = []
        self.last_analysis_table: Optional[str] = None
        self.last_analysis_suggestions: Optional[str] = None
        self.last_roofline_estimate: Optional[Dict[str, Any]] = None
        
    async def process_message(self, message: str) -> str:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶æ‰§è¡Œåˆ†æ"""
        
        # æå–æ¨¡å‹åç§°
        model_name = self._extract_model_name(message)
        
        # æå–åˆ†æç±»å‹
        analysis_type = self._extract_analysis_type(message)
        
        # æå–å‚æ•°
        params = self._extract_parameters(message)
        
        # å¦‚æœæ²¡æœ‰æä¾›å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not params.get('batch_size'):
            params['batch_size'] = self.analysis_defaults.get('batch_size', [1])
        if not params.get('input_len'):
            params['input_len'] = self.analysis_defaults.get('input_len', [128])
        if not params.get('output_len'):
            params['output_len'] = self.analysis_defaults.get('output_len', [1])
        
        # ç”Ÿæˆåˆå§‹å“åº”
        response = f"""âœ… **å·²è§£ææ‚¨çš„è¯·æ±‚**

ğŸ¤– **æ¨¡å‹**: {model_name or 'æœªæŒ‡å®š'}
ğŸ”¬ **åˆ†æç±»å‹**: {analysis_type}
ğŸ“Š **å‚æ•°**:
  â€¢ batch_size: {params.get('batch_size', [])}
  â€¢ input_len: {params.get('input_len', [])}
  â€¢ output_len: {params.get('output_len', [])}

"""
        
        # å¦‚æœæ¨¡å‹åç§°æ˜ç¡®ï¼Œæ‰§è¡Œå®é™…åˆ†æ
        if model_name:
            # è·å–æ¨¡å‹è·¯å¾„
            model_path = self._resolve_model_path(model_name)
            
            if not model_path:
                response += f"""
âŒ **é”™è¯¯**: æœªæ‰¾åˆ°æ¨¡å‹ '{model_name}'
ğŸ“‹ å¯ç”¨æ¨¡å‹: {', '.join(self.model_mappings.keys())}

ğŸ’¡ **æç¤º**: è¯·åœ¨ config.yaml ä¸­é…ç½®æ¨¡å‹è·¯å¾„
"""
                return response
            
            response += f"""ğŸš€ **å¼€å§‹åˆ†æ...**

ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}
â³ é¢„è®¡æ—¶é—´: 3-10åˆ†é’Ÿï¼ˆå–å†³äºå‚æ•°ç»„åˆæ•°é‡ï¼‰

"""
            
            # æ‰§è¡Œåˆ†æï¼ˆå¼‚æ­¥ï¼‰
            try:
                analysis_results = await self._run_analysis(
                    model_path=model_path,
                    analysis_type=analysis_type,
                    params=params
                )
                
                response += analysis_results
                
            except Exception as e:
                response += f"""
âŒ **åˆ†æå¤±è´¥**: {str(e)}

ğŸ’¡ **å¯èƒ½åŸå› **:
1. NSys/NCUå·¥å…·æœªå®‰è£…æˆ–æœªåœ¨PATHä¸­
2. æ¨¡å‹è·¯å¾„ä¸æ­£ç¡®
3. GPUä¸å¯ç”¨æˆ–é©±åŠ¨é—®é¢˜
4. å‚æ•°é…ç½®é”™è¯¯

ğŸ”§ **è°ƒè¯•æ­¥éª¤**:
1. è¿è¡Œ `nsys --version` å’Œ `ncu --version` æ£€æŸ¥å·¥å…·
2. è¿è¡Œ `nvidia-smi` æ£€æŸ¥GPU
3. æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
"""
        else:
            response += """
ğŸ’¡ **ä¸‹ä¸€æ­¥**:
è¯·æŒ‡å®šè¦åˆ†æçš„æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ï¼š
â€¢ "åˆ†æ llama-7b"
â€¢ "å¯¹ qwen-14b è¿›è¡Œæ€§èƒ½åˆ†æ"
â€¢ "ä½¿ç”¨ ncu æ·±åº¦åˆ†æ chatglm-6b"

ğŸ“‹ **å¯ç”¨æ¨¡å‹**: """ + ', '.join(self.model_mappings.keys())
        
        return response
    
    async def _run_analysis(self, model_path: str, analysis_type: str, params: Dict) -> str:
        """æ‰§è¡Œå®é™…çš„æ€§èƒ½åˆ†æ"""
        
        results = []

        # é‡ç½®ç¼“å­˜ï¼Œé˜²æ­¢ä½¿ç”¨é™ˆæ—§ç»“æœ
        self.last_analysis_table = None
        self.last_analysis_reports = []
        self.last_analysis_dirs = []
        self.last_analysis_dir = None
        self.last_analysis_suggestions = None
        self.last_roofline_estimate = None
        
        # è·å–å‚æ•°ç»„åˆ
        batch_sizes = params.get('batch_size', [1])
        input_lens = params.get('input_len', [128])
        output_lens = params.get('output_len', [1])
        
        # åªåˆ†æç¬¬ä¸€ç»„å‚æ•°ï¼ˆé¿å…æ—¶é—´è¿‡é•¿ï¼‰
        batch_size = batch_sizes[0] if isinstance(batch_sizes, list) else batch_sizes
        input_len = input_lens[0] if isinstance(input_lens, list) else input_lens
        output_len = output_lens[0] if isinstance(output_lens, list) else output_lens

        precision_cfg = self.analysis_defaults.get('precision', {}) if isinstance(self.analysis_defaults.get('precision', {}), dict) else {}

        def _parse_int(value, default):
            try:
                return int(value)
            except (TypeError, ValueError):
                return default

        w_bit = _parse_int(precision_cfg.get('w_bit'), 16)
        a_bit = _parse_int(precision_cfg.get('a_bit'), 16)
        kv_bit_candidate = precision_cfg.get('kv_bit')
        parsed_kv_bit = _parse_int(kv_bit_candidate, None) if kv_bit_candidate is not None else None
        kv_bit = parsed_kv_bit if isinstance(parsed_kv_bit, int) else None
        use_flashattention = bool(precision_cfg.get('use_flashattention', False))
        hardware_key = self.analysis_defaults.get('hardware') or os.getenv('ROOFLINE_HARDWARE') or 'nvidia_H800_SXM5_80G'
        
        try:
            # åˆ›å»ºåˆ†æå·¥ä½œæµ
            analysis_workflow = create_sglang_analysis_workflow()
            
            # æ‰§è¡Œåˆ†æ
            loop = asyncio.get_event_loop()
            workflow_callable = partial(
                analysis_workflow,
                str(model_path),
                batch_size,
                input_len,
                output_len,
                True,
                None,
                hardware_key,
                w_bit,
                a_bit,
                kv_bit,
                use_flashattention,
            )
            workflow_output = await loop.run_in_executor(None, workflow_callable)

            run_records: List[Tuple[str, Path]] = []
            if isinstance(workflow_output, list):
                for idx, item in enumerate(workflow_output):
                    gpu_label: str
                    output_path: Optional[str] = None
                    if isinstance(item, dict):
                        gpu_label = str(item.get('gpu', idx))
                        output_path = item.get('dir') or item.get('path')
                    else:
                        gpu_label = str(idx)
                        output_path = str(item)
                    if output_path:
                        run_records.append((gpu_label, Path(output_path)))
            elif workflow_output:
                run_records.append(("0", Path(str(workflow_output))))

            if not run_records:
                results.append("âš ï¸ **åˆ†æå·²å®Œæˆï¼Œä½†æœªæ‰¾åˆ°è¾“å‡ºç›®å½•**")
                return '\n'.join(results)

            self.last_analysis_dirs = [str(path) for _, path in run_records]

            report_infos = []
            roofline_infos: List[Tuple[str, Path, Dict[str, Any]]] = []
            missing_reports = []
            for idx, (gpu_label, output_dir) in enumerate(run_records):
                report_path = output_dir / "integrated_performance_report.md"
                if report_path.exists():
                    report_text = report_path.read_text(encoding='utf-8')
                    report_infos.append({
                        'gpu': gpu_label,
                        'dir': output_dir,
                        'report': report_path,
                        'text': report_text,
                        'index': idx
                    })
                    roofline_path = output_dir / "roofline_estimate.json"
                    if roofline_path.exists():
                        try:
                            with open(roofline_path, 'r', encoding='utf-8') as rf:
                                roofline_data = json.load(rf)
                            roofline_infos.append((gpu_label, roofline_path, roofline_data))
                        except Exception as roof_exc:
                            print(f"âš ï¸ è¯»å– Roofline é¢„æµ‹å¤±è´¥ ({roofline_path}): {roof_exc}")
                else:
                    missing_reports.append(output_dir)

            if not report_infos:
                dir_lines = '\n'.join(f"  â€¢ {path}" for _, path in run_records)
                results.append(f"""
âš ï¸ **åˆ†æå·²å®Œæˆï¼Œä½†æœªç”ŸæˆæŠ¥å‘Šæ–‡ä»¶**

ğŸ“ ç»“æœç›®å½•:
{dir_lines}
ğŸ’¡ è¯·æ£€æŸ¥ç›®å½•ä¸­çš„å…¶ä»–è¾“å‡ºæ–‡ä»¶
""")
                return '\n'.join(results)

            primary_info = report_infos[0]
            self.last_analysis_dir = str(primary_info['dir'])
            self.last_analysis_reports = [str(info['report']) for info in report_infos]
            summary = self._extract_report_summary(primary_info['text'])

            try:
                loop = asyncio.get_event_loop()
                if len(report_infos) > 1:
                    table_markdown = self._generate_multi_gpu_table(
                        [info['text'] for info in report_infos],
                        [info['gpu'] for info in report_infos]
                    )
                else:
                    table_markdown = await loop.run_in_executor(
                        None,
                        self._generate_report_table,
                        primary_info['text']
                    )
            except Exception as table_exc:
                table_markdown = f"âš ï¸ è¡¨æ ¼ç”Ÿæˆå¤±è´¥: {table_exc}"

            self.last_analysis_table = table_markdown

            suggestions = self._generate_optimization_suggestions(report_infos)
            self.last_analysis_suggestions = suggestions if suggestions else None

            dir_lines = '\n'.join(
                f"  â€¢ {self._format_gpu_label(info['gpu'], info['index'])}: {info['dir']}" for info in report_infos
            )

            missing_lines = ''
            if missing_reports:
                missing_lines = '\n'.join(f"  â€¢ {path}" for path in missing_reports)
                missing_lines = f"\nâš ï¸ æœªæ‰¾åˆ°ä»¥ä¸‹ç›®å½•çš„æŠ¥å‘Šæ–‡ä»¶:\n{missing_lines}\n"

            roofline_section = "ğŸ“ **Roofline é¢„æµ‹**:\næš‚æœªç”Ÿæˆ Roofline é¢„æµ‹\n"
            if roofline_infos:
                self.last_roofline_estimate = roofline_infos[0][2]
                roofline_preview = self._render_roofline_preview(self.last_roofline_estimate)
                roofline_source = str(roofline_infos[0][1])
                roofline_section = (
                    f"ğŸ“ **Roofline é¢„æµ‹** (æ¥æº: {roofline_source}):\n{roofline_preview}\n"
                )

            results.append(f"""
âœ… **åˆ†æå®Œæˆ!**

ğŸ“ **ç»“æœç›®å½•**:
{dir_lines}
ğŸ“„ **æŠ¥å‘Šæ–‡ä»¶**: {primary_info['report']}
{missing_lines}
{summary}

{roofline_section}

ğŸ“Œ **çƒ­ç‚¹Kernelè¡¨æ ¼é¢„è§ˆ**:
{table_markdown}

ğŸ’¡ **ä¼˜åŒ–å»ºè®®**:
{suggestions or 'æš‚æœªç”Ÿæˆä¼˜åŒ–å»ºè®®'}

ğŸ” **è¯¦ç»†æŠ¥å‘Š**: è¯·æŸ¥çœ‹ {primary_info['report']}
ğŸ“Š **å¯è§†åŒ–å›¾è¡¨**: è¯·æŸ¥çœ‹å¯¹åº”ç»“æœç›®å½•ä¸­çš„å›¾ç‰‡æ–‡ä»¶
""")
            
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            results.append(f"""
âŒ **åˆ†ææ‰§è¡Œå¤±è´¥**

é”™è¯¯ä¿¡æ¯: {str(e)}

è¯¦ç»†é”™è¯¯:
```
{error_detail}
```

ğŸ’¡ **å¸¸è§é—®é¢˜è§£å†³**:
1. ç¡®ä¿å·²å®‰è£… nsys å’Œ ncu å·¥å…·
2. ç¡®ä¿ SGlang å·²æ­£ç¡®å®‰è£…
3. ç¡®ä¿æ¨¡å‹æ–‡ä»¶è·¯å¾„æ­£ç¡®
4. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜
""")
        
        return '\n'.join(results)

    def _render_roofline_preview(self, roofline: Dict[str, Any]) -> str:
        if not roofline:
            return "æš‚æœªç”Ÿæˆ Roofline æ•°æ®"

        def _fmt_time(seconds: Optional[float]) -> str:
            if seconds is None:
                return "N/A"
            if isinstance(seconds, (int, float)):
                if seconds == float('inf'):
                    return "âˆ"
                return f"{seconds * 1000:.3f} ms"
            return str(seconds)

        def _fmt_perf(value: Optional[float]) -> str:
            if not isinstance(value, (int, float)):
                return "N/A"
            return f"{value / 1e12:.2f} TOPS"

        def _fmt_ai(value: Optional[float]) -> str:
            if not isinstance(value, (int, float)):
                return "N/A"
            return f"{value:.2f} OPs/Byte"

        def _fmt_mem(value: Optional[float]) -> str:
            if not isinstance(value, (int, float)):
                return "N/A"
            return f"{value / 1e9:.2f} GB"

        bits = roofline.get('precision_bits', {})
        params = roofline.get('params', {})
        prefill = roofline.get('prefill', {})
        decode = roofline.get('decode', {})
        overall = roofline.get('overall', {})

        lines = []
        lines.append(f"- æ¨¡å‹ç¡¬ä»¶: {roofline.get('hardware', 'unknown')}")
        lines.append(
            f"- ç²¾åº¦: W{bits.get('w', 'N/A')} / A{bits.get('a', 'N/A')} / KV{bits.get('kv', 'N/A')}"
        )
        lines.append(
            f"- Prefill: å¼ºåº¦ {_fmt_ai(prefill.get('arithmetic_intensity'))}, "
            f"æ€§èƒ½ {_fmt_perf(prefill.get('performance'))}, "
            f"è€—æ—¶ {_fmt_time(prefill.get('inference_time'))}, "
            f"å†…å­˜è®¿é—® {_fmt_mem(prefill.get('memory_access'))}"
        )
        lines.append(
            f"- Decode(å• token): å¼ºåº¦ {_fmt_ai(decode.get('arithmetic_intensity'))}, "
            f"æ€§èƒ½ {_fmt_perf(decode.get('performance'))}, "
            f"è€—æ—¶ {_fmt_time(decode.get('inference_time'))}"
        )
        lines.append(
            f"- æ€»ä½“ä¼°è®¡: å¼ºåº¦ {_fmt_ai(overall.get('arithmetic_intensity'))}, "
            f"æ€§èƒ½ {_fmt_perf(overall.get('performance'))}, "
            f"æ€»è€—æ—¶ {_fmt_time(overall.get('inference_time'))}"
        )
        if params:
            lines.append(
                f"- åˆ†æå‚æ•°: batch={params.get('batch_size')}, prompt={params.get('prompt_len')}, output={params.get('output_len')}"
            )
        return '\n'.join(lines)

    @staticmethod
    def _generate_report_table(report_text: str) -> str:
        client = get_offline_qwen_client(OFFLINE_QWEN_PATH)
        return client.report_to_table(report_text)

    @staticmethod
    def _collect_ncu_csv_snippets(output_dir: Path, limit: int = 1200) -> List[Tuple[str, str]]:
        snippets: List[Tuple[str, str]] = []
        if not output_dir.exists():
            return snippets
        for csv_path in sorted(output_dir.glob("ncu_kernel*.csv")):
            try:
                raw = csv_path.read_text(encoding='utf-8', errors='ignore')
            except Exception:
                continue
            snippet = raw[:limit]
            if snippet.strip():
                snippets.append((csv_path.name, snippet))
        return snippets

    def _generate_optimization_suggestions(self, report_infos: List[Dict[str, str]]) -> str:
        if not report_infos:
            return ""

        try:
            client = get_offline_qwen_client(OFFLINE_QWEN_PATH)
        except Exception as exc:
            return f"âš ï¸ ä¼˜åŒ–å»ºè®®ç”Ÿæˆå¤±è´¥: {exc}"

        labeled_reports: List[Tuple[str, str]] = []
        raw_snippets: List[Tuple[str, str]] = []
        for info in report_infos:
            label = self._format_gpu_label(info['gpu'], info['index'])
            labeled_reports.append((label, info.get('text', '')))
            output_dir = Path(info['dir'])
            for name, data in self._collect_ncu_csv_snippets(output_dir):
                raw_snippets.append((f"{label} / {name}", data))

        output_sections: List[str] = []

        try:
            if labeled_reports:
                suggestions = client.suggest_optimizations(labeled_reports)
                if suggestions:
                    output_sections.append(suggestions)
        except Exception as exc:
            output_sections.append(f"âš ï¸ ä¼˜åŒ–å»ºè®®ç”Ÿæˆå¤±è´¥: {exc}")

        try:
            if raw_snippets:
                raw_suggestions = client.suggest_raw_data_optimizations(raw_snippets, max_new_tokens=1024)
                if raw_suggestions:
                    output_sections.append(f"ğŸ“Š åŸå§‹æ•°æ®å»ºè®®:\n{raw_suggestions}")
        except Exception as exc:
            output_sections.append(f"âš ï¸ åŸå§‹æ•°æ®å»ºè®®ç”Ÿæˆå¤±è´¥: {exc}")

        return "\n\n".join(output_sections).strip()

    def _generate_multi_gpu_table(self, report_texts: List[str], gpu_labels: List[str]) -> str:
        if not report_texts:
            return "âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„æŠ¥å‘Šå†…å®¹"

        formatted_labels: List[str] = []
        per_gpu_tables: List[Tuple[str, str]] = []
        for idx, report_text in enumerate(report_texts):
            raw_label = gpu_labels[idx] if idx < len(gpu_labels) else str(idx)
            label = self._format_gpu_label(raw_label, idx)
            formatted_labels.append(label)
            try:
                table_md = self._generate_report_table(report_text)
            except Exception:
                table_md = ""
            per_gpu_tables.append((label, table_md))

        try:
            client = get_offline_qwen_client(OFFLINE_QWEN_PATH)
            if any(table for _, table in per_gpu_tables):
                merged = client.merge_gpu_tables(per_gpu_tables)
                if merged and merged.count('|') >= len(formatted_labels) * 2:
                    return merged
        except Exception:
            pass

        return self._generate_multi_gpu_table_python(report_texts, gpu_labels)

    def _generate_multi_gpu_table_python(self, report_texts: List[str], gpu_labels: List[str]) -> str:
        if not report_texts:
            return "âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„æŠ¥å‘Šå†…å®¹"

        parsed_entries = [self._parse_kernel_entries_from_report(text) for text in report_texts]
        if not parsed_entries or not parsed_entries[0]:
            return "âš ï¸ æœªèƒ½è§£æå¤šGPUè¡¨æ ¼æ•°æ®"

        label_cells = [self._format_gpu_label(lbl, idx) for idx, lbl in enumerate(gpu_labels)]
        header_cells = ["Kernel"]
        last_index = len(label_cells) - 1
        for idx_lbl, lbl in enumerate(label_cells):
            header_cells.append(f"{lbl} Duration(ms)")
            add_ratio = len(label_cells) == 1 or idx_lbl != last_index
            if add_ratio:
                header_cells.append(f"{lbl} Ratio(%)")
        if len(label_cells) >= 2:
            header_cells.append(f"{label_cells[0]}ï¼š{label_cells[1]} æ—¶é—´å æ¯”")

        header = "| " + " | ".join(header_cells) + " |"
        divider = "| " + " | ".join(["---"] * len(header_cells)) + " |"

        max_len = max(len(entries) for entries in parsed_entries)
        rows = []
        def _parse_duration(val: str) -> float:
            try:
                return float(val)
            except (TypeError, ValueError):
                return 0.0
        def _parse_ratio_component(val: str) -> Optional[Fraction]:
            if val is None:
                return None
            text = str(val).strip()
            if not text:
                return None
            try:
                frac = Fraction(text)
            except (ValueError, ZeroDivisionError):
                return None
            if frac < 0:
                return None
            return frac
        def _fractions_to_ints(fracs: List[Optional[Fraction]]) -> List[Optional[int]]:
            positives = [f for f in fracs if isinstance(f, Fraction) and f > 0]
            scale = None
            if positives:
                scale = positives[0].denominator
                for frac in positives[1:]:
                    scale = lcm(scale, frac.denominator)
            ints: List[Optional[int]] = []
            for frac in fracs:
                if frac is None:
                    ints.append(None)
                elif frac == 0:
                    ints.append(0)
                else:
                    if scale is None:
                        scale = frac.denominator
                    ints.append(frac.numerator * (scale // frac.denominator))
            if positives:
                common = None
                for val in ints:
                    if isinstance(val, int) and val > 0:
                        common = val if common is None else gcd(common, val)
                if common and common > 1:
                    ints = [val // common if isinstance(val, int) and val > 0 else val for val in ints]
            return ints
        for idx in range(max_len):
            name_candidates = []
            for entries in parsed_entries:
                if idx < len(entries) and entries[idx]['name']:
                    name_candidates.append(entries[idx]['name'])
            base_name = name_candidates[0] if name_candidates else f"Kernel {idx + 1}"
            alt_names = {nm for nm in name_candidates if nm != base_name}
            if alt_names:
                merged_name = base_name + " / " + " / ".join(sorted(alt_names))
            else:
                merged_name = base_name

            row_cells = [merged_name]
            duration_values: List[float] = []
            pair_ratios: Optional[List[Optional[Fraction]]] = [None, None] if len(parsed_entries) >= 2 else None
            for gpu_idx, entries in enumerate(parsed_entries):
                if idx < len(entries):
                    duration = entries[idx]['duration']
                    ratio = entries[idx]['ratio']
                    row_cells.append(duration)
                    duration_values.append(_parse_duration(duration))
                    add_ratio = len(parsed_entries) == 1 or gpu_idx != last_index
                    if add_ratio:
                        row_cells.append(ratio)
                    if pair_ratios is not None and gpu_idx < 2:
                        pair_ratios[gpu_idx] = _parse_ratio_component(ratio)
                else:
                    row_cells.append('')
                    add_ratio = len(parsed_entries) == 1 or gpu_idx != last_index
                    if add_ratio:
                        row_cells.append('')
                    if pair_ratios is not None and gpu_idx < 2:
                        pair_ratios[gpu_idx] = Fraction(0, 1)
            if pair_ratios is not None:
                simplified_ints = _fractions_to_ints(pair_ratios)
                pair_strings = []
                for val in simplified_ints:
                    if val is None:
                        pair_strings.append('')
                    else:
                        pair_strings.append(str(val))
                combined = f"{pair_strings[0]}ï¼š{pair_strings[1]}" if any(pair_strings) else ''
                row_cells.append(combined)
            sort_key = max(duration_values) if duration_values else 0.0
            rows.append((sort_key, row_cells))
        sorted_rows = ["| " + " | ".join(cells) + " |" for _, cells in sorted(rows, key=lambda item: item[0], reverse=True)]

        return "\n".join([header, divider, *sorted_rows])

    def _parse_kernel_entries_from_report(self, report_text: str) -> List[Dict[str, str]]:
        entries: List[Dict[str, str]] = []
        lines = report_text.splitlines()
        idx = 0
        total_lines = len(lines)
        while idx < total_lines:
            raw_line = lines[idx]
            if raw_line.strip().startswith('äºŒã€'):
                break
            match = re.match(r'^\s*\d+\.\s+(.*)$', raw_line)
            if match:
                name = match.group(1).strip()
                duration = ''
                ratio = ''
                idx += 1
                while idx < total_lines:
                    line = lines[idx].strip()
                    if line.startswith('- æ‰§è¡Œæ—¶é—´'):
                        dur_match = re.search(r'([0-9.]+)\s*ms', line)
                        if dur_match:
                            duration = dur_match.group(1)
                    elif line.startswith('- æ—¶é—´å æ¯”'):
                        ratio_match = re.search(r'([0-9.]+)\s*%', line)
                        if ratio_match:
                            ratio = ratio_match.group(1)
                    elif re.match(r'^\s*\d+\.', lines[idx]) or line.startswith('äºŒã€'):
                        break
                    idx += 1
                entries.append({
                    'name': name,
                    'duration': duration,
                    'ratio': ratio
                })
            else:
                idx += 1
        return entries

    @staticmethod
    def _format_gpu_label(label: str, index: int) -> str:
        if not label:
            return f"GPU{index}"
        normalized = label.strip()
        if not normalized:
            return f"GPU{index}"
        if normalized.lower().startswith('gpu'):
            return normalized.upper()
        return f"GPU{normalized}"
    
    def _extract_report_summary(self, report_content: str) -> str:
        """ä»æŠ¥å‘Šä¸­æå–å…³é”®æ‘˜è¦ä¿¡æ¯"""
        
        lines = report_content.split('\n')
        summary_lines = []
        
        # æå–å…³é”®ç»Ÿè®¡ä¿¡æ¯
        for i, line in enumerate(lines):
            if 'æ€»kernelsæ•°é‡' in line or 'æ€»kernelæ‰§è¡Œæ—¶é—´' in line:
                summary_lines.append(line)
            elif 'ğŸ”¥ è¯†åˆ«çš„çƒ­ç‚¹Kernels' in line:
                # æå–å‰3ä¸ªçƒ­ç‚¹kernel
                summary_lines.append("\n**ğŸ”¥ çƒ­ç‚¹Kernels (Top 3):**")
                for j in range(i+1, min(i+10, len(lines))):
                    if lines[j].strip() and lines[j].startswith(('1.', '2.', '3.')):
                        summary_lines.append(lines[j][:100])
                break
        
        if summary_lines:
            return '\n'.join(summary_lines)
        else:
            return "**ğŸ“Š åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æ–‡ä»¶**"
    
    def _resolve_model_path(self, model_name: str) -> Optional[str]:
        """è§£ææ¨¡å‹è·¯å¾„"""
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ˜ å°„è¡¨ä¸­
        if model_name in self.model_mappings:
            mapped_path = self.model_mappings[model_name]
            
            # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥è¿”å›
            if Path(mapped_path).is_absolute():
                return mapped_path
            
            # å¦åˆ™ï¼Œç›¸å¯¹äº models_path
            full_path = self.models_path / mapped_path
            return str(full_path)
        
        # å¦‚æœä¸åœ¨æ˜ å°„è¡¨ä¸­ï¼Œå°è¯•ç›´æ¥ä½œä¸ºè·¯å¾„
        if Path(model_name).exists():
            return model_name
        
        # å°è¯•ç›¸å¯¹äº models_path
        potential_path = self.models_path / model_name
        if potential_path.exists():
            return str(potential_path)
        
        return None
    
    def _extract_model_name(self, prompt: str) -> Optional[str]:
        """æå–æ¨¡å‹åç§°"""
        
        # é¦–å…ˆæ£€æŸ¥å·²çŸ¥çš„æ¨¡å‹åˆ«å
        for model_name in self.model_mappings.keys():
            if model_name.lower() in prompt.lower():
                return model_name
        
        # ç„¶åä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…é€šç”¨æ¨¡å‹åç§°æ¨¡å¼
        patterns = [
            r'llama[^/\s]*-?\d*[^/\s]*-?\d+[bB]?',
            r'qwen[^/\s]*-?\d*[^/\s]*-?\d+[bB]?',
            r'chatglm[^/\s]*-?\d+[bB]?',
            r'baichuan[^/\s]*-?\d+[bB]?',
            r'vicuna[^/\s]*-?\d+[bB]?',
            r'mistral[^/\s]*-?\d+[bB]?',
            r'mixtral[^/\s]*-?\d+[bB]?',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, prompt, re.IGNORECASE)
            if match:
                return match.group(0)
        
        return None
    
    def _extract_analysis_type(self, prompt: str) -> str:
        """æå–åˆ†æç±»å‹"""
        prompt_lower = prompt.lower()
        
        if 'ncu' in prompt_lower or 'kernel' in prompt_lower or 'æ·±åº¦' in prompt_lower or 'nsight compute' in prompt_lower:
            return 'ncu (æ·±åº¦kernelåˆ†æ)'
        elif 'nsys' in prompt_lower or 'å…¨å±€' in prompt_lower or 'nsight systems' in prompt_lower:
            return 'nsys (å…¨å±€æ€§èƒ½åˆ†æ)'
        elif 'é›†æˆ' in prompt_lower or 'ç»¼åˆ' in prompt_lower or 'å®Œæ•´' in prompt_lower:
            return 'auto (é›†æˆåˆ†æ: nsys + ncu)'
        else:
            return 'auto (é›†æˆåˆ†æ: nsys + ncu)'
    
    def _extract_parameters(self, prompt: str) -> Dict:
        """æå–å‚æ•°"""
        params = {}
        
        # æå–batch_size
        batch_match = re.search(r'batch[-_\s]*size?[ï¼š:\s=]*(\d+(?:\s*[,ï¼Œ]\s*\d+)*)', prompt, re.IGNORECASE)
        if batch_match:
            batch_sizes = [int(x.strip()) for x in re.split(r'[,ï¼Œ\s]+', batch_match.group(1)) if x.strip()]
            params['batch_size'] = batch_sizes
        
        # æå–input_len
        input_match = re.search(r'input[-_\s]*len[gth]*[ï¼š:\s=]*(\d+(?:\s*[,ï¼Œ]\s*\d+)*)', prompt, re.IGNORECASE)
        if input_match:
            input_lens = [int(x.strip()) for x in re.split(r'[,ï¼Œ\s]+', input_match.group(1)) if x.strip()]
            params['input_len'] = input_lens
        
        # æå–output_len
        output_match = re.search(r'output[-_\s]*len[gth]*[ï¼š:\s=]*(\d+(?:\s*[,ï¼Œ]\s*\d+)*)', prompt, re.IGNORECASE)
        if output_match:
            output_lens = [int(x.strip()) for x in re.split(r'[,ï¼Œ\s]+', output_match.group(1)) if x.strip()]
            params['output_len'] = output_lens
        
        return params

    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        return list(self.model_mappings.keys())
    
    def get_analysis_status(self) -> Dict:
        """è·å–å½“å‰åˆ†æçŠ¶æ€"""
        return {
            'available_models': self.get_available_models(),
            'results_directory': str(self.results_dir),
            'nsys_enabled': self.profiling_config.get('nsys', {}).get('enabled', True),
            'ncu_enabled': self.profiling_config.get('ncu', {}).get('enabled', True),
        }
