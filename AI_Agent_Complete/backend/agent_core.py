#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Agentæ ¸å¿ƒæ¨¡å— - é›†æˆNSyså’ŒNCUæ€§èƒ½åˆ†æ
"""

import re
import os
import json
import asyncio
import sys
import yaml
from functools import partial
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from math import gcd, lcm
from fractions import Fraction

# å¯¼å…¥åˆ†æå·¥å…·
from utils.nsys_to_ncu_analyzer import create_sglang_analysis_workflow
from offline_llm import get_offline_qwen_client
from knowledge_bases.vector_kb_manager import VectorKBManager

try:
    from .utils.roofline_estimator import compute_roofline
except Exception:
    try:
        from utils.roofline_estimator import compute_roofline
    except Exception:
        compute_roofline = None

OFFLINE_QWEN_PATH = Path(os.getenv("QWEN_LOCAL_MODEL_PATH", "/workspace/Qwen3-32B"))


class AIAgent:
    """AI Agentæ ¸å¿ƒç±» - è‡ªåŠ¨åŒ–æ€§èƒ½åˆ†æ"""

    def __init__(self, config: Dict):
        self.config = config

        # sglang å’Œæ¨¡å‹è·¯å¾„
        self.sglang_path = Path(config.get("sglang_path"))
        self.models_path = Path(config.get("models_path"))
        self.model_mappings = config.get("model_mappings", {})

        # åˆ†æç»“æœè¾“å‡ºç›®å½•
        self.results_dir = Path(config.get("output", {}).get("results_dir", "results"))
        self.results_dir.mkdir(exist_ok=True, parents=True)

        # åˆ†æå·¥å…·é…ç½®
        self.profiling_config = config.get("profiling_tools", {})
        self.analysis_defaults = config.get("analysis_defaults", {})

        # ç¼“å­˜æœ€è¿‘ä¸€æ¬¡åˆ†æçš„å…³é”®ä¿¡æ¯ï¼Œä¾¿äºå¯¹å¤–æ¥å£å¤ç”¨
        self.last_analysis_dir: Optional[str] = None
        self.last_analysis_dirs: List[str] = []
        self.last_analysis_reports: List[str] = []
        self.last_analysis_table: Optional[str] = None
        self.last_analysis_suggestions: Optional[str] = None
        self.last_roofline_estimate: Optional[Dict[str, Any]] = None

        # æœ¬åœ° LLM å®¢æˆ·ç«¯
        self.offline_qwen_path = Path(config.get("offline_qwen_path"))
        self.llm_client = get_offline_qwen_client(self.offline_qwen_path)

        # å‘é‡çŸ¥è¯†åº“ç›¸å…³
        # kb_config = config.get("vector_store", {})
        # self.embedding_model = kb_config.get("embedding_model")
        # self.persist_directory = kb_config.get("persist_directory")
        # self.chunk_size = kb_config.get("chunk_size")
        # self.chunk_overlap = kb_config.get("chunk_overlap")
        # self.default_search_k = kb_config.get("default_search_k", 8)
        # self.max_distance = kb_config.get("max_distance", 0.5)
        self.kb = VectorKBManager(config=config)

        # å¯¹è¯å†å²ç¼“å†²åŒº
        self.chat_history: List[Dict[str, str]] = []  # å¯¹è¯å†å²ï¼ˆå®Œæ•´ä¿ç•™ï¼‰
        self.default_history_turns = 3  # é»˜è®¤æ‹‰å–æœ€è¿‘å¯¹è¯è½®æ•°ï¼ˆæŒ‰éœ€æ‹‰å–ï¼‰

        # intent-prompt æ˜ å°„
        self.intent_mappings = {
            "analysis": "ç”¨æˆ·å¸Œæœ›**ç«‹å³æ‰§è¡Œ**æ€§èƒ½åˆ†æä»»åŠ¡ï¼ˆå¦‚â€œè·‘ä¸€ä¸‹qwenâ€ã€â€œnsys/ncuåˆ†æâ€ï¼‰ã€‚å¿…é¡»åŒ…å«**åŠ¨ä½œæ„å›¾**ï¼ˆè¿è¡Œ/æµ‹è¯•/åˆ†æç­‰ï¼‰ã€‚",
            "rag-qa": "ç”¨æˆ·åœ¨**è¯¢é—®çŸ¥è¯†ã€æ•°æ®æˆ–å»ºè®®**ï¼ˆå¦‚â€œç“¶é¢ˆæ˜¯ä»€ä¹ˆâ€ã€â€œæ¨èbatch_sizeâ€ã€â€œæŸä¸ªKernelçš„è¿è¡Œæƒ…å†µâ€ï¼‰ã€‚æ— æ‰§è¡Œæ„å›¾ã€‚",
            "chat": "æ‰“æ‹›å‘¼ã€æ„Ÿè°¢ã€é—²èŠä»¥åŠä¸€åˆ‡æ— æ³•å½’ç±»çš„å†…å®¹ï¼ˆå¦‚â€œä½ å¥½â€ã€â€œä½ æ˜¯è°â€ï¼‰ã€‚",
        }

    def _format_history_str(
        self, history: List[Dict[str, str]], limit: int = -1
    ) -> str:
        """
        å–å‡ºå¹¶æ ¼å¼åŒ–å†å²å¯¹è¯ã€‚
        Args:
            history: å®Œæ•´å†å²åˆ—è¡¨
            limit: å–æœ€è¿‘å¤šå°‘è½®å¯¹è¯ã€‚1è½®=ç”¨æˆ·+Agentå…±2æ¡å¯¹è¯ã€‚
                   -1   : (é»˜è®¤) ä½¿ç”¨ self.default_history_turns
                   0 : ä¸å–å†å²å¯¹è¯
                   int  : æŒ‡å®šå…·ä½“è½®æ•°
        """
        if not history or limit == 0:
            return ""

        if limit == -1:
            limit = self.default_history_turns

        # å–æœ€è¿‘ limit è½®å¯¹è¯
        target_history = history[-(limit * 2) :] if limit else history

        lines = []
        for msg in target_history:
            role = "User" if msg["role"] == "user" else "Assistant"
            # é™åˆ¶å•æ¡æ¶ˆæ¯é•¿åº¦ï¼Œé˜²æ­¢ Prompt è¿‡é•¿
            content = msg["content"].replace("\n", " ").strip()[:512]
            lines.append(f"{role}: {content}")

        return "\n".join(lines)

    async def _parse_intent(self, user_query: str, intent: str = "auto") -> str:
        """
        æ„å›¾è¯†åˆ«å‡½æ•°ã€‚

        Args:
            user_query: ç”¨æˆ·å½“å‰è¾“å…¥
            history: å¯¹è¯å†å²ï¼ˆç”¨äºä¸Šä¸‹æ–‡ï¼Œä½†éå¿…éœ€ï¼‰
            intent: æŒ‡å®šæ„å›¾æ¨¡å¼ã€‚è‹¥ä¸º "auto"ï¼Œåˆ™ç”± LLM åˆ¤æ–­ï¼›å¦åˆ™ç›´æ¥è¿”å› intentã€‚

        Returns:
            str: æ„å›¾ç±»åˆ«ï¼Œå–å€¼ä¸º self.intent_mappings çš„é”®ä¹‹ä¸€ï¼ˆå¦‚ "rag-qa", "analysis", "chat"ï¼‰
        """
        supported_intents = set(self.intent_mappings.keys())

        # å¦‚æœ intent ä¸æ˜¯ autoï¼Œç›´æ¥è¿”å›
        if intent != "auto":
            return intent

        # === LLM è‡ªåŠ¨æ„å›¾è¯†åˆ« ===
        # å‘Šè¯‰ LLM å½“å‰å¯ç”¨æ¨¡å‹æœ‰å“ªäº›
        available_models = list(self.model_mappings.keys())
        models_str = ", ".join([f'"{m}"' for m in available_models])

        # æ„å»ºå†å²å¯¹è¯
        history_str = self._format_history_str(self.chat_history)

        # ç”Ÿæˆæ„å›¾å®šä¹‰æè¿°
        intent_definitions = "\n".join(
            f"- **{intent}**: {desc}" for intent, desc in self.intent_mappings.items()
        )

        prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªæ„å›¾åˆ†ç±»å™¨ã€‚è¯·ä»…æ ¹æ®ç”¨æˆ·å½“å‰è¾“å…¥çš„**è¯­ä¹‰**åˆ¤æ–­å…¶æ„å›¾ç±»åˆ«ã€‚
            
            ### æ„å›¾å®šä¹‰ï¼ˆé‡è¦ï¼‰
            {intent_definitions}
            
            ### è¾“å‡ºè¦æ±‚ï¼ˆé‡è¦ï¼‰
            ä»…è¾“å‡ºä»¥ä¸‹å•è¯ä¹‹ä¸€ï¼š{" | ".join(supported_intents)}ï¼Œä¸è¦è§£é‡Šï¼Œä¸è¦æ ‡ç‚¹ï¼Œä¸è¦JSONã€‚
            
            ### åˆ¤æ–­åŸåˆ™
            1. ä¼˜å…ˆçœ‹**ç”¨æˆ·å½“å‰è¾“å…¥**ï¼Œå†å²ä»…ä½œè¾…åŠ©ã€‚
            2. è‹¥æ— æ³•æ˜ç¡®å½’ç±»ï¼Œè¯·è¿”å›"chat"ã€‚

            ### å½“å‰å¯ç”¨æ¨¡å‹å‚è€ƒï¼ˆä»…ä½œèƒŒæ™¯å‚è€ƒï¼Œä¸å½±å“åˆ†ç±»ï¼‰
            [{models_str}]
            
            ### ç”¨æˆ·å½“å‰è¾“å…¥ï¼ˆä¸»è¦åˆ¤æ–­ä¾æ®ï¼‰
            {user_query}

            {"### æœ€è¿‘å¯¹è¯å†å²ï¼ˆå¯å¿½ç•¥çš„æ¬¡è¦åˆ¤æ–­ä¾æ®ï¼‰\n" + history_str if history_str else "ï¼ˆæ— å†å²è®°å½•ï¼‰"}
        """

        raw_output = self.llm_client.generate(prompt, max_tokens=32).strip()
        raw_output = raw_output.lower().strip(" .,!?\"'")
        if raw_output in supported_intents:
            return raw_output
        else:
            # è§„åˆ™å…œåº•
            user_query_lower = user_query.lower()
            if any(
                kw in user_query_lower
                for kw in ["åˆ†æ", "è·‘", "æµ‹", "profile", "è¿è¡Œ", "æ‰§è¡Œ", "æµ‹è¯•"]
            ):
                return "analysis"
            elif any(
                kw in user_query_lower
                for kw in [
                    "ä»€ä¹ˆ",
                    "å¤šå°‘",
                    "æ˜¯å¦",
                    "ä¸ºä»€ä¹ˆ",
                    "å¦‚ä½•",
                    "æ¨è",
                    "æ–‡æ¡£",
                    "æŸ¥è¯¢",
                    "æ˜¯å¤šå°‘",
                ]
            ):
                return "rag-qa"
            else:
                return "chat"

    async def _parse_raw_params(
        self, user_query: str, rewrite_query: bool = False
    ) -> Dict[str, Any]:
        """
        ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–åŸå§‹çš„æ¨¡å‹åç§°ã€åˆ†æå‚æ•°å’Œæ”¹å†™åçš„queryã€‚åç»­å°†å…¶è½¬ä¸ºæ€§èƒ½åˆ†æçš„å‚æ•°ï¼Œæˆ–è€…ç”¨äºRAG-QAçš„filterã€‚
        æŠŠæå–jsonå’Œæ”¹å†™queryæ”¾åœ¨ä¸€èµ·ï¼Œå› ä¸ºè¿™æ ·åšè¯­ä¹‰æ›´è¿è´¯ã€‚
        è¿”å›ç¤ºä¾‹è¯·å‚è€ƒä¸‹é¢çš„promptã€‚

        """

        available_models = list(self.model_mappings.keys())
        models_str = ", ".join([f'"{m}"' for m in available_models])

        # æ„å»ºå†å²å¯¹è¯
        history_str = self._format_history_str(self.chat_history, limit=1)

        prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å‚æ•°æå–åŠ©æ‰‹ã€‚è¯·ä»ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è¾“å…¥ä¸­æå–æ‰§è¡Œå‚æ•°ï¼Œå¹¶åŒæ—¶ç”Ÿæˆç”¨äºå‘é‡æ£€ç´¢çš„â€œæ”¹å†™æŸ¥è¯¢â€ã€‚
            
            ### 0. è¾“å‡ºæ ¼å¼ï¼ˆéå¸¸é‡è¦ï¼‰
            ä½ å¿…é¡»**ä»…è¾“å‡ºä¸€ä¸ªæ ‡å‡† JSON å¯¹è±¡**ï¼ˆä¸èƒ½æœ‰ä»»ä½•è§£é‡Šã€ä¸èƒ½æœ‰ Markdown ä»£ç å—ã€ä¸èƒ½æœ‰å¤šä½™æ–‡æœ¬ï¼‰ã€‚
            JSON å¿…é¡»ä»…åŒ…å«ä»¥ä¸‹ 4 ä¸ªå­—æ®µï¼šmodel, params, analysis_type, search_queryã€‚

            ### 1. å­—æ®µå®šä¹‰
            - **model**: æ¨¡å‹åç§°ã€‚å¿…é¡»ä¸¥æ ¼åŒ¹é…åˆ—è¡¨ï¼š[{models_str}]ã€‚å¦‚æœæ‰¾ä¸åˆ°åŒ¹é…é¡¹åˆ™ä¸ºnullã€‚
            - **params**: ä¸€ä¸ªå¿…é¡»å­˜åœ¨çš„ JSON å¯¹è±¡ï¼Œå…è®¸ä¸ºç©ºå¯¹è±¡ï¼ŒåŒ…å«æ•´æ•°é”®ï¼šbatch_size/input_len/output_lenã€‚å¦‚æœç”¨æˆ·æ²¡æåˆ°å¯¹åº”é”®ï¼Œåˆ™ç›´æ¥å¿½ç•¥ã€‚
            - **analysis_type**: åˆ†æç±»å‹ï¼Œå–å€¼åªèƒ½æ˜¯ "nsys", "ncu"æˆ–nullã€‚åˆ¤æ–­é€»è¾‘å¦‚ä¸‹ï¼š
                1. **nsys**: ç”¨æˆ·æåˆ° "nsys"ã€"å…¨å±€"ã€"æ•´ä½“"ã€"profile"ã€"timeline"ã€‚
                2. **ncu**: ç”¨æˆ·æåˆ° "ncu"ã€"æ·±åº¦"ã€"kernelç»†èŠ‚"ã€"æŒ‡ä»¤çº§"ã€‚
                3. null: ç”¨æˆ·æœªæ˜ç¡®æåŠåˆ†æç±»å‹ï¼Œæˆ–ç”¨æˆ·æ˜ç¡®æåŠäº†nsyså’Œncuä¸¤ç§åˆ†æç±»å‹ã€‚
            - **search_query**: æ”¹å†™åçš„â€œå¹²å‡€é—®é¢˜â€ï¼Œç”¨äºå‘é‡æ£€ç´¢ã€‚
                1. åªåœ¨rewrite_queryä¸ºtrueæ—¶æ”¹å†™ã€‚å¦‚æœrewrite_queryä¸ºfalseï¼Œåˆ™search_queryè¾“å‡ºä¸€ä¸ªç©ºå­—ç¬¦ä¸²ã€‚
                2. æ”¹å†™æ—¶ï¼šç§»é™¤æœ¬æ¬¡è¾“å‡º JSON ä¸­çš„ model/params/analysis_type ç­‰çº¦æŸä¿¡æ¯ï¼ˆä»¥åŠä¸å…¶è¯­ä¹‰ç­‰ä»·çš„é™å®šè¡¨è¾¾ï¼‰ï¼Œåªä¿ç•™ç”¨æˆ·è¦æŸ¥è¯¢çš„â€œçŸ¥è¯†ç‚¹/æŒ‡æ ‡/ç»“è®ºâ€ã€‚
                3. search_query å°½é‡çŸ­ï¼ˆ5~30 ä¸ªæ±‰å­—æˆ–å•è¯ï¼‰ã€‚å¦‚æœç”¨æˆ·é—®é¢˜æœ¬èº«å·²ç»å¾ˆå¹²å‡€ï¼Œå¯ä¸åŸé—®é¢˜ç­‰ä»·æˆ–æ›´çŸ­ã€‚
                4. ä¸è¦åœ¨ search_query ä¸­æ·»åŠ åŸé—®é¢˜é‡Œä¸å­˜åœ¨çš„ä¿¡æ¯ã€‚
                
            ### 2. å‚è€ƒç¤ºä¾‹
            user_query: "ç”¨nsyså’Œncuè·‘ä¸€ä¸‹qwen3-4bï¼Œbatch_sizeè®¾ä¸º1ï¼Œoutput_len=1"
            rewrite_query: false
            Output: {{"model": "qwen3-4b", "params": {{"batch_size": 1, "output_len": 1}}, "analysis_type": null, "search_query": ""}}

            user_query: "å¸®æˆ‘ç»™æ¨¡å‹åšä¸ªncuæ·±åº¦åˆ†æ"
            rewrite_query: false
            Output: {{"model": null, "params": {{}}, "analysis_type": "ncu", "search_query": ""}}
            
            user_query: "llama-7båœ¨batch_size=1ã€input_len=128çš„å…¨å±€åˆ†ææŠ¥å‘Šä¸­ï¼Œæ€»kernelæ•°æœ‰å¤šå°‘ï¼Ÿ"
            rewrite_query: true
            Output: {{"model": "llama-7b", "params": {{"batch_size": 1, "input_len": 128}}, "analysis_type": "nsys", "search_query": "æ€»kernelæ•°æœ‰å¤šå°‘ï¼Ÿ"}}

            user_query: "qwen-7båœ¨batch_size=16çš„æƒ…å†µä¸‹ç“¶é¢ˆæœ€å¤šçš„kernelæœ‰å“ªäº›ï¼Ÿ"
            rewrite_query: true
            Output: {{"model": "qwen-7b", "params": {{"batch_size": 16}}, "analysis_type": null, "search_query": "ç“¶é¢ˆæœ€å¤šçš„kernelæœ‰å“ªäº›ï¼Ÿ"}}
            
            ### 3. æœ¬æ¬¡æ˜¯å¦éœ€è¦æ”¹å†™queryï¼ˆéå¸¸é‡è¦ï¼‰
            {str(rewrite_query).lower()}

            ### 4. ç”¨æˆ·è¾“å…¥ï¼ˆä¸»è¦ä¾æ®ï¼‰
            {user_query}
            
            ### 5. æœ€è¿‘å¯¹è¯å†å²ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
            #### tips: ä»…å½“ç”¨æˆ·æåˆ°â€œä¹‹å‰/ä¸Šä¸€æ¬¡/ä¸Šä¸ªé—®é¢˜/åˆšæ‰/æ²¿ç”¨/è·Ÿå‰é¢ä¸€æ ·â€ç­‰éœ€è¦ä½ æŸ¥è¯¢å¯¹è¯å†å²çš„å…³é”®è¯æ—¶ï¼Œæ‰å¯ä»¥çº³å…¥å‚è€ƒï¼Œå¦åˆ™å¿½ç•¥å¯¹è¯å†å²
            {history_str if history_str else "å¯¹è¯å†å²ä¸ºç©ºã€‚"}

            ### 6. ä½ çš„JSONè¾“å‡ºï¼š
        """.strip()

        def _strip_code_fence(text: str) -> str:
            """å»é™¤ä»£ç å—æ ‡è®°"""
            text = text.strip()
            # å¤„ç†å¼€å¤´
            if text.startswith("```"):
                # æ‰¾åˆ°ç¬¬ä¸€è¡Œæ¢è¡Œç¬¦
                if "\n" in text:
                    text = text.split("\n", 1)[1]
                else:
                    # æå…¶ç½•è§æƒ…å†µï¼šåªæœ‰ ```json æ²¡æœ‰æ¢è¡Œ
                    text = text.lstrip("`").lstrip("json").strip()

            # å¤„ç†ç»“å°¾
            if text.endswith("```"):
                text = text[:-3]

            return text.strip()

        def _extract_first_json_object(text: str) -> Optional[str]:
            """
            ä»ä»»æ„æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡ï¼ˆä»ç¬¬ä¸€ä¸ª { å¼€å§‹åšæ‹¬å·é…å¯¹ï¼‰ã€‚
            åªåšè½»é‡æå–ï¼šä¸å¤„ç†å­—ç¬¦ä¸²å†…èŠ±æ‹¬å·çš„å¤æ‚æƒ…å½¢ï¼Œä½†å¯¹ LLM å¸¸è§è¾“å‡ºè¶³å¤Ÿç¨³ã€‚
            """
            s = text
            start = s.find("{")
            if start < 0:
                return None

            depth = 0
            for i in range(start, len(s)):
                if s[i] == "{":
                    depth += 1
                elif s[i] == "}":
                    depth -= 1
                    if depth == 0:
                        return s[start : i + 1]
            return None

        parsed_params: Dict[str, Any] = {}
        model_name: Optional[str] = None
        analysis_type: Optional[str] = None
        search_query: Optional[str] = None

        try:
            raw = self.llm_client.generate(
                prompt, max_tokens=512, mode="structured"
            ).strip()
            json_text = _strip_code_fence(raw)

            # å¦‚æœ raw ä¸æ˜¯çº¯ JSONï¼Œåˆ™å°è¯•æŠ½å–å…¶ä¸­ç¬¬ä¸€ä¸ª JSON å¯¹è±¡
            if not (json_text.startswith("{") and json_text.endswith("}")):
                extracted = _extract_first_json_object(json_text)
                if extracted:
                    json_text = extracted

            result = json.loads(json_text)
            model_name = result.get("model")
            parsed_params = result.get("params", {}) or {}
            raw_type = result.get("analysis_type")
            analysis_type = raw_type if raw_type in ("nsys", "ncu") else None
            search_query = (
                str(result.get("search_query", "")).strip() if rewrite_query else ""
            )
        except Exception as e:
            print(
                f"[_parse_raw_params] LLM parse failed: {e}. raw={locals().get('raw', None)!r}"
            )

        # è§„åˆ™å…œåº•ï¼šå°è¯•ä» query ä¸­æå–æ¨¡å‹åï¼ˆå¦‚æœ LLM æ²¡æ‹¿åˆ°ï¼‰
        if not model_name:
            q = user_query.lower()
            # æœ€é•¿ä¼˜å…ˆï¼Œé¿å…çŸ­åå­—è¯¯å‘½ä¸­
            for model in sorted(available_models, key=len, reverse=True):
                if model.lower() in q:
                    model_name = model
                    break

        # åˆå¹¶å‚æ•°ï¼Œåªä¿ç•™ç”¨æˆ·æåˆ°çš„å­—æ®µ
        final_params: Dict[str, int] = {}
        if isinstance(parsed_params, dict):
            for key in ("batch_size", "input_len", "output_len"):
                if key in parsed_params and parsed_params[key] not in (None, ""):
                    try:
                        final_params[key] = int(parsed_params[key])
                    except (ValueError, TypeError):
                        pass

        return {
            "model": model_name,
            "params": final_params,
            "analysis_type": analysis_type,
            "search_query": search_query,
        }

    def _finalize_params_for_analysis(self, raw: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹messageè§£æçš„å‚æ•°åšé»˜è®¤å€¼è¡¥å…¨ï¼Œç”¨äºæ€§èƒ½åˆ†æ"""

        defaults = {"batch_size": 1, "input_len": 128, "output_len": 1}
        params = {**defaults, **(raw.get("params") or {})}
        analysis_type = raw.get("analysis_type")

        return {
            "model": raw.get("model"),
            "params": params,
            "analysis_type": analysis_type,
        }

    def _finalize_params_for_rag_filter(
        self, raw: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """
        å°†messageè§£æçš„å‚æ•°è½¬åŒ–ä¸ºchromaè¯­æ³•çš„where_fileter
         - æŠŠè§£æå‡ºæ¥çš„æ‰€æœ‰å­—æ®µæ”¾åœ¨ä¸€ä¸ª dict ä¸­ï¼Œä½œä¸º where_filter
         - å¦‚æœvalueæ˜¯ None/null/ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™å¿½ç•¥è¯¥å­—æ®µ
         - _parse_raw_paramsè¿”å›çš„æ˜¯ä¸€ä¸ªåµŒå¥—å­—å…¸ï¼Œè€Œchromaçš„metadataæ˜¯æ‰å¹³çš„ï¼Œè½¬æ¢æˆwhere_filteræ—¶éœ€è¦æŠŠparamsæ‹å¹³
         - ç›®å‰åªå®ç°äº†=ï¼Œæš‚ä¸æ”¯æŒ>ã€<ç­‰èŒƒå›´æŸ¥è¯¢
        """

        if not isinstance(raw, dict):
            return None

        def is_empty(v: Any) -> bool:
            """åˆ¤æ–­å€¼æ˜¯å¦ä¸ºç©ºï¼ˆNoneã€ç©ºå­—ç¬¦ä¸²ã€"none"ã€"null"ï¼‰"""
            if v is None:
                return True
            if isinstance(v, str) and v.strip() in ("", "none", "null"):
                return True
            return False

        def coerce_scalar(v: Any) -> Any:
            """è¯•å›¾æŠŠå­—ç¬¦ä¸²å½¢å¼çš„æ•°å­—è½¬ä¸ºå®é™…æ ¼å¼ï¼Œä¾‹å¦‚ "128" -> 128 or "1.28" -> 1.28"""
            if isinstance(v, str):
                s = v.strip()
                # int
                if s.isdigit() or (s.startswith("-") and s[1:].isdigit()):
                    try:
                        return int(s)
                    except Exception:
                        return v
                # float
                try:
                    if "." in s:
                        return float(s)
                except Exception:
                    pass
            return v

        where: Dict[str, Any] = {}

        # 1) é¡¶å±‚å­—æ®µï¼ˆé™¤äº† paramsï¼‰
        for k, v in raw.items():
            if k == "params":
                continue
            if is_empty(v):
                continue
            where[k] = coerce_scalar(v)

        # 2) params æ‹å¹³
        params = raw.get("params")
        if isinstance(params, dict):
            for k, v in params.items():
                if is_empty(v):
                    continue
                where[k] = coerce_scalar(v)

        print(where)
        return where or None

    def _resolve_model_path(self, model_name: str) -> Optional[str]:
        """
        è§£æå¹¶è¿”å›æ¨¡å‹è·¯å¾„
        """

        if not model_name:
            return None
        # 1. å¦‚æœ model_mappings é…ç½®çš„æ˜¯ç»å¯¹è·¯å¾„ï¼Œåˆ™ç›´æ¥è¿”å›è¯¥è·¯å¾„
        if model_name in self.model_mappings:
            mapped_path = self.model_mappings[model_name]
            if Path(mapped_path).is_absolute():
                return mapped_path
            return str(self.models_path / mapped_path)

        # 2. å¦‚æœ model_mappings é…ç½®çš„æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œåˆ™ä¸ models_path æ‹¼æ¥æˆç»å¯¹è·¯å¾„ï¼Œè‹¥è·¯å¾„å­˜åœ¨åˆ™è¿”å›ï¼Œå¦åˆ™è¿”å›None
        if Path(model_name).exists():
            return model_name
        potential_path = self.models_path / model_name
        if potential_path.exists():
            return str(potential_path)
        return None

    async def _agent_analysis(self, message: str) -> str:
        """
        å¤„ç†æ€§èƒ½åˆ†ææ„å›¾ã€‚
        ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–æ¨¡å‹å’Œå‚æ•°ï¼Œå¯åŠ¨åˆ†ææµç¨‹ï¼Œå¹¶è¿”å›ç»“æœæ‘˜è¦æˆ–é”™è¯¯ä¿¡æ¯ã€‚
        """
        try:
            response = """âœ… **å·²è§£ææ‚¨çš„è¯·æ±‚**\n"""

            # Step 1: è§£æåˆ†æå‚æ•°ï¼ˆmodel + kwargsï¼‰
            parsed_raw = await self._parse_raw_params(message)
            parsed = self._finalize_params_for_analysis(parsed_raw)
            print(parsed)

            model_name = parsed.get("model")
            params = parsed.get("params", {})
            analysis_type = parsed.get("analysis_type", None)

            response += f"ğŸ¤– **æ¨¡å‹**: {model_name or 'æœªæŒ‡å®š'}\nğŸ”¬ **åˆ†æç±»å‹**: {analysis_type or 'æœªæŒ‡å®š (é»˜è®¤nsys+ncu)'}\nğŸ“Š **å‚æ•°**: {params}\n"

            available = ", ".join(self.model_mappings.keys())

            if not model_name or model_name not in self.model_mappings:
                return (
                    response
                    + f"âŒ **åˆ†æå¤±è´¥**: æœªæŒ‡å®šæ¨¡å‹æˆ–æ¨¡å‹ä¸å¯ç”¨ã€‚å¯ç”¨æ¨¡å‹ï¼š{available}"
                )

            # Step 2: æ‰§è¡Œåˆ†ææµç¨‹
            model_path = self._resolve_model_path(model_name)
            if not model_path:
                # æ˜ç¡®æŠ›å‡ºé”™è¯¯ï¼Œè®©ç”¨æˆ·çŸ¥é“æ˜¯æ¨¡å‹é…ç½®é—®é¢˜
                raise ValueError(
                    f"æ¨¡å‹è·¯å¾„è§£æå¤±è´¥: '{model_name}'ã€‚\n"
                    f"è¯·æ£€æŸ¥ config.yaml ä¸­çš„ 'model_mappings' æ˜¯å¦åŒ…å«è¯¥æ¨¡å‹ï¼Œ"
                    f"æˆ–è€…æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨äº: {self.models_path}"
                )

            analysis_result = await self._run_analysis(
                model_path=model_path,
                analysis_type=analysis_type,
                params=params,
            )
            return response + analysis_result

        except Exception as e:
            return response + f"âŒ **åˆ†ææ‰§è¡Œå¼‚å¸¸**: {str(e)}"

    async def _agent_rag_qa(self, message: str) -> str:
        """
        å¤„ç†ä¸“ä¸šé—®ç­”ï¼ˆRAG-QAï¼‰æ„å›¾ã€‚
        æ‰§è¡ŒçŸ¥è¯†åº“æ£€ç´¢ï¼Œå¹¶åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆä¸¥è°¨ã€æœ‰ä¾æ®çš„å›ç­”ã€‚
        """

        # Step 1: æ„å»ºwhere_filterï¼Œæ”¹å†™queryï¼Œæ£€ç´¢ç›¸å…³çŸ¥è¯†ç‰‡æ®µ
        parsed_raw = await self._parse_raw_params(message, rewrite_query=True)
        where_filter = self._finalize_params_for_rag_filter(
            {k: v for k, v in parsed_raw.items() if k != "search_query"}
        )
        search_query = parsed_raw.get("search_query") or message
        retrieved_contexts = self.kb.search(
            query=search_query, where_filter=where_filter
        )
        # debugç”¨ï¼šæ‰“å°é‡å†™åçš„queryå’Œç”¨äºå…ƒæ•°æ®è¿‡æ»¤çš„where_filter
        # print(f"search_query: {search_query}")
        # print(f"where_filter: {where_filter}")

        # Step 2: æ„å»º RAG ä¸Šä¸‹æ–‡å’Œå†å²å¯¹è¯
        rag_context = ""
        if retrieved_contexts:
            rag_snippets = [
                f"ã€æ–‡æ¡£ç‰‡æ®µ {i + 1}ã€‘\n{res['content']}"
                for i, res in enumerate(retrieved_contexts)
            ]
            rag_context = "\n\n".join(rag_snippets)
        # debugç”¨ï¼šæ‰“å°RAGå¬å›ç»“æœ
        # print(rag_context)

        history_str = self._format_history_str(self.chat_history, limit=1)

        # Step 3: ä¸¥æ ¼çº¦æŸçš„ RAG-QA ç”Ÿæˆ
        prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ•°æ®åˆ†æå‘˜ã€‚ä½ å¿…é¡»å®Œå…¨ä¾æ®ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ç”¨æˆ·å…³äº GPU æ€§èƒ½æ•°æ®çš„æé—®ã€‚

            ### å‚è€ƒèµ„æ–™
            {rag_context if rag_context else "ï¼ˆè­¦å‘Šï¼šæœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå¯èƒ½éœ€è¦å‘ŠçŸ¥ç”¨æˆ·èµ„æ–™ç¼ºå¤±ï¼‰"}

            ### ç”¨æˆ·é—®é¢˜ï¼ˆä¸»è¦çš„ç”¨æˆ·æ„å›¾ä¾æ®ï¼‰
            {message}

            ### å¯¹è¯å†å²ï¼ˆæ¬¡è¦ã€å¯å¿½ç•¥çš„ç”¨æˆ·æ„å›¾è¡¥å……ï¼‰
            {history_str if history_str else "ï¼ˆæ— å†å²è®°å½•ï¼‰"}

            ### ä¸¥æ ¼çº¦æŸ (Strict Rules)
            1. **æ•°æ®ç²¾ç¡®æ€§**ï¼šå¦‚æœç”¨æˆ·è¯¢é—®æŸä¸ª Kernel çš„å…·ä½“æŒ‡æ ‡ï¼ˆå¦‚ç“¶é¢ˆæ•°ã€å¸¦å®½ï¼‰ï¼Œ**å¿…é¡»**åœ¨å‚è€ƒèµ„æ–™ä¸­æ‰¾åˆ°**å®Œå…¨åŒ¹é…**çš„ Kernel åç§°åæ‰èƒ½å›ç­”ã€‚
            2. **æ‹’ç»çŒœæµ‹**ï¼šå¦‚æœèµ„æ–™é‡Œæœ‰ "Kernel A" å’Œ "Kernel B"ï¼Œä½†ç”¨æˆ·é—® "Kernel C"ï¼Œä½ å¿…é¡»å›ç­”ï¼š"èµ„æ–™ä¸­æœªæ‰¾åˆ° Kernel C çš„æ•°æ®"ã€‚**ä¸¥ç¦**æŠŠ A çš„æ•°æ®å®‰åœ¨ C ä¸Šã€‚
            3. **åŸæ–‡å¼•ç”¨**ï¼šå›ç­”æ—¶å°½é‡ä½¿ç”¨èµ„æ–™ä¸­çš„åŸè¯æˆ–æ•°æ®ã€‚
            4. **ç©ºå€¼å¤„ç†**ï¼šå¦‚æœèµ„æ–™ä¸ºç©ºæˆ–ä¸ç›¸å…³ï¼Œç›´æ¥å›ç­”ï¼šâ€œæŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ã€‚â€

            ### å›ç­”ï¼š
        """.strip()

        try:
            answer = self.llm_client.generate(prompt, max_tokens=1024).strip()
            ref_count = len(retrieved_contexts) if retrieved_contexts else 0
            return f"ğŸ¤– **RAG-QA**\n{answer}\n\n---\nğŸ’¡ *åŸºäº {ref_count} æ¡çŸ¥è¯†åº“ç‰‡æ®µå›ç­”*"
        except Exception as e:
            return f"âŒ **RAG-QAç”Ÿæˆå¤±è´¥**: {str(e)}"

    async def _agent_chat(self, message: str) -> str:
        """
        é—²èŠæ¨¡å¼Agent
        """

        # æ„å»ºå†å²å¯¹è¯
        history_str = self._format_history_str(self.chat_history)

        prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI æ€§èƒ½åˆ†æä¸“å®¶ã€‚
            ä¸è¦èƒ¡ç¼–ä¹±é€ æŠ€æœ¯æ•°æ®ï¼Œå¯ä»¥è¿›è¡Œç®€çŸ­çš„æ—¥å¸¸å¯¹è¯ã€‚
            
            å¯¹è¯å†å²ï¼ˆå¯å¿½ç•¥çš„ç”¨æˆ·æ„å›¾è¡¥å……ï¼‰
            {history_str if history_str else "ï¼ˆæ— å†å²è®°å½•ï¼‰"}
            
            ç”¨æˆ·: {message}
            
            ä½ çš„å›å¤:
        """
        raw = self.llm_client.generate(prompt, max_tokens=256).strip()
        return f"ğŸ¤– **é—²èŠæ¨¡å¼**\n{raw}"

    async def process_message(self, message: str, intent: str = "auto") -> str:
        """
        Agentic-RAG æ„å›¾è·¯ç”±:
        1. [Router] æ„å›¾è¯†åˆ« â†’ è¿”å›ä¸€ä¸ªintent_mappingsä¸­çš„key ï¼ˆä¾‹å¦‚ "analysis" | "rag-qa" | "chat"ï¼‰
        2. [Branch] æ ¹æ®æ„å›¾è¿›è¡Œåˆ†æ”¯å¤„ç†
        3. [History] ä¿å­˜å¯¹è¯å†å²
        """

        # Step1: æ„å›¾è¯†åˆ«ï¼ˆè¿”å›ä¸€ä¸ªintent_mappingsä¸­çš„keyï¼‰
        try:
            intent = await self._parse_intent(message, intent)
        except Exception as e:
            return f"âŒ **æ„å›¾è¯†åˆ«å¤±è´¥**: {str(e)}"

        response_text = ""

        # Step2: æ ¹æ®æ„å›¾è·¯ç”±åˆ°å¯¹åº”agent
        if intent == "analysis":
            # === åˆ†æ”¯ A: æ€§èƒ½åˆ†æ (Analysis) ===
            print("[analysis] è¯†åˆ«ä¸ºåˆ†ææ„å›¾")
            response_text = await self._agent_analysis(message)

        elif intent == "rag-qa":
            # === åˆ†æ”¯ B: RAGé—®ç­” (RAG-QA) ===
            print("[rag-qa] è¯†åˆ«ä¸ºRAGé—®ç­”æ„å›¾")
            response_text = await self._agent_rag_qa(message)

        elif intent == "chat":
            # === åˆ†æ”¯ C: é—²èŠ (Chat) ===
            print("[chat] è¯†åˆ«ä¸ºé—²èŠæ„å›¾")
            response_text = await self._agent_chat(message)

        else:
            # ç†è®ºä¸Šä¸ä¼šèµ°åˆ°è¿™é‡Œ
            response_text = "â“ æ— æ³•ç†è§£æ‚¨çš„æ„å›¾ï¼Œè¯·æ¢ç§æ–¹å¼æé—®ã€‚"

        # Step3: ä¿å­˜å¯¹è¯å†å²ï¼Œå¦‚æœintent=analysisï¼Œåˆ™åªè¿”å›æ‘˜è¦
        self.chat_history.append({"role": "user", "content": message})

        history_response = (
            response_text if intent != "analysis" else "å·²å®Œæˆæ€§èƒ½åˆ†æä»»åŠ¡ã€‚"
        )
        self.chat_history.append({"role": "assistant", "content": history_response})

        return response_text

    # å·²æ•´åˆè¿›_agent_analysis()
    async def _execute_analysis_flow(
        self, model_name: str, analysis_type: str, params: Dict
    ) -> str:
        model_path = self._resolve_model_path(model_name)
        if not model_path:
            # æ˜ç¡®æŠ›å‡ºé”™è¯¯ï¼Œè®©ç”¨æˆ·çŸ¥é“æ˜¯æ¨¡å‹é…ç½®é—®é¢˜
            raise ValueError(
                f"æ¨¡å‹è·¯å¾„è§£æå¤±è´¥: '{model_name}'ã€‚\n"
                f"è¯·æ£€æŸ¥ config.yaml ä¸­çš„ 'model_mappings' æ˜¯å¦åŒ…å«è¯¥æ¨¡å‹ï¼Œ"
                f"æˆ–è€…æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨äº: {self.models_path}"
            )
        return await self._run_analysis(
            model_path=model_path, analysis_type=analysis_type, params=params
        )

    async def _run_analysis(
        self, model_path: str, analysis_type: str, params: Dict
    ) -> str:
        """æ‰§è¡Œå®é™…çš„æ€§èƒ½åˆ†æ"""

        results = []

        # é‡ç½®ç¼“å­˜ï¼Œé˜²æ­¢ä½¿ç”¨é™ˆæ—§ç»“æœ
        self.last_analysis_table = None
        self.last_analysis_reports = []
        self.last_analysis_dirs = []
        self.last_analysis_dir = None
        self.last_analysis_suggestions = None
        self.last_roofline_estimate = None

        # è·å–å‚æ•°ç»„åˆ
        batch_sizes = params.get("batch_size", [1])
        input_lens = params.get("input_len", [128])
        output_lens = params.get("output_len", [1])

        # åªåˆ†æç¬¬ä¸€ç»„å‚æ•°ï¼ˆé¿å…æ—¶é—´è¿‡é•¿ï¼‰
        batch_size = batch_sizes[0] if isinstance(batch_sizes, list) else batch_sizes
        input_len = input_lens[0] if isinstance(input_lens, list) else input_lens
        output_len = output_lens[0] if isinstance(output_lens, list) else output_lens

        precision_cfg = (
            self.analysis_defaults.get("precision", {})
            if isinstance(self.analysis_defaults.get("precision", {}), dict)
            else {}
        )

        def _parse_int(value, default):
            try:
                return int(value)
            except (TypeError, ValueError):
                return default

        w_bit = _parse_int(precision_cfg.get("w_bit"), 16)
        a_bit = _parse_int(precision_cfg.get("a_bit"), 16)
        kv_bit_candidate = precision_cfg.get("kv_bit")
        parsed_kv_bit = (
            _parse_int(kv_bit_candidate, None) if kv_bit_candidate is not None else None
        )
        kv_bit = parsed_kv_bit if isinstance(parsed_kv_bit, int) else None
        use_flashattention = bool(precision_cfg.get("use_flashattention", False))
        hardware_key = (
            self.analysis_defaults.get("hardware")
            or os.getenv("ROOFLINE_HARDWARE")
            or "nvidia_H800_SXM5_80G"
        )

        try:
            # åˆ›å»ºåˆ†æå·¥ä½œæµ
            analysis_workflow = create_sglang_analysis_workflow()

            # æ‰§è¡Œåˆ†æ
            loop = asyncio.get_event_loop()
            workflow_callable = partial(
                analysis_workflow,
                str(model_path),
                batch_size,
                input_len,
                output_len,
                True,
                None,
                hardware_key,
                w_bit,
                a_bit,
                kv_bit,
                use_flashattention,
            )
            workflow_output = await loop.run_in_executor(None, workflow_callable)

            run_records: List[Tuple[str, Path]] = []
            if isinstance(workflow_output, list):
                for idx, item in enumerate(workflow_output):
                    gpu_label: str
                    output_path: Optional[str] = None
                    if isinstance(item, dict):
                        gpu_label = str(item.get("gpu", idx))
                        output_path = item.get("dir") or item.get("path")
                    else:
                        gpu_label = str(idx)
                        output_path = str(item)
                    if output_path:
                        run_records.append((gpu_label, Path(output_path)))
            elif workflow_output:
                run_records.append(("0", Path(str(workflow_output))))

            if not run_records:
                results.append("âš ï¸ **åˆ†æå·²å®Œæˆï¼Œä½†æœªæ‰¾åˆ°è¾“å‡ºç›®å½•**")
                return "\n".join(results)

            self.last_analysis_dirs = [str(path) for _, path in run_records]

            report_infos = []
            roofline_infos: List[Tuple[str, Path, Dict[str, Any]]] = []
            missing_reports = []
            for idx, (gpu_label, output_dir) in enumerate(run_records):
                report_path = output_dir / "integrated_performance_report.md"
                if report_path.exists():
                    report_text = report_path.read_text(encoding="utf-8")
                    report_infos.append(
                        {
                            "gpu": gpu_label,
                            "dir": output_dir,
                            "report": report_path,
                            "text": report_text,
                            "index": idx,
                        }
                    )
                    roofline_path = output_dir / "roofline_estimate.json"
                    if roofline_path.exists():
                        try:
                            with open(roofline_path, "r", encoding="utf-8") as rf:
                                roofline_data = json.load(rf)
                            roofline_infos.append(
                                (gpu_label, roofline_path, roofline_data)
                            )
                        except Exception as roof_exc:
                            print(
                                f"âš ï¸ è¯»å– Roofline é¢„æµ‹å¤±è´¥ ({roofline_path}): {roof_exc}"
                            )
                else:
                    missing_reports.append(output_dir)

            if not report_infos:
                dir_lines = "\n".join(f"  â€¢ {path}" for _, path in run_records)
                results.append(f"""
âš ï¸ **åˆ†æå·²å®Œæˆï¼Œä½†æœªç”ŸæˆæŠ¥å‘Šæ–‡ä»¶**

ğŸ“ ç»“æœç›®å½•:
{dir_lines}
ğŸ’¡ è¯·æ£€æŸ¥ç›®å½•ä¸­çš„å…¶ä»–è¾“å‡ºæ–‡ä»¶
""")
                return "\n".join(results)

            primary_info = report_infos[0]
            self.last_analysis_dir = str(primary_info["dir"])
            self.last_analysis_reports = [str(info["report"]) for info in report_infos]
            summary = self._extract_report_summary(primary_info["text"])

            try:
                loop = asyncio.get_event_loop()
                if len(report_infos) > 1:
                    table_markdown = self._generate_multi_gpu_table(
                        [info["text"] for info in report_infos],
                        [info["gpu"] for info in report_infos],
                    )
                else:
                    table_markdown = await loop.run_in_executor(
                        None, self._generate_report_table, primary_info["text"]
                    )
            except Exception as table_exc:
                table_markdown = f"âš ï¸ è¡¨æ ¼ç”Ÿæˆå¤±è´¥: {table_exc}"

            self.last_analysis_table = table_markdown

            suggestions = self._generate_optimization_suggestions(report_infos)
            self.last_analysis_suggestions = suggestions if suggestions else None

            dir_lines = "\n".join(
                f"  â€¢ {self._format_gpu_label(info['gpu'], info['index'])}: {info['dir']}"
                for info in report_infos
            )

            missing_lines = ""
            if missing_reports:
                missing_lines = "\n".join(f"  â€¢ {path}" for path in missing_reports)
                missing_lines = f"\nâš ï¸ æœªæ‰¾åˆ°ä»¥ä¸‹ç›®å½•çš„æŠ¥å‘Šæ–‡ä»¶:\n{missing_lines}\n"

            roofline_section = "ğŸ“ **Roofline é¢„æµ‹**:\næš‚æœªç”Ÿæˆ Roofline é¢„æµ‹\n"
            if roofline_infos:
                self.last_roofline_estimate = roofline_infos[0][2]
                roofline_preview = self._render_roofline_preview(
                    self.last_roofline_estimate
                )
                roofline_source = str(roofline_infos[0][1])
                roofline_section = f"ğŸ“ **Roofline é¢„æµ‹** (æ¥æº: {roofline_source}):\n{roofline_preview}\n"

            results.append(f"""
âœ… **åˆ†æå®Œæˆ!**

ğŸ“ **ç»“æœç›®å½•**:
{dir_lines}
ğŸ“„ **æŠ¥å‘Šæ–‡ä»¶**: {primary_info["report"]}
{missing_lines}
{summary}

{roofline_section}

ğŸ“Œ **çƒ­ç‚¹Kernelè¡¨æ ¼é¢„è§ˆ**:
{table_markdown}

ğŸ’¡ **ä¼˜åŒ–å»ºè®®**:
{suggestions or "æš‚æœªç”Ÿæˆä¼˜åŒ–å»ºè®®"}

ğŸ” **è¯¦ç»†æŠ¥å‘Š**: è¯·æŸ¥çœ‹ {primary_info["report"]}
ğŸ“Š **å¯è§†åŒ–å›¾è¡¨**: è¯·æŸ¥çœ‹å¯¹åº”ç»“æœç›®å½•ä¸­çš„å›¾ç‰‡æ–‡ä»¶
""")

        except Exception as e:
            import traceback

            error_detail = traceback.format_exc()
            results.append(f"""
âŒ **åˆ†ææ‰§è¡Œå¤±è´¥**

é”™è¯¯ä¿¡æ¯: {str(e)}

è¯¦ç»†é”™è¯¯:
```
{error_detail}
```

ğŸ’¡ **å¸¸è§é—®é¢˜è§£å†³**:
1. ç¡®ä¿å·²å®‰è£… nsys å’Œ ncu å·¥å…·
2. ç¡®ä¿ SGlang å·²æ­£ç¡®å®‰è£…
3. ç¡®ä¿æ¨¡å‹æ–‡ä»¶è·¯å¾„æ­£ç¡®
4. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜
""")

        return "\n".join(results)

    def _render_roofline_preview(self, roofline: Dict[str, Any]) -> str:
        if not roofline:
            return "æš‚æœªç”Ÿæˆ Roofline æ•°æ®"

        def _fmt_time(seconds: Optional[float]) -> str:
            if seconds is None:
                return "N/A"
            if isinstance(seconds, (int, float)):
                if seconds == float("inf"):
                    return "âˆ"
                return f"{seconds * 1000:.3f} ms"
            return str(seconds)

        def _fmt_perf(value: Optional[float]) -> str:
            if not isinstance(value, (int, float)):
                return "N/A"
            return f"{value / 1e12:.2f} TOPS"

        def _fmt_ai(value: Optional[float]) -> str:
            if not isinstance(value, (int, float)):
                return "N/A"
            return f"{value:.2f} OPs/Byte"

        def _fmt_mem(value: Optional[float]) -> str:
            if not isinstance(value, (int, float)):
                return "N/A"
            return f"{value / 1e9:.2f} GB"

        bits = roofline.get("precision_bits", {})
        params = roofline.get("params", {})
        prefill = roofline.get("prefill", {})
        decode = roofline.get("decode", {})
        overall = roofline.get("overall", {})
        observed = (
            roofline.get("observed", {})
            if isinstance(roofline.get("observed"), dict)
            else {}
        )
        comparison = (
            roofline.get("comparison", {})
            if isinstance(roofline.get("comparison"), dict)
            else {}
        )

        lines = []
        lines.append(f"- æ¨¡å‹ç¡¬ä»¶: {roofline.get('hardware', 'unknown')}")
        lines.append(
            f"- ç²¾åº¦: W{bits.get('w', 'N/A')} / A{bits.get('a', 'N/A')} / KV{bits.get('kv', 'N/A')}"
        )
        lines.append(
            f"- Prefill: å¼ºåº¦ {_fmt_ai(prefill.get('arithmetic_intensity'))}, "
            f"æ€§èƒ½ {_fmt_perf(prefill.get('performance'))}, "
            f"è€—æ—¶ {_fmt_time(prefill.get('inference_time'))}, "
            f"å†…å­˜è®¿é—® {_fmt_mem(prefill.get('memory_access'))}"
        )
        lines.append(
            f"- Decode(å• token): å¼ºåº¦ {_fmt_ai(decode.get('arithmetic_intensity'))}, "
            f"æ€§èƒ½ {_fmt_perf(decode.get('performance'))}, "
            f"è€—æ—¶ {_fmt_time(decode.get('inference_time'))}"
        )
        lines.append(
            f"- æ€»ä½“ä¼°è®¡: å¼ºåº¦ {_fmt_ai(overall.get('arithmetic_intensity'))}, "
            f"æ€§èƒ½ {_fmt_perf(overall.get('performance'))}, "
            f"æ€»è€—æ—¶ {_fmt_time(overall.get('inference_time'))}"
        )
        if params:
            lines.append(
                f"- åˆ†æå‚æ•°: batch={params.get('batch_size')}, prompt={params.get('prompt_len')}, output={params.get('output_len')}"
            )
        if observed:
            avg_sm = observed.get("avg_sm_efficiency")
            avg_sm_str = f"{avg_sm:.1f}%" if isinstance(avg_sm, (int, float)) else "N/A"
            mem_gbps = observed.get("observed_memory_throughput_gbps")
            mem_str = (
                f"{mem_gbps:.1f} GB/s" if isinstance(mem_gbps, (int, float)) else "N/A"
            )
            lines.append(
                f"- å®æµ‹: SMæ•ˆç‡ {avg_sm_str}, å†…å­˜ {mem_str}, "
                f"ç®—æœ¯å¼ºåº¦ {_fmt_ai(observed.get('observed_arithmetic_intensity'))}, è®¡ç®— {_fmt_perf(observed.get('observed_compute_flops'))}"
            )
            util_c = observed.get("compute_utilization")
            util_m = observed.get("memory_utilization")
            util_parts = []
            if isinstance(util_c, (int, float)):
                util_parts.append(f"è®¡ç®—åˆ©ç”¨ç‡ {util_c * 100:.1f}%")
            if isinstance(util_m, (int, float)):
                util_parts.append(f"å†…å­˜åˆ©ç”¨ç‡ {util_m * 100:.1f}%")
            if util_parts:
                lines.append(f"- åˆ©ç”¨ç‡: {'ï¼Œ'.join(util_parts)}")
        if comparison:
            perf_gap = comparison.get("performance_gap_ratio")
            ai_gap = comparison.get("arithmetic_intensity_gap_ratio")
            bound_alignment = comparison.get("bound_alignment")
            gap_parts = []
            if isinstance(perf_gap, (int, float)):
                gap_parts.append(f"æ€§èƒ½å·® {perf_gap * 100:.1f}%")
            if isinstance(ai_gap, (int, float)):
                gap_parts.append(f"å¼ºåº¦å·® {ai_gap * 100:.1f}%")
            if bound_alignment:
                gap_parts.append(f"è¾¹ç•Œ {bound_alignment}")
            if gap_parts:
                lines.append(f"- ä¸é¢„æµ‹å·®å¼‚: {'ï¼Œ'.join(gap_parts)}")
        return "\n".join(lines)

    @staticmethod
    def _generate_report_table(report_text: str) -> str:
        client = get_offline_qwen_client(OFFLINE_QWEN_PATH)
        return client.report_to_table(report_text)

    @staticmethod
    def _collect_ncu_csv_snippets(
        output_dir: Path, limit: int = 1200
    ) -> List[Tuple[str, str]]:
        snippets: List[Tuple[str, str]] = []
        if not output_dir.exists():
            return snippets
        for csv_path in sorted(output_dir.glob("ncu_kernel*.csv")):
            try:
                raw = csv_path.read_text(encoding="utf-8", errors="ignore")
            except Exception:
                continue
            snippet = raw[:limit]
            if snippet.strip():
                snippets.append((csv_path.name, snippet))
        return snippets

    def _generate_optimization_suggestions(
        self, report_infos: List[Dict[str, str]]
    ) -> str:
        if not report_infos:
            return ""

        try:
            client = get_offline_qwen_client(OFFLINE_QWEN_PATH)
        except Exception as exc:
            return f"âš ï¸ ä¼˜åŒ–å»ºè®®ç”Ÿæˆå¤±è´¥: {exc}"

        labeled_reports: List[Tuple[str, str]] = []
        raw_snippets: List[Tuple[str, str]] = []
        for info in report_infos:
            label = self._format_gpu_label(info["gpu"], info["index"])
            labeled_reports.append((label, info.get("text", "")))
            output_dir = Path(info["dir"])
            for name, data in self._collect_ncu_csv_snippets(output_dir):
                raw_snippets.append((f"{label} / {name}", data))

        output_sections: List[str] = []

        try:
            if labeled_reports:
                suggestions = client.suggest_optimizations(labeled_reports)
                if suggestions:
                    output_sections.append(suggestions)
        except Exception as exc:
            output_sections.append(f"âš ï¸ ä¼˜åŒ–å»ºè®®ç”Ÿæˆå¤±è´¥: {exc}")

        try:
            if raw_snippets:
                raw_suggestions = client.suggest_raw_data_optimizations(
                    raw_snippets, max_new_tokens=1024
                )
                if raw_suggestions:
                    output_sections.append(f"ğŸ“Š åŸå§‹æ•°æ®å»ºè®®:\n{raw_suggestions}")
        except Exception as exc:
            output_sections.append(f"âš ï¸ åŸå§‹æ•°æ®å»ºè®®ç”Ÿæˆå¤±è´¥: {exc}")

        return "\n\n".join(output_sections).strip()

    def _generate_multi_gpu_table(
        self, report_texts: List[str], gpu_labels: List[str]
    ) -> str:
        if not report_texts:
            return "âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„æŠ¥å‘Šå†…å®¹"

        formatted_labels: List[str] = []
        per_gpu_tables: List[Tuple[str, str]] = []
        for idx, report_text in enumerate(report_texts):
            raw_label = gpu_labels[idx] if idx < len(gpu_labels) else str(idx)
            label = self._format_gpu_label(raw_label, idx)
            formatted_labels.append(label)
            try:
                table_md = self._generate_report_table(report_text)
            except Exception:
                table_md = ""
            per_gpu_tables.append((label, table_md))

        try:
            client = get_offline_qwen_client(OFFLINE_QWEN_PATH)
            if any(table for _, table in per_gpu_tables):
                merged = client.merge_gpu_tables(per_gpu_tables)
                if merged and merged.count("|") >= len(formatted_labels) * 2:
                    return merged
        except Exception:
            pass

        return self._generate_multi_gpu_table_python(report_texts, gpu_labels)

    def _generate_multi_gpu_table_python(
        self, report_texts: List[str], gpu_labels: List[str]
    ) -> str:
        if not report_texts:
            return "âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„æŠ¥å‘Šå†…å®¹"

        parsed_entries = [
            self._parse_kernel_entries_from_report(text) for text in report_texts
        ]
        if not parsed_entries or not parsed_entries[0]:
            return "âš ï¸ æœªèƒ½è§£æå¤šGPUè¡¨æ ¼æ•°æ®"

        label_cells = [
            self._format_gpu_label(lbl, idx) for idx, lbl in enumerate(gpu_labels)
        ]
        header_cells = ["Kernel"]
        last_index = len(label_cells) - 1
        for idx_lbl, lbl in enumerate(label_cells):
            header_cells.append(f"{lbl} Duration(ms)")
            add_ratio = len(label_cells) == 1 or idx_lbl != last_index
            if add_ratio:
                header_cells.append(f"{lbl} Ratio(%)")
        if len(label_cells) >= 2:
            header_cells.append(f"{label_cells[0]}ï¼š{label_cells[1]} æ—¶é—´å æ¯”")

        header = "| " + " | ".join(header_cells) + " |"
        divider = "| " + " | ".join(["---"] * len(header_cells)) + " |"

        max_len = max(len(entries) for entries in parsed_entries)
        rows = []

        def _parse_duration(val: str) -> float:
            try:
                return float(val)
            except (TypeError, ValueError):
                return 0.0

        def _parse_ratio_component(val: str) -> Optional[Fraction]:
            if val is None:
                return None
            text = str(val).strip()
            if not text:
                return None
            try:
                frac = Fraction(text)
            except (ValueError, ZeroDivisionError):
                return None
            if frac < 0:
                return None
            return frac

        def _fractions_to_ints(fracs: List[Optional[Fraction]]) -> List[Optional[int]]:
            positives = [f for f in fracs if isinstance(f, Fraction) and f > 0]
            scale = None
            if positives:
                scale = positives[0].denominator
                for frac in positives[1:]:
                    scale = lcm(scale, frac.denominator)
            ints: List[Optional[int]] = []
            for frac in fracs:
                if frac is None:
                    ints.append(None)
                elif frac == 0:
                    ints.append(0)
                else:
                    if scale is None:
                        scale = frac.denominator
                    ints.append(frac.numerator * (scale // frac.denominator))
            if positives:
                common = None
                for val in ints:
                    if isinstance(val, int) and val > 0:
                        common = val if common is None else gcd(common, val)
                if common and common > 1:
                    ints = [
                        val // common if isinstance(val, int) and val > 0 else val
                        for val in ints
                    ]
            return ints

        for idx in range(max_len):
            name_candidates = []
            for entries in parsed_entries:
                if idx < len(entries) and entries[idx]["name"]:
                    name_candidates.append(entries[idx]["name"])
            base_name = name_candidates[0] if name_candidates else f"Kernel {idx + 1}"
            alt_names = {nm for nm in name_candidates if nm != base_name}
            if alt_names:
                merged_name = base_name + " / " + " / ".join(sorted(alt_names))
            else:
                merged_name = base_name

            row_cells = [merged_name]
            duration_values: List[float] = []
            pair_ratios: Optional[List[Optional[Fraction]]] = (
                [None, None] if len(parsed_entries) >= 2 else None
            )
            for gpu_idx, entries in enumerate(parsed_entries):
                if idx < len(entries):
                    duration = entries[idx]["duration"]
                    ratio = entries[idx]["ratio"]
                    row_cells.append(duration)
                    duration_values.append(_parse_duration(duration))
                    add_ratio = len(parsed_entries) == 1 or gpu_idx != last_index
                    if add_ratio:
                        row_cells.append(ratio)
                    if pair_ratios is not None and gpu_idx < 2:
                        pair_ratios[gpu_idx] = _parse_ratio_component(ratio)
                else:
                    row_cells.append("")
                    add_ratio = len(parsed_entries) == 1 or gpu_idx != last_index
                    if add_ratio:
                        row_cells.append("")
                    if pair_ratios is not None and gpu_idx < 2:
                        pair_ratios[gpu_idx] = Fraction(0, 1)
            if pair_ratios is not None:
                simplified_ints = _fractions_to_ints(pair_ratios)
                pair_strings = []
                for val in simplified_ints:
                    if val is None:
                        pair_strings.append("")
                    else:
                        pair_strings.append(str(val))
                combined = (
                    f"{pair_strings[0]}ï¼š{pair_strings[1]}" if any(pair_strings) else ""
                )
                row_cells.append(combined)
            sort_key = max(duration_values) if duration_values else 0.0
            rows.append((sort_key, row_cells))
        sorted_rows = [
            "| " + " | ".join(cells) + " |"
            for _, cells in sorted(rows, key=lambda item: item[0], reverse=True)
        ]

        return "\n".join([header, divider, *sorted_rows])

    def _parse_kernel_entries_from_report(
        self, report_text: str
    ) -> List[Dict[str, str]]:
        entries: List[Dict[str, str]] = []
        lines = report_text.splitlines()
        idx = 0
        total_lines = len(lines)
        while idx < total_lines:
            raw_line = lines[idx]
            if raw_line.strip().startswith("äºŒã€"):
                break
            match = re.match(r"^\s*\d+\.\s+(.*)$", raw_line)
            if match:
                name = match.group(1).strip()
                duration = ""
                ratio = ""
                idx += 1
                while idx < total_lines:
                    line = lines[idx].strip()
                    if line.startswith("- æ‰§è¡Œæ—¶é—´"):
                        dur_match = re.search(r"([0-9.]+)\s*ms", line)
                        if dur_match:
                            duration = dur_match.group(1)
                    elif line.startswith("- æ—¶é—´å æ¯”"):
                        ratio_match = re.search(r"([0-9.]+)\s*%", line)
                        if ratio_match:
                            ratio = ratio_match.group(1)
                    elif re.match(r"^\s*\d+\.", lines[idx]) or line.startswith("äºŒã€"):
                        break
                    idx += 1
                entries.append({"name": name, "duration": duration, "ratio": ratio})
            else:
                idx += 1
        return entries

    @staticmethod
    def _format_gpu_label(label: str, index: int) -> str:
        if not label:
            return f"GPU{index}"
        normalized = label.strip()
        if not normalized:
            return f"GPU{index}"
        if normalized.lower().startswith("gpu"):
            return normalized.upper()
        return f"GPU{normalized}"

    def _extract_report_summary(self, report_content: str) -> str:
        """ä»æŠ¥å‘Šä¸­æå–å…³é”®æ‘˜è¦ä¿¡æ¯"""

        lines = report_content.split("\n")
        summary_lines = []

        # æå–å…³é”®ç»Ÿè®¡ä¿¡æ¯
        for i, line in enumerate(lines):
            if "æ€»kernelsæ•°é‡" in line or "æ€»kernelæ‰§è¡Œæ—¶é—´" in line:
                summary_lines.append(line)
            elif "ğŸ”¥ è¯†åˆ«çš„çƒ­ç‚¹Kernels" in line:
                # æå–å‰3ä¸ªçƒ­ç‚¹kernel
                summary_lines.append("\n**ğŸ”¥ çƒ­ç‚¹Kernels (Top 3):**")
                for j in range(i + 1, min(i + 10, len(lines))):
                    if lines[j].strip() and lines[j].startswith(("1.", "2.", "3.")):
                        summary_lines.append(lines[j][:100])
                break

        if summary_lines:
            return "\n".join(summary_lines)
        else:
            return "**ğŸ“Š åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æ–‡ä»¶**"

    def _resolve_model_path(self, model_name: str) -> Optional[str]:
        """è§£ææ¨¡å‹è·¯å¾„"""

        # æ£€æŸ¥æ˜¯å¦åœ¨æ˜ å°„è¡¨ä¸­
        if model_name in self.model_mappings:
            mapped_path = self.model_mappings[model_name]

            # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥è¿”å›
            if Path(mapped_path).is_absolute():
                return mapped_path

            # å¦åˆ™ï¼Œç›¸å¯¹äº models_path
            full_path = self.models_path / mapped_path
            return str(full_path)

        # å¦‚æœä¸åœ¨æ˜ å°„è¡¨ä¸­ï¼Œå°è¯•ç›´æ¥ä½œä¸ºè·¯å¾„
        if Path(model_name).exists():
            return model_name

        # å°è¯•ç›¸å¯¹äº models_path
        potential_path = self.models_path / model_name
        if potential_path.exists():
            return str(potential_path)

        return None

    # æ”¹ä¸ºLLMè¯†åˆ«
    def _extract_model_name(self, prompt: str) -> Optional[str]:
        """æå–æ¨¡å‹åç§°"""

        # é¦–å…ˆæ£€æŸ¥å·²çŸ¥çš„æ¨¡å‹åˆ«å
        for model_name in self.model_mappings.keys():
            if model_name.lower() in prompt.lower():
                return model_name

        # ç„¶åä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…é€šç”¨æ¨¡å‹åç§°æ¨¡å¼
        patterns = [
            r"llama[^/\s]*-?\d*[^/\s]*-?\d+[bB]?",
            r"qwen[^/\s]*-?\d*[^/\s]*-?\d+[bB]?",
            r"chatglm[^/\s]*-?\d+[bB]?",
            r"baichuan[^/\s]*-?\d+[bB]?",
            r"vicuna[^/\s]*-?\d+[bB]?",
            r"mistral[^/\s]*-?\d+[bB]?",
            r"mixtral[^/\s]*-?\d+[bB]?",
        ]

        for pattern in patterns:
            match = re.search(pattern, prompt, re.IGNORECASE)
            if match:
                return match.group(0)

        return None

    # æ”¹ä¸ºLLMè¯†åˆ«
    def _extract_analysis_type(self, prompt: str) -> str:
        """æå–åˆ†æç±»å‹"""
        prompt_lower = prompt.lower()

        if (
            "ncu" in prompt_lower
            or "kernel" in prompt_lower
            or "æ·±åº¦" in prompt_lower
            or "nsight compute" in prompt_lower
        ):
            return "ncu (æ·±åº¦kernelåˆ†æ)"
        elif (
            "nsys" in prompt_lower
            or "å…¨å±€" in prompt_lower
            or "nsight systems" in prompt_lower
        ):
            return "nsys (å…¨å±€æ€§èƒ½åˆ†æ)"
        elif "é›†æˆ" in prompt_lower or "ç»¼åˆ" in prompt_lower or "å®Œæ•´" in prompt_lower:
            return "auto (é›†æˆåˆ†æ: nsys + ncu)"
        else:
            return "auto (é›†æˆåˆ†æ: nsys + ncu)"

    # æ”¹ä¸ºLLMè¯†åˆ«
    def _extract_parameters(self, prompt: str) -> Dict:
        """æå–å‚æ•°"""
        params = {}

        # æå–batch_size
        batch_match = re.search(
            r"batch[-_\s]*size?[ï¼š:\s=]*(\d+(?:\s*[,ï¼Œ]\s*\d+)*)", prompt, re.IGNORECASE
        )
        if batch_match:
            batch_sizes = [
                int(x.strip())
                for x in re.split(r"[,ï¼Œ\s]+", batch_match.group(1))
                if x.strip()
            ]
            params["batch_size"] = batch_sizes

        # æå–input_len
        input_match = re.search(
            r"input[-_\s]*len[gth]*[ï¼š:\s=]*(\d+(?:\s*[,ï¼Œ]\s*\d+)*)",
            prompt,
            re.IGNORECASE,
        )
        if input_match:
            input_lens = [
                int(x.strip())
                for x in re.split(r"[,ï¼Œ\s]+", input_match.group(1))
                if x.strip()
            ]
            params["input_len"] = input_lens

        # æå–output_len
        output_match = re.search(
            r"output[-_\s]*len[gth]*[ï¼š:\s=]*(\d+(?:\s*[,ï¼Œ]\s*\d+)*)",
            prompt,
            re.IGNORECASE,
        )
        if output_match:
            output_lens = [
                int(x.strip())
                for x in re.split(r"[,ï¼Œ\s]+", output_match.group(1))
                if x.strip()
            ]
            params["output_len"] = output_lens

        return params

    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        return list(self.model_mappings.keys())

    # æ²¡ç”¨åˆ°
    def get_analysis_status(self) -> Dict:
        """è·å–å½“å‰åˆ†æçŠ¶æ€"""
        return {
            "available_models": self.get_available_models(),
            "results_directory": str(self.results_dir),
            "nsys_enabled": self.profiling_config.get("nsys", {}).get("enabled", True),
            "ncu_enabled": self.profiling_config.get("ncu", {}).get("enabled", True),
        }


if __name__ == "__main__":
    # 0. CLI style ä¾èµ–é¡¹
    from prompt_toolkit import PromptSession
    from prompt_toolkit.history import InMemoryHistory
    from prompt_toolkit.formatted_text import HTML
    from prompt_toolkit.styles import Style

    # 1. åŠ è½½ Config
    config_path = "/workspaces/ai-agent/AI_Agent_Complete/config.yaml"
    if not os.path.exists(config_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {config_path}")
        sys.exit(1)

    with open(config_path, "r", encoding="utf-8") as f:
        config_yaml = yaml.safe_load(f)

    # 2. åˆå§‹åŒ– Agent
    print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ– AI Agent...")
    try:
        agent = AIAgent(config_yaml)
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        sys.exit(1)

    # # 3. åŠ è½½çŸ¥è¯†åº“
    # document_dir = Path("/workspaces/ai-agent/AI_Agent_Complete/documents")
    # if document_dir.exists():
    #     print("ğŸ“š æ­£åœ¨åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£...")
    #     count = 0
    #     for file_path in document_dir.iterdir():
    #         if file_path.is_file() and file_path.suffix in [".md", ".txt"]:
    #             agent.kb.add_document(str(file_path))
    #             count += 1
    #     print(f"âœ… å·²åŠ è½½ {count} ä¸ªæ–‡æ¡£ã€‚")
    # else:
    #     print("âš ï¸ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡åŠ è½½ã€‚")

    # 4. å¯¹è¯æµ‹è¯•
    async def interactive_chat_loop():
        style = Style.from_dict({"user-prompt": "#00aa00 bold", "text": "#ffffff"})
        session = PromptSession(history=InMemoryHistory())

        print("\n" + "=" * 60)
        print("ğŸ¤– AI æ€§èƒ½åˆ†æå™¨")
        print("ğŸ’¡ æ”¯æŒæŒ‡ä»¤: 'åˆ†æ llama-7b' | æé—®: 'kernel æœ‰å¤šå°‘' | é—²èŠ: 'ä½ æ˜¯è°'")
        print("=" * 60 + "\n")

        while True:
            try:
                user_input = await session.prompt_async(
                    HTML("<user-prompt>User ></user-prompt> "), style=style
                )
                user_input = user_input.strip()
                if not user_input:
                    continue
                if user_input.lower() in ["exit", "quit", "q"]:
                    print("\nğŸ‘‹ å†è§ï¼")
                    break

                print("\nâ³ Agent æ­£åœ¨æ€è€ƒ...")
                response = await agent.process_message(user_input)
                print("-" * 20 + " Agent å›å¤ " + "-" * 20)
                print(response)
                print("-" * 52 + "\n")

            except (KeyboardInterrupt, EOFError):
                break
            except Exception as e:
                print(f"\nâŒ é”™è¯¯: {e}")

    asyncio.run(interactive_chat_loop())
