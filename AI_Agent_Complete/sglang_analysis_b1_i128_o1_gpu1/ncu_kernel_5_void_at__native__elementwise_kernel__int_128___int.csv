"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description","Estimated Speedup Type","Estimated Speedup"

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.58",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9,201",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","11.05",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.54",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","5.82",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","6.27",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","17.56",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","5,477.45",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","14.56",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full waves across all SMs. Look at Launch Statistics for more details.","",""

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","33.55",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.96",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.56",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","25.03",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.00",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","25.03",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.44"

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","183.91",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","11.05",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","8.82",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","22.19",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","65.99",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","5.08",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","25.04",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","74.96",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","6.83",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.48",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 6.83 active warps per scheduler, but only an average of 0.48 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","74.96"

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","27.28",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","28.36",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","32",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.74",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 13.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 48.1% of the total average of 27.3 cycles between issuing two instructions.","global","48.08"

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1,318.79",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","696,320",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1,371.20",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","723,994",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1,024",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","18",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","32.77",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","131,072",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.48",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","21",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","32",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","64",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","100",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","43.38",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","27.76",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (100.0%) and measured achieved occupancy (43.4%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","56.62"

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","836.80",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","604,672",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","5,477.45",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1,243,474",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","5,571.27",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","788,960",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","5,477.45",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1,243,474",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","5,476.49",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4,973,896",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 11.07% above the average, while the minimum instance value is 11.49% below the average.","global","6.255"

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","77,824",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","100",

"0","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.59",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.58",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","9,739",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","10.42",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","5.22",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.18",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","6.18",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","16.85",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","5,548.76",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","14.55",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full waves across all SMs. Look at Launch Statistics for more details.","",""

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","33.55",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.95",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.56",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","24.72",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.99",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","24.72",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","82.66"

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","173.51",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","10.42",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","8.35",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","22.01",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","65.73",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","5.06",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","24.91",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.25",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","75.09",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","6.87",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.48",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 6.87 active warps per scheduler, but only an average of 0.48 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","75.09"

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","27.58",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","28.68",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","32",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.74",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 13.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 47.5% of the total average of 27.6 cycles between issuing two instructions.","global","47.53"

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","1,318.79",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","696,320",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","1,371.68",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","724,247",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1,024",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","18",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","32.77",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","131,072",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.48",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","21",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","32",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","64",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","100",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","43.50",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","27.84",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (100.0%) and measured achieved occupancy (43.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","56.5"

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","837.20",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","641,024",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","5,548.76",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1,244,094",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","5,492.50",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","835,280",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","5,548.76",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1,244,094",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","5,506.23",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","4,976,376",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 12.85% above the average, while the minimum instance value is 10.84% below the average.","global","6.757"

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","77,824",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","100",

"1","946741","python3.12","127.0.0.1","void elementwise_kernel<128, 4, void gpu_kernel_impl_nocast<direct_copy_kernel_cuda(TensorIteratorBase &)::[lambda() (instance 3)]::operator ()() lambda() (instance 10)]::operator ()() lambda(Half) (instance 1)]>(TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0",
