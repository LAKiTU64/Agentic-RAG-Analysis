import os

import shutil
import uuid
import hashlib
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Optional

from langchain_chroma import Chroma
from chromadb.config import Settings
from langchain_community.document_loaders import TextLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
import torch


# --- Config ---
DEFAULT_SEARCH_K = 5
MAX_DISTANCE_THRESHOLD = 0.5  # MAX_DISTANCE_THRESHOLD è¶Šå°ï¼Œæ£€ç´¢æ¡ä»¶è¶Šä¸¥æ ¼

# åŒ—äº¬æ—¶é—´
BEIJING_TZ = timezone(timedelta(hours=8))

# åˆ†å—ç­–ç•¥
CHUNKING_STRATEGY = {
    "default": {
        "chunk_size": 300,
        "chunk_overlap": 60,
        "add_start_index": True,
        "separators": [
            "\n# ",
            "# ",
            "\n## ",
            "## ",
            "\n### ",
            "### ",
            "\n#### ",
            "#### ",
            "\n\n",
            ".\n",
            "?\n",
            "!\n",
            "\n",
            " ",
            "",
        ],
    }
}


class VectorKBManager:
    """
    å‘é‡çŸ¥è¯†åº“ç®¡ç†ç±»ï¼ˆChromaDBï¼‰ï¼šåªä½¿ç”¨æœ¬åœ° Embedding æ¨¡å‹ã€‚
    æ”¯æŒ document_id (UUID4) å’Œ doc_hash (SHA256) åŒæ ‡è¯†ã€‚
    """

    def __init__(
        self,
        persist_directory: str = "/workspaces/ai-agent/AI_Agent_Complete/.chroma_db",
        embedding_model_path: str = "/workspaces/models/bge-small-zh-v1.5",
    ) -> None:
        self.persist_directory = persist_directory
        self.embedding_model_path = embedding_model_path

        # æ£€æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾„
        if not os.path.exists(embedding_model_path):
            raise FileNotFoundError(
                f"âŒ æ‰¾ä¸åˆ°æœ¬åœ°æ¨¡å‹ç›®å½•: {embedding_model_path}ã€‚è¯·ç¡®ä¿æ¨¡å‹å·²ä¸‹è½½åˆ°è¯¥ä½ç½®ã€‚"
            )

        # åˆå§‹åŒ– Embeddingï¼šå¼ºåˆ¶å¼€å¯ local_files_onlyï¼Œç¦æ­¢è”ç½‘ä¸‹è½½ embedding æ¨¡å‹ï¼›å¢åŠ  CUDA ç¯å¢ƒçš„æ”¯æŒ
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model_path,
            model_kwargs={
                "device": "cuda" if torch.cuda.is_available() else "cpu",
                "local_files_only": True,
            },
            encode_kwargs={"normalize_embeddings": True},  # å¼€å¯å‘é‡å½’ä¸€åŒ–
        )

        self.vectorstore: Optional[Chroma] = None
        self._load_or_create()

    def _load_or_create(self, is_reset: bool = False) -> None:
        """åˆå§‹åŒ–åŠ è½½ã€‚å¦‚æœæœ¬åœ°æœ‰æ•°æ®åˆ™è¯»å–ï¼Œå¦åˆ™åˆ›å»ºä¸€ä¸ªç©ºåº“ï¼›is_reset=True åˆ™å¼ºåˆ¶åˆ›å»ºæ–°åº“"""
        if is_reset and os.path.exists(self.persist_directory):
            shutil.rmtree(self.persist_directory)

        os.makedirs(self.persist_directory, exist_ok=True)

        self.vectorstore = Chroma(
            persist_directory=self.persist_directory,
            embedding_function=self.embeddings,
            collection_name="rag_collection",
            collection_metadata={"hnsw:space": "cosine"},
            client_settings=Settings(anonymized_telemetry=False),  # ç¦ç”¨é¥æµ‹
        )

    def _get_loader(self, file_path: str) -> TextLoader:
        """æ ¹æ®æ–‡ä»¶ç±»å‹ï¼Œè¿”å›å¯¹åº”çš„ Loader å¯¹è±¡ï¼Œæ”¯æŒ txt/md"""
        ext = file_path.split(".")[-1].lower()
        if ext in ("txt", "md"):
            return TextLoader(file_path, encoding="utf-8")
        else:
            raise ValueError(f"ä¸æ”¯æŒè¯¥æ–‡ä»¶ç±»å‹ï¼š.{ext}")

    def _compute_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶çš„ SHA256 å“ˆå¸Œï¼ˆåŸºäºåŸå§‹å­—èŠ‚ï¼‰ï¼Œç”¨äºå†…å®¹æŒ‡çº¹"""
        sha256 = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                sha256.update(chunk)
        return sha256.hexdigest()

    def add_document(
        self,
        file_path: str,
        document_id: Optional[str] = None,
        overwrite_document_id: Optional[str] = None,
        chunking_strategy: Optional[str] = None,
        runtime_info: Optional[Dict[str, Any]] = None,
        override_filename: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        å¯¼å…¥æ–‡æ¡£åˆ°å‘é‡åº“ï¼ˆåˆ‡ç‰‡åå…¥åº“ï¼‰ã€‚

        å‚æ•°ï¼š
        - document_id: å¯é€‰ï¼Œä¼ å…¥åˆ™ä½¿ç”¨æŒ‡å®š UUIDï¼ˆä¾‹å¦‚æ¥è‡ªä¸Šå±‚æ–‡æ¡£è¡¨ï¼‰
        - overwrite_document_id: å¯é€‰ï¼Œå¦‚æœæƒ³â€œæ›´æ–°â€æŸä¸ªå·²æœ‰æ–‡æ¡£ï¼Œä¼ å®ƒä¼šå…ˆæŒ‰è¯¥ id åˆ é™¤æ—§ chunksï¼Œå†å†™å…¥æ–° chunks
        - chunking_strategy: å¯é€‰ï¼ŒæŒ‡å®šåˆ†å—ç­–ç•¥ï¼Œé»˜è®¤ç­–ç•¥defaultï¼Œåç»­å°†ä¼šç»´æŠ¤å…¶ä»–ç­–ç•¥
        - runtime_info: å¯é€‰ï¼Œä¼ å…¥çš„sglangè¿è¡Œæ—¶å…ƒæ•°æ®å­—å…¸ï¼Œå†™å…¥æ¯ä¸ª chunk çš„ metadata ä¸­
        - override_filename: å¯é€‰ï¼Œè¦†ç›–æ–‡ä»¶åï¼ˆç”¨äºå‰ç«¯å±•ç¤ºï¼‰

        è¿”å›ï¼š
        - ç»“æ„åŒ–ç»“æœï¼Œä¾¿äº Web/æ¥å£å±‚ä½¿ç”¨
        """
        assert self.vectorstore is not None

        if not os.path.exists(file_path):
            return {"ok": False, "error": f"file not found: {file_path}"}

        # æå–åŸºç¡€metadataï¼›å¦‚æœä¼ äº† override_filename å°±ç”¨å®ƒä½œä¸ºå®é™…æ–‡ä»¶åï¼ˆä»¥é˜²æ–‡ä»¶åæ˜¯ä¸´æ—¶æ–‡ä»¶tmpxxxï¼‰
        filename = (
            override_filename if override_filename else os.path.basename(file_path)
        )
        add_time = datetime.now(BEIJING_TZ).isoformat()
        doc_hash = self._compute_file_hash(file_path)

        # ç”Ÿæˆ/æ ¡éªŒ UUID
        if overwrite_document_id:
            # å¦‚æœæ˜¯æ›´æ–°æ–‡æ¡£ï¼Œæ ¡éªŒæ˜¯å¦æ˜¯åˆæ³•UUID
            try:
                uuid.UUID(overwrite_document_id)
            except Exception:
                return {
                    "ok": False,
                    "error": "overwrite_document_id is not a valid UUID",
                }
            doc_uuid = overwrite_document_id
        else:
            # å¦‚æœæ˜¯ç›´æ¥æ·»åŠ æ–‡æ¡£
            if document_id is None:
                # æ²¡ä¼ å‚document_idçš„æƒ…å†µä¸‹ç”Ÿæˆæ–°UUID
                doc_uuid = str(uuid.uuid4())
            else:
                # ä¼ å‚document_idæ—¶ï¼Œæ ¡éªŒæ˜¯å¦æ˜¯åˆæ³•UUID
                try:
                    uuid.UUID(document_id)
                except Exception:
                    return {"ok": False, "error": "document_id is not a valid UUID"}
                doc_uuid = document_id

        # å¦‚æœæ˜¯æ›´æ–°æ–‡æ¡£ï¼Œå…ˆåˆ æ—§ chunksï¼ˆæŒ‰ document_id åˆ é™¤ï¼‰
        if overwrite_document_id:
            try:
                self.vectorstore.delete(where={"document_id": doc_uuid})
            except Exception as delete_error:
                return {
                    "ok": False,
                    "document_id": doc_uuid,
                    "filename": filename,
                    "error": f"failed to delete old chunks: {delete_error}",
                }

        try:
            loader = self._get_loader(file_path)
            docs = loader.load()

            # é…ç½®åˆ†å—ç­–ç•¥
            strategy_name = "default"
            strategy = CHUNKING_STRATEGY[strategy_name]

            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=strategy["chunk_size"],
                chunk_overlap=strategy["chunk_overlap"],
                add_start_index=strategy["add_start_index"],
                separators=strategy["separators"],
            )
            splits = text_splitter.split_documents(docs)

            # å†™å…¥ç»Ÿä¸€å…ƒæ•°æ®
            for split in splits:
                split.metadata.update(
                    {
                        "document_id": doc_uuid,  # åŸºäºUUIDçš„document_idï¼Œæ ‡è®°å”¯ä¸€æ€§
                        "doc_hash": doc_hash,  # å†…å®¹å“ˆå¸Œ
                        "filename": filename,  # æ–‡ä»¶å
                        "add_time": add_time,  # æ·»åŠ æ—¶é—´
                    }
                )
                # å†™å…¥runtime_infoå…ƒæ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
                if isinstance(runtime_info, dict):
                    split.metadata.update(runtime_info)

            self.vectorstore.add_documents(documents=splits)

            return {
                "ok": True,
                "document_id": doc_uuid,
                "doc_hash": doc_hash,
                "filename": filename,
                "chunk_count": len(splits),
                "add_time": add_time,
            }

        except Exception as e:
            return {
                "ok": False,
                "document_id": doc_uuid,
                "filename": filename,
                "error": str(e),
            }

    def delete_document(self, document_id: str) -> Dict[str, Any]:
        """æŒ‰ document_id(UUID) åˆ é™¤æ–‡æ¡£çš„æ‰€æœ‰ chunks"""
        assert self.vectorstore is not None
        try:
            uuid.UUID(document_id)
        except Exception:
            return {"ok": False, "error": "document_id is not a valid UUID"}

        try:
            self.vectorstore.delete(where={"document_id": document_id})
            return {"ok": True, "document_id": document_id}
        except Exception as e:
            return {"ok": False, "document_id": document_id, "error": str(e)}

    def search(
        self,
        query: str,
        k: int = DEFAULT_SEARCH_K,
        max_distance: float = MAX_DISTANCE_THRESHOLD,
        *,
        where_filter: Optional[Dict[str, Any]] = None,
    ) -> List[Dict[str, Any]]:
        """
        ç›¸ä¼¼åº¦æ£€ç´¢

        params:
        - where_filter: ç¬¦åˆ Chroma è¯­æ³•çš„è¿‡æ»¤å­—å…¸ã€‚
        ä¾‹å¦‚: æ‰¾å‡ºä½¿ç”¨ A100 æˆ– H800 GPUã€batch_size å¤§äº 8ã€ä¸” accuracy å°äºç­‰äº 0.85 çš„æ–‡æ¡£
        where_filter = {
            "$and": [
                {
                    "$or": [
                        {"gpu": "A100"},
                        {"gpu": "H800"}
                    ]
                },
                {"batch_size": {"$gt": 8}},
                {"accuracy": {"$lte": 0.85}}
            ]
        }
        """
        assert self.vectorstore is not None

        # æ‰§è¡Œæ£€ç´¢
        docs_and_scores = self.vectorstore.similarity_search_with_score(
            query, k=k, filter=where_filter
        )

        results: List[Dict[str, Any]] = []
        for doc, score in docs_and_scores:
            if score <= max_distance:
                meta = doc.metadata or {}
                res_item = {
                    "content": doc.page_content,
                    "score": round(float(score), 4),
                    **meta,  # å±•å¼€æ‰€æœ‰å…ƒæ•°æ®
                }
                results.append(res_item)

        return results

    def get_overview(self) -> Dict[str, Any]:
        """
        çŸ¥è¯†åº“æ¦‚è§ˆï¼šè¿”å›åˆ‡ç‰‡æ•°ã€æ–‡æ¡£æ•°ã€æ–‡æ¡£å…ƒæ•°æ®
        """
        assert self.vectorstore is not None
        all_data = self.vectorstore.get(include=["metadatas"])
        metadatas = all_data.get("metadatas", []) or []

        # doc_stats ç»“æ„è°ƒæ•´ï¼Œå­˜å‚¨å®Œæ•´çš„å…ƒæ•°æ®å­—å…¸
        doc_stats: Dict[str, Dict[str, Any]] = {}

        for meta in metadatas:
            did = meta.get("document_id")
            atime = meta.get("add_time")
            if did:
                # è®°å½•è¯¥ document_id ä¸‹æœ€æ–°çš„å…ƒæ•°æ®ï¼ˆæˆ–è€…ç¬¬ä¸€æ¡ï¼‰
                if did not in doc_stats or (
                    atime and atime > doc_stats[did].get("add_time", "")
                ):
                    doc_stats[did] = meta

        sorted_docs = sorted(
            doc_stats.values(), key=lambda x: x.get("add_time", ""), reverse=True
        )

        return {
            "persist_directory": self.persist_directory,
            "total_chunks": len(metadatas),
            "total_documents": len(doc_stats),
            "documents": sorted_docs,  # ç›´æ¥è¿”å›åŒ…å«æ‰€æœ‰å…ƒæ•°æ®çš„åˆ—è¡¨
        }

    def update_document_metadata(
        self, document_id: str, new_metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ›´æ–°æŒ‡å®šæ–‡æ¡£çš„æ‰€æœ‰ chunks çš„å…ƒæ•°æ®"""
        assert self.vectorstore is not None

        # ç¦æ­¢ä¿®æ”¹çš„æ ¸å¿ƒå­—æ®µ
        protected_keys = ["document_id", "doc_hash", "add_time"]
        filtered_metadata = {
            k: v for k, v in new_metadata.items() if k not in protected_keys
        }

        try:
            # 1. è·å–è¯¥æ–‡æ¡£æ‰€æœ‰çš„ chunk ID å’Œ ç°æœ‰å…ƒæ•°æ®
            existing_data = self.vectorstore.get(
                where={"document_id": document_id}, include=["metadatas"]
            )
            ids = existing_data.get("ids", [])
            if not ids:
                return {"ok": False, "error": "æœªæ‰¾åˆ°å¯¹åº”çš„æ–‡æ¡£chunk"}

            metadatas = existing_data.get("metadatas", [])

            # 2. å‡†å¤‡æ›´æ–°åçš„å…ƒæ•°æ®åˆ—è¡¨
            updated_metadatas = []
            for old_meta in metadatas:
                new_meta = old_meta.copy()
                new_meta.update(filtered_metadata)
                updated_metadatas.append(new_meta)

            # 3. æ‰§è¡Œæ›´æ–°
            self.vectorstore._collection.update(ids=ids, metadatas=updated_metadatas)
            return {"ok": True, "document_id": document_id}
        except Exception as e:
            print(f"å…ƒæ•°æ®æ›´æ–°é”™è¯¯: {str(e)}")  # åå°æ‰“å°å…·ä½“é”™è¯¯
            return {"ok": False, "error": str(e)}

    def reset_index(self) -> None:
        """é‡ç½®å‘é‡åº“ï¼ˆç‰©ç†åˆ é™¤æŒä¹…åŒ–ç›®å½•å¹¶é‡å»ºï¼‰"""
        if os.path.exists(self.persist_directory):
            shutil.rmtree(self.persist_directory)
        self._load_or_create(is_reset=False)

    def as_retrieriter(self, **kwargs):
        from langchain_core.documents import Document
        from langchain_core.retrievers import BaseRetriever
        from pydantic import PrivateAttr
        from typing import Optional, Dict, Any

        class KBRetriever(BaseRetriever):
            _kb_manager: "VectorKBManager" = PrivateAttr()
            k: int = DEFAULT_SEARCH_K
            max_distance: float = MAX_DISTANCE_THRESHOLD

            def __init__(self, kb_manager, k, max_distance, **data):
                super().__init__(**data)
                self._kb_manager = kb_manager
                self.k = k
                self.max_distance = max_distance

            def _get_relevant_documents(
                self,
                query: str,
                *,
                run_manager=None,  # LangChain å†…éƒ¨å‚æ•°ï¼Œå¯å¿½ç•¥
                **kwargs,
            ) -> List[Document]:
                # ä» kwargs ä¸­æå– filterï¼ˆç¬¦åˆ LangChain è§„èŒƒï¼‰
                where_filter: Optional[Dict[str, Any]] = kwargs.get("filter")

                search_results = self._kb_manager.search(
                    query,
                    k=self.k,
                    max_distance=self.max_distance,
                    where_filter=where_filter,  # ğŸ‘ˆ é€ä¼ ï¼
                )
                return [
                    Document(
                        page_content=res["content"],
                        metadata={
                            "document_id": res.get("document_id"),
                            "doc_hash": res.get("doc_hash"),
                            "filename": res.get("filename"),
                            "add_time": res.get("add_time"),
                            "score": res.get("score"),
                            "start_index": res.get("start_index"),
                            "source": res.get("source"),
                        },
                    )
                    for res in search_results
                ]

        k = kwargs.get("k", DEFAULT_SEARCH_K)
        max_distance = kwargs.get("max_distance", MAX_DISTANCE_THRESHOLD)
        return KBRetriever(kb_manager=self, k=k, max_distance=max_distance)


if __name__ == "__main__":
    # 1. åˆå§‹åŒ–ï¼ˆç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼‰
    print("\nStep 1: åˆå§‹åŒ–å‘é‡åº“\n")
    chroma_path = "workspaces/ai-agent/AI_Agent_Complete/.chroma_db"
    embedding_model_path = "/workspaces/models/bge-small-zh-v1.5"
    docs_dir = "workspaces/ai-agent/AI_Agent_Complete/documents"  # æµ‹è¯•æ–‡æ¡£ç›®å½•

    kb = VectorKBManager(
        persist_directory=chroma_path, embedding_model_path=embedding_model_path
    )

    # 2. æ‰¹é‡å¯¼å…¥
    print("\nStep 2: æ‰¹é‡å¯¼å…¥\n")
    # æ¨¡æ‹Ÿä¸€ä¸ªç»Ÿä¸€çš„ runtime_infoï¼ˆæ‰€æœ‰æ–‡æ¡£å…±äº«ï¼‰
    mock_runtime_info = {
        "model_name": "qwen2-7b",
        "batch_size": 16,  # æ”¹ä¸º >8ï¼Œæ–¹ä¾¿åç»­è¿‡æ»¤
        "gpu": "H800",
        "accuracy": 0.91,  # æ–°å¢å­—æ®µï¼Œç”¨äº filter æµ‹è¯•
        "input_len": 512,
    }

    for filename in os.listdir(docs_dir):
        full_path = os.path.join(docs_dir, filename)
        if os.path.isfile(full_path):
            result = kb.add_document(full_path, runtime_info=mock_runtime_info)
            print(result)

    # 3. æ¦‚è§ˆ
    print("\nStep 3: æ¦‚è§ˆ\n")
    overview = kb.get_overview()
    print("overview:", overview)

    # 4. åŸºç¡€æ£€ç´¢æµ‹è¯•
    print("\nStep 4: åŸºç¡€æ£€ç´¢æµ‹è¯•\n")
    test_query = "L2ç¼“å­˜å‘½ä¸­ç‡ä½"
    results = kb.search(test_query)
    for res in results:
        print(
            f"doc_hash={res.get('doc_hash')} | "
            f"filename={res.get('filename')} | "
            f"score={res.get('score')} | "
            f"content=\n{res.get('content')[:100]}..."
        )

    # 5. å¸¦å…ƒæ•°æ®è¿‡æ»¤çš„æ£€ç´¢æµ‹è¯•
    print("\nStep 5: å…ƒæ•°æ®è¿‡æ»¤æ£€ç´¢æµ‹è¯• (filter: gpu=H800 ä¸” batch_size > 8)\n")
    filtered_results = kb.search(
        test_query,
        where_filter={
            "$and": [
                {"gpu": "H800"},
                {"batch_size": {"$gt": 8}},
            ]
        },
    )
    for res in filtered_results:
        print(
            f"âœ… è¿‡æ»¤å‘½ä¸­ | gpu={res.get('gpu')} | batch_size={res.get('batch_size')} | "
            f"score={res.get('score')}"
        )

    # 6. æµ‹è¯• as_retriever + filterï¼ˆLangChain æ ‡å‡†ç”¨æ³•ï¼‰
    print("\nStep 6: æµ‹è¯• as_retriever + filter\n")
    retriever = kb.as_retrieriter(k=3, max_distance=0.6)
    docs = retriever.invoke(
        "æ€§èƒ½ç“¶é¢ˆ",
        filter={"accuracy": {"$gt": 0.90}},  # åˆ©ç”¨ runtime_info ä¸­çš„ accuracy
    )
    print(f"Retriever è¿”å› {len(docs)} ä¸ªæ–‡æ¡£")
    for doc in docs:
        print(
            f" - score={doc.metadata.get('score')}, accuracy={doc.metadata.get('accuracy')}"
        )
