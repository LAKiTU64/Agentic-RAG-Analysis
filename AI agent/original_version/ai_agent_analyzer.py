#!/usr/bin/env python3
"""
AI Agent for Automatic LLM Performance Analysis

è¿™ä¸ªAI Agentèƒ½å¤Ÿï¼š
1. è§£æç”¨æˆ·æç¤ºè¯ï¼Œè‡ªåŠ¨é…ç½®SGlangè„šæœ¬å‚æ•°
2. æ ¹æ®éœ€æ±‚è¿è¡Œnsys/ncuæ€§èƒ½åˆ†æ
3. è°ƒç”¨åˆ†æè„šæœ¬ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š

ä½œè€…: AIåŠ©æ‰‹
ç‰ˆæœ¬: 1.0
"""

import os
import sys
import re
import json
import subprocess
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Union, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import shlex

# å¯¼å…¥åˆ†æå·¥å…·
tools_dir = Path(__file__).parent.parent.parent / "TOOLS" / "Auto_Anlyze_tool"
if tools_dir.exists():
    sys.path.append(str(tools_dir))
    try:
        from nsys_parser import NsysParser, NsysAnalyzer, NsysVisualizer, NsysReporter
        from ncu_parser import NCUParser, NCUAnalyzer, NCUVisualizer, NCUReporter  
        from nsys_to_ncu_analyzer import NSysToNCUAnalyzer
    except ImportError as e:
        print(f"âš ï¸  è­¦å‘Š: æ— æ³•å¯¼å…¥åˆ†æå·¥å…·: {e}")
        print("è¯·ç¡®ä¿ TOOLS/Auto_Anlyze_tool/ ç›®å½•å­˜åœ¨ä¸”åŒ…å«ç›¸å…³è„šæœ¬")
else:
    print(f"âš ï¸  è­¦å‘Š: åˆ†æå·¥å…·ç›®å½•ä¸å­˜åœ¨: {tools_dir}")
    # åˆ›å»ºå ä½ç¬¦ç±»ï¼Œé¿å…è¿è¡Œæ—¶é”™è¯¯
    class MockAnalyzer:
        def __init__(self, *args, **kwargs): pass
        def parse(self): pass
        def analyze(self): return {}
        def create_visualizations(self): pass
        def generate_report(self): pass
    
    NsysParser = NsysAnalyzer = NsysVisualizer = NsysReporter = MockAnalyzer
    NCUParser = NCUAnalyzer = NCUVisualizer = NCUReporter = MockAnalyzer
    NSysToNCUAnalyzer = MockAnalyzer

@dataclass
class AnalysisRequest:
    """åˆ†æè¯·æ±‚çš„æ•°æ®ç»“æ„"""
    # åŸºæœ¬ä¿¡æ¯
    model_name: str
    script_type: str = "bench_one_batch_server"  # bench_one_batch_server, launch_serverç­‰
    analysis_type: str = "auto"  # nsys, ncu, auto(é›†æˆåˆ†æ)
    
    # è„šæœ¬å‚æ•°
    batch_size: List[int] = None
    input_len: List[int] = None 
    output_len: List[int] = None
    temperature: float = 0.0
    trust_device: bool = True
    
    # åˆ†æå‚æ•°
    profile_steps: int = 3
    profile_by_stage: bool = False
    max_ncu_kernels: int = 5
    output_dir: str = None
    
    # é«˜çº§å‚æ•°
    tp_size: int = 1
    host: str = "127.0.0.1"
    port: int = 30000
    
    def __post_init__(self):
        if self.batch_size is None:
            self.batch_size = [1, 8, 16]
        if self.input_len is None:
            self.input_len = [512, 1024]
        if self.output_len is None:
            self.output_len = [64, 128]
        if self.output_dir is None:
            safe_model_name = re.sub(r'[^\w\-_]', '_', self.model_name)
            self.output_dir = f"analysis_{safe_model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

class PromptParser:
    """æç¤ºè¯è§£æå™¨"""
    
    def __init__(self):
        # æ¨¡å‹åç§°æ¨¡å¼
        self.model_patterns = [
            r'--model[=\s]+([^\s]+)',
            r'æ¨¡å‹[ï¼š:]\s*([^\s,ï¼Œ]+)',
            r'model[ï¼š:]\s*([^\s,ï¼Œ]+)',
        ]
        
        # è„šæœ¬ç±»å‹æ¨¡å¼
        self.script_patterns = {
            'bench_one_batch_server': [
                r'bench_one_batch_server',
                r'batch.*server', 
                r'å•æ‰¹æ¬¡.*æœåŠ¡å™¨',
                r'benchmarking?'
            ],
            'launch_server': [
                r'launch_server',
                r'å¯åŠ¨.*æœåŠ¡å™¨',
                r'server.*launch'
            ]
        }
        
        # åˆ†æç±»å‹æ¨¡å¼
        self.analysis_patterns = {
            'nsys': [
                r'nsys',
                r'nsight.*systems?',
                r'å…¨å±€.*åˆ†æ',
                r'timeline.*åˆ†æ'
            ],
            'ncu': [
                r'ncu', 
                r'nsight.*compute',
                r'kernel.*åˆ†æ',
                r'ç®—å­.*åˆ†æ',
                r'æ·±åº¦.*åˆ†æ'
            ],
            'auto': [
                r'é›†æˆ.*åˆ†æ',
                r'ç»¼åˆ.*åˆ†æ', 
                r'auto.*analy',
                r'å®Œæ•´.*åˆ†æ'
            ]
        }
    
    def parse_prompt(self, prompt: str) -> AnalysisRequest:
        """è§£æç”¨æˆ·æç¤ºè¯"""
        print(f"ğŸ” æ­£åœ¨è§£ææç¤ºè¯: {prompt}")
        
        # æå–æ¨¡å‹åç§°
        model_name = self._extract_model_name(prompt)
        if not model_name:
            raise ValueError("æœªèƒ½ä»æç¤ºè¯ä¸­æå–æ¨¡å‹åç§°ï¼Œè¯·æ˜ç¡®æŒ‡å®šæ¨¡å‹")
        
        # æå–è„šæœ¬ç±»å‹
        script_type = self._extract_script_type(prompt)
        
        # æå–åˆ†æç±»å‹  
        analysis_type = self._extract_analysis_type(prompt)
        
        # æå–å‚æ•°
        params = self._extract_parameters(prompt)
        
        request = AnalysisRequest(
            model_name=model_name,
            script_type=script_type,
            analysis_type=analysis_type,
            **params
        )
        
        print(f"âœ… è§£æç»“æœ:")
        print(f"  - æ¨¡å‹: {request.model_name}")
        print(f"  - è„šæœ¬ç±»å‹: {request.script_type}")  
        print(f"  - åˆ†æç±»å‹: {request.analysis_type}")
        print(f"  - æ‰¹æ¬¡å¤§å°: {request.batch_size}")
        print(f"  - è¾“å…¥é•¿åº¦: {request.input_len}")
        print(f"  - è¾“å‡ºé•¿åº¦: {request.output_len}")
        
        return request
    
    def _extract_model_name(self, prompt: str) -> Optional[str]:
        """æå–æ¨¡å‹åç§°"""
        for pattern in self.model_patterns:
            match = re.search(pattern, prompt, re.IGNORECASE)
            if match:
                return match.group(1)
        
        # å°è¯•æå–å¸¸è§æ¨¡å‹åç§°
        common_models = [
            r'llama[^/]*-?\d*[^/]*-?\d+[bB]?',
            r'qwen[^/]*-?\d*[^/]*-?\d+[bB]?',
            r'chatglm[^/]*-?\d+[bB]?',
            r'baichuan[^/]*-?\d+[bB]?',
            r'vicuna[^/]*-?\d+[bB]?'
        ]
        
        for pattern in common_models:
            match = re.search(pattern, prompt, re.IGNORECASE)
            if match:
                return match.group(0)
        
        return None
    
    def _extract_script_type(self, prompt: str) -> str:
        """æå–è„šæœ¬ç±»å‹"""
        for script_type, patterns in self.script_patterns.items():
            for pattern in patterns:
                if re.search(pattern, prompt, re.IGNORECASE):
                    return script_type
        return "bench_one_batch_server"  # é»˜è®¤
    
    def _extract_analysis_type(self, prompt: str) -> str:
        """æå–åˆ†æç±»å‹"""
        for analysis_type, patterns in self.analysis_patterns.items():
            for pattern in patterns:
                if re.search(pattern, prompt, re.IGNORECASE):
                    return analysis_type
        return "auto"  # é»˜è®¤é›†æˆåˆ†æ
    
    def _extract_parameters(self, prompt: str) -> Dict:
        """æå–å‚æ•°"""
        params = {}
        
        # æå–æ‰¹æ¬¡å¤§å°
        batch_match = re.search(r'batch[-_\s]*size?[ï¼š:\s=]*(\d+(?:\s*[,ï¼Œ]\s*\d+)*)', prompt, re.IGNORECASE)
        if batch_match:
            batch_sizes = [int(x.strip()) for x in re.split(r'[,ï¼Œ\s]+', batch_match.group(1))]
            params['batch_size'] = batch_sizes
        
        # æå–è¾“å…¥é•¿åº¦
        input_match = re.search(r'input[-_\s]*len[gth]*[ï¼š:\s=]*(\d+(?:\s*[,ï¼Œ]\s*\d+)*)', prompt, re.IGNORECASE)
        if input_match:
            input_lens = [int(x.strip()) for x in re.split(r'[,ï¼Œ\s]+', input_match.group(1))]
            params['input_len'] = input_lens
        
        # æå–è¾“å‡ºé•¿åº¦
        output_match = re.search(r'output[-_\s]*len[gth]*[ï¼š:\s=]*(\d+(?:\s*[,ï¼Œ]\s*\d+)*)', prompt, re.IGNORECASE)
        if output_match:
            output_lens = [int(x.strip()) for x in re.split(r'[,ï¼Œ\s]+', output_match.group(1))]
            params['output_len'] = output_lens
        
        # æå–æ¸©åº¦
        temp_match = re.search(r'temperature[ï¼š:\s=]*([0-9.]+)', prompt, re.IGNORECASE)
        if temp_match:
            params['temperature'] = float(temp_match.group(1))
        
        # æå–tensorå¹¶è¡Œåº¦
        tp_match = re.search(r'tp[-_\s]*size[ï¼š:\s=]*(\d+)', prompt, re.IGNORECASE)
        if tp_match:
            params['tp_size'] = int(tp_match.group(1))
        
        return params

class ConfigGenerator:
    """å‚æ•°é…ç½®ç”Ÿæˆå™¨"""
    
    def __init__(self, workspace_root: str = "."):
        self.workspace_root = Path(workspace_root)
        self.models_dir = self.workspace_root / "workspace" / "models"
        
    def generate_sglang_config(self, request: AnalysisRequest) -> Dict:
        """ç”ŸæˆSGlangè„šæœ¬é…ç½®"""
        
        # æŸ¥æ‰¾æ¨¡å‹è·¯å¾„
        model_path = self._resolve_model_path(request.model_name)
        
        config = {
            # æœåŠ¡å™¨å‚æ•°
            'model_path': model_path,
            'host': request.host,
            'port': request.port,
            'tp_size': request.tp_size,
            'trust_remote_code': request.trust_device,
            
            # åŸºå‡†æµ‹è¯•å‚æ•°
            'batch_size': request.batch_size,
            'input_len': request.input_len, 
            'output_len': request.output_len,
            'temperature': request.temperature,
            
            # åˆ†æå‚æ•°
            'profile': True,
            'profile_steps': request.profile_steps,
            'profile_by_stage': request.profile_by_stage,
            
            # è¾“å‡ºé…ç½®
            'show_report': True,
            'result_filename': f"{request.output_dir}/benchmark_results.jsonl"
        }
        
        print(f"ğŸ“‹ ç”Ÿæˆçš„é…ç½®:")
        print(f"  - æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"  - TPå¤§å°: {request.tp_size}")
        print(f"  - æ‰¹æ¬¡å¤§å°: {request.batch_size}")
        
        return config
    
    def _resolve_model_path(self, model_name: str) -> str:
        """è§£ææ¨¡å‹è·¯å¾„"""
        
        # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥è¿”å›
        if Path(model_name).is_absolute():
            return model_name
        
        # åœ¨workspace/modelsä¸‹æŸ¥æ‰¾
        possible_paths = [
            self.models_dir / model_name,
            self.models_dir / model_name.replace('/', '_'),
            self.models_dir / model_name.split('/')[-1],
        ]
        
        for path in possible_paths:
            if path.exists():
                print(f"âœ… æ‰¾åˆ°æ¨¡å‹è·¯å¾„: {path}")
                return str(path)
        
        # å¦‚æœæœ¬åœ°æ‰¾ä¸åˆ°ï¼Œå‡è®¾æ˜¯HuggingFaceæ¨¡å‹ID
        print(f"âš ï¸  æœ¬åœ°æœªæ‰¾åˆ°æ¨¡å‹ï¼Œä½¿ç”¨HuggingFace ID: {model_name}")
        return model_name
    
    def build_command(self, request: AnalysisRequest, config: Dict) -> List[str]:
        """æ„å»ºSGlangæ‰§è¡Œå‘½ä»¤"""
        
        if request.script_type == "bench_one_batch_server":
            cmd = [
                'python', '-m', 'sglang.bench_one_batch_server',
                '--model', config['model_path'],
                '--host', config['host'],
                '--port', str(config['port']),
                '--tp-size', str(config['tp_size']),
                '--temperature', str(config['temperature']),
                '--batch-size'] + [str(bs) for bs in config['batch_size']] + [
                '--input-len'] + [str(il) for il in config['input_len']] + [
                '--output-len'] + [str(ol) for ol in config['output_len']] + [
                '--result-filename', config['result_filename'],
                '--show-report'
            ]
            
            if config.get('trust_remote_code'):
                cmd.extend(['--trust-remote-code'])
            
            if config.get('profile'):
                cmd.extend(['--profile', '--profile-steps', str(config['profile_steps'])])
                
                if config.get('profile_by_stage'):
                    cmd.extend(['--profile-by-stage'])
        
        elif request.script_type == "launch_server":
            cmd = [
                'python', '-m', 'sglang.launch_server',
                '--model-path', config['model_path'],
                '--host', config['host'],
                '--port', str(config['port']),
                '--tp-size', str(config['tp_size'])
            ]
            
            if config.get('trust_remote_code'):
                cmd.extend(['--trust-remote-code'])
        
        return cmd

class AnalysisOrchestrator:
    """åˆ†æç¼–æ’å™¨"""
    
    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def run_analysis(self, request: AnalysisRequest, sglang_command: List[str]) -> Dict:
        """æ ¹æ®è¯·æ±‚ç±»å‹è¿è¡Œç›¸åº”çš„åˆ†æ"""
        
        print(f"ğŸš€ å¼€å§‹æ‰§è¡Œ {request.analysis_type} åˆ†æ...")
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'request': asdict(request),
            'command': sglang_command,
            'analysis_results': {}
        }
        
        if request.analysis_type == "nsys":
            results['analysis_results'] = self._run_nsys_analysis(sglang_command, request)
            
        elif request.analysis_type == "ncu":
            results['analysis_results'] = self._run_ncu_analysis(sglang_command, request)
            
        elif request.analysis_type == "auto":
            results['analysis_results'] = self._run_integrated_analysis(sglang_command, request)
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {request.analysis_type}")
        
        # ä¿å­˜ç»“æœ
        results_file = self.output_dir / "analysis_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“‹ åˆ†æç»“æœå·²ä¿å­˜: {results_file}")
        return results
    
    def _run_nsys_analysis(self, sglang_command: List[str], request: AnalysisRequest) -> Dict:
        """è¿è¡Œnsysåˆ†æ"""
        
        nsys_file = self.output_dir / "profile.nsys-rep"
        
        # æ„å»ºnsyså‘½ä»¤
        nsys_cmd = [
            'nsys', 'profile',
            '-o', str(nsys_file.with_suffix('')),
            '-t', 'cuda,nvtx,osrt',
            '--cuda-memory-usage=true',
            '--force-overwrite=true'
        ] + sglang_command
        
        print(f"ğŸ”„ æ‰§è¡Œnsyså‘½ä»¤: {' '.join(nsys_cmd)}")
        
        try:
            # è¿è¡Œnsys profiling
            result = subprocess.run(nsys_cmd, capture_output=True, text=True, check=True,
                                  cwd='SGlang')
            
            print(f"âœ… nsysåˆ†æå®Œæˆ: {nsys_file}")
            
            # ä½¿ç”¨nsysè§£æå™¨åˆ†æç»“æœ
            parser = NsysParser(str(nsys_file))
            parser.parse()
            
            analyzer = NsysAnalyzer(parser)
            stats = analyzer.analyze()
            
            # ç”Ÿæˆå¯è§†åŒ–
            visualizer = NsysVisualizer(parser, analyzer)
            visualizer.output_dir = self.output_dir / "nsys_visualization"
            visualizer.create_visualizations()
            
            # ç”ŸæˆæŠ¥å‘Š
            reporter = NsysReporter(parser, analyzer)
            reporter.output_dir = self.output_dir / "nsys_reports" 
            reporter.generate_report()
            
            return {
                'nsys_file': str(nsys_file),
                'stats': stats,
                'kernels_count': len(parser.kernels),
                'memory_transfers_count': len(parser.memory_transfers),
                'visualization_dir': str(visualizer.output_dir),
                'reports_dir': str(reporter.output_dir)
            }
            
        except subprocess.CalledProcessError as e:
            error_msg = f"nsysåˆ†æå¤±è´¥: {e.stderr}"
            print(f"âŒ {error_msg}")
            return {'error': error_msg}
    
    def _run_ncu_analysis(self, sglang_command: List[str], request: AnalysisRequest) -> Dict:
        """è¿è¡Œncuåˆ†æ"""
        
        # å…ˆè¿è¡Œnsysè·å–çƒ­ç‚¹kernels
        print("ğŸ” é¦–å…ˆè¿è¡Œnsysè¯†åˆ«çƒ­ç‚¹kernels...")
        nsys_result = self._run_nsys_analysis(sglang_command, request)
        
        if 'error' in nsys_result:
            return nsys_result
        
        # æå–çƒ­ç‚¹kernels (è¿™é‡Œç®€åŒ–å¤„ç†)
        ncu_file = self.output_dir / "ncu_profile.ncu-rep"
        
        # æ„å»ºncuå‘½ä»¤ (åˆ†ææ‰€æœ‰kernels)
        ncu_cmd = [
            'ncu',
            '--set', 'full',
            '-o', str(ncu_file.with_suffix('')),
            '--force-overwrite'
        ] + sglang_command
        
        print(f"ğŸ”„ æ‰§è¡Œncuå‘½ä»¤: {' '.join(ncu_cmd)}")
        
        try:
            # è¿è¡Œncu profiling
            result = subprocess.run(ncu_cmd, capture_output=True, text=True, 
                                  check=True, timeout=600, cwd='SGlang')
            
            print(f"âœ… ncuåˆ†æå®Œæˆ: {ncu_file}")
            
            # å¯¼å‡ºä¸ºCSV
            csv_file = ncu_file.with_suffix('.csv')
            export_cmd = ['ncu', '--csv', '--log-file', str(csv_file), 
                         '--import', str(ncu_file)]
            subprocess.run(export_cmd, check=True)
            
            # ä½¿ç”¨ncuè§£æå™¨åˆ†æç»“æœ
            parser = NCUParser(str(csv_file))
            parser.parse()
            
            analyzer = NCUAnalyzer(parser)
            stats = analyzer.analyze()
            
            # ç”Ÿæˆå¯è§†åŒ–
            visualizer = NCUVisualizer(parser, analyzer)
            visualizer.output_dir = self.output_dir / "ncu_visualization"
            visualizer.create_visualizations()
            
            # ç”ŸæˆæŠ¥å‘Š
            reporter = NCUReporter(parser, analyzer)
            reporter.output_dir = self.output_dir / "ncu_reports"
            reporter.generate_report()
            
            return {
                'ncu_file': str(ncu_file),
                'csv_file': str(csv_file),
                'stats': stats,
                'kernels_count': len(parser.kernels),
                'bottlenecks_count': len(analyzer.bottlenecks),
                'visualization_dir': str(visualizer.output_dir),
                'reports_dir': str(reporter.output_dir),
                'nsys_result': nsys_result
            }
            
        except subprocess.CalledProcessError as e:
            error_msg = f"ncuåˆ†æå¤±è´¥: {e.stderr}"
            print(f"âŒ {error_msg}")
            return {'error': error_msg}
        except subprocess.TimeoutExpired:
            error_msg = "ncuåˆ†æè¶…æ—¶"
            print(f"â° {error_msg}")
            return {'error': error_msg}
    
    def _run_integrated_analysis(self, sglang_command: List[str], request: AnalysisRequest) -> Dict:
        """è¿è¡Œé›†æˆåˆ†æ"""
        
        # ä½¿ç”¨é›†æˆåˆ†æå™¨
        analyzer = NSysToNCUAnalyzer(str(self.output_dir / "integrated"))
        
        try:
            # æ­¥éª¤1: nsyså…¨å±€åˆ†æ
            nsys_file = analyzer.step1_nsys_analysis(sglang_command, "sglang_overview")
            
            # æ­¥éª¤2: æå–çƒ­ç‚¹kernels
            hot_kernels = analyzer.step2_extract_hot_kernels(nsys_file, top_k=10)
            
            if not hot_kernels:
                return {'error': 'æœªå‘ç°çƒ­ç‚¹kernels'}
            
            # æ­¥éª¤3: ncuæ·±åº¦åˆ†æ
            ncu_files = analyzer.step3_ncu_targeted_analysis(
                sglang_command, hot_kernels, request.max_ncu_kernels
            )
            
            # æ­¥éª¤4: ç»¼åˆåˆ†æ
            comprehensive_results = analyzer.step4_comprehensive_analysis(ncu_files)
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            report_file = analyzer.generate_final_report(comprehensive_results)
            
            return {
                'nsys_file': nsys_file,
                'hot_kernels': hot_kernels,
                'ncu_files': ncu_files,
                'comprehensive_results': comprehensive_results,
                'final_report': report_file,
                'analysis_dir': str(analyzer.output_dir)
            }
            
        except Exception as e:
            error_msg = f"é›†æˆåˆ†æå¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return {'error': error_msg}

class AIAgentAnalyzer:
    """AIæ€§èƒ½åˆ†æAgentä¸»ç±»"""
    
    def __init__(self, workspace_root: str = "."):
        self.workspace_root = workspace_root
        self.parser = PromptParser()
        self.config_generator = ConfigGenerator(workspace_root)
    
    def analyze_from_prompt(self, prompt: str) -> Dict:
        """ä»ç”¨æˆ·æç¤ºè¯å¼€å§‹å®Œæ•´çš„åˆ†ææµç¨‹"""
        
        print("ğŸ¤– AI Agentæ€§èƒ½åˆ†æå™¨å¯åŠ¨")
        print("=" * 60)
        
        try:
            # 1. è§£ææç¤ºè¯
            print("\nğŸ“ æ­¥éª¤1: è§£æç”¨æˆ·æç¤ºè¯")
            request = self.parser.parse_prompt(prompt)
            
            # 2. ç”Ÿæˆé…ç½®
            print("\nâš™ï¸  æ­¥éª¤2: ç”Ÿæˆè„šæœ¬é…ç½®")
            config = self.config_generator.generate_sglang_config(request)
            sglang_command = self.config_generator.build_command(request, config)
            
            # 3. åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = Path(request.output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # 4. è¿è¡Œåˆ†æ
            print(f"\nğŸ”¬ æ­¥éª¤3: æ‰§è¡Œ{request.analysis_type}åˆ†æ")
            orchestrator = AnalysisOrchestrator(request.output_dir)
            results = orchestrator.run_analysis(request, sglang_command)
            
            print(f"\nğŸ‰ åˆ†æå®Œæˆ!")
            print(f"ğŸ“ ç»“æœç›®å½•: {request.output_dir}")
            
            return results
            
        except Exception as e:
            error_msg = f"AI Agentåˆ†æå¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return {'error': error_msg}
    
    def analyze_existing_files(self, file_path: str, analysis_type: str = "auto") -> Dict:
        """åˆ†æå·²æœ‰çš„profileæ–‡ä»¶"""
        
        file_path = Path(file_path)
        if not file_path.exists():
            return {'error': f'æ–‡ä»¶ä¸å­˜åœ¨: {file_path}'}
        
        output_dir = file_path.parent / f"analysis_{file_path.stem}_{datetime.now().strftime('%H%M%S')}"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“Š åˆ†æå·²æœ‰æ–‡ä»¶: {file_path}")
        
        try:
            if file_path.suffix.lower() == '.nsys-rep' or 'nsys' in analysis_type:
                # nsysæ–‡ä»¶åˆ†æ
                parser = NsysParser(str(file_path))
                parser.parse()
                
                analyzer = NsysAnalyzer(parser)
                stats = analyzer.analyze()
                
                visualizer = NsysVisualizer(parser, analyzer)
                visualizer.output_dir = output_dir / "visualization"
                visualizer.create_visualizations()
                
                reporter = NsysReporter(parser, analyzer)
                reporter.output_dir = output_dir / "reports"
                reporter.generate_report()
                
                return {
                    'file_type': 'nsys',
                    'stats': stats,
                    'visualization_dir': str(visualizer.output_dir),
                    'reports_dir': str(reporter.output_dir)
                }
            
            elif file_path.suffix.lower() in ['.ncu-rep', '.csv'] or 'ncu' in analysis_type:
                # ncuæ–‡ä»¶åˆ†æ
                parser = NCUParser(str(file_path))
                parser.parse()
                
                analyzer = NCUAnalyzer(parser)
                stats = analyzer.analyze()
                
                visualizer = NCUVisualizer(parser, analyzer) 
                visualizer.output_dir = output_dir / "visualization"
                visualizer.create_visualizations()
                
                reporter = NCUReporter(parser, analyzer)
                reporter.output_dir = output_dir / "reports"
                reporter.generate_report()
                
                return {
                    'file_type': 'ncu',
                    'stats': stats,
                    'bottlenecks_count': len(analyzer.bottlenecks),
                    'visualization_dir': str(visualizer.output_dir),
                    'reports_dir': str(reporter.output_dir)
                }
            
            else:
                return {'error': f'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path.suffix}'}
                
        except Exception as e:
            return {'error': f'åˆ†ææ–‡ä»¶å¤±è´¥: {str(e)}'}

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='AI Agentè‡ªåŠ¨LLMæ€§èƒ½åˆ†æå™¨')
    
    subparsers = parser.add_subparsers(dest='command', help='å‘½ä»¤ç±»å‹')
    
    # ä»æç¤ºè¯åˆ†æ
    prompt_parser = subparsers.add_parser('prompt', help='ä»æç¤ºè¯å¼€å§‹åˆ†æ')
    prompt_parser.add_argument('prompt', help='ç”¨æˆ·æç¤ºè¯')
    prompt_parser.add_argument('--workspace', default='.', help='å·¥ä½œç©ºé—´æ ¹ç›®å½•')
    
    # åˆ†æå·²æœ‰æ–‡ä»¶  
    file_parser = subparsers.add_parser('file', help='åˆ†æå·²æœ‰profileæ–‡ä»¶')
    file_parser.add_argument('file_path', help='profileæ–‡ä»¶è·¯å¾„')
    file_parser.add_argument('--analysis-type', choices=['nsys', 'ncu', 'auto'], 
                            default='auto', help='åˆ†æç±»å‹')
    
    # äº¤äº’å¼æ¨¡å¼
    interactive_parser = subparsers.add_parser('interactive', help='äº¤äº’å¼æ¨¡å¼')
    interactive_parser.add_argument('--workspace', default='.', help='å·¥ä½œç©ºé—´æ ¹ç›®å½•')
    
    args = parser.parse_args()
    
    if args.command == 'prompt':
        agent = AIAgentAnalyzer(args.workspace)
        results = agent.analyze_from_prompt(args.prompt)
        
        if 'error' not in results:
            print(f"\nâœ… åˆ†ææˆåŠŸå®Œæˆ")
        else:
            print(f"\nâŒ åˆ†æå¤±è´¥: {results['error']}")
    
    elif args.command == 'file':
        agent = AIAgentAnalyzer()
        results = agent.analyze_existing_files(args.file_path, args.analysis_type)
        
        if 'error' not in results:
            print(f"\nâœ… æ–‡ä»¶åˆ†æå®Œæˆ")
            print(f"ğŸ“Š åˆ†æç±»å‹: {results['file_type']}")
        else:
            print(f"\nâŒ æ–‡ä»¶åˆ†æå¤±è´¥: {results['error']}")
    
    elif args.command == 'interactive':
        agent = AIAgentAnalyzer(args.workspace)
        
        print("ğŸ¤– AI Agentäº¤äº’å¼æ¨¡å¼")
        print("è¾“å…¥'quit'æˆ–'exit'é€€å‡º")
        print("=" * 40)
        
        while True:
            try:
                prompt = input("\nğŸ’¬ è¯·è¾“å…¥åˆ†æéœ€æ±‚: ").strip()
                
                if prompt.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ğŸ‘‹ å†è§!")
                    break
                
                if not prompt:
                    continue
                
                results = agent.analyze_from_prompt(prompt)
                
                if 'error' not in results:
                    print(f"âœ… åˆ†æå®Œæˆ")
                else:
                    print(f"âŒ åˆ†æå¤±è´¥: {results['error']}")
                    
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§!")
                break
            except Exception as e:
                print(f"âŒ æ„å¤–é”™è¯¯: {e}")
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
