#!/usr/bin/env python3
"""
NVIDIA æ€§èƒ½åˆ†æé›†æˆå·¥å…·
å…ˆç”¨ nsys è¯†åˆ«çƒ­ç‚¹kernelsï¼Œå†ç”¨ ncu æ·±åº¦åˆ†æ

å·¥ä½œæµç¨‹ï¼š
1. nsys profile -> è·å–å…¨å±€æ€§èƒ½overview  
2. æå–çƒ­ç‚¹kernelåç§°
3. ncu profile -> é’ˆå¯¹çƒ­ç‚¹kernelsæ·±åº¦åˆ†æ
4. ç»¼åˆåˆ†ææŠ¥å‘Š

ä½œè€…: xjw
ç‰ˆæœ¬: 1.0
"""

import os
import sys
import json
import subprocess
import argparse
import re
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime
import pandas as pd

# å¯¼å…¥æˆ‘ä»¬çš„åˆ†æå·¥å…·
sys.path.append(str(Path(__file__).parent))
from nsys_parser import NsysParser, NsysAnalyzer
from ncu_parser import NCUParser, NCUAnalyzer, NCUVisualizer, NCUReporter

# å¼•å…¥é«˜é˜¶æŠ¥å‘Šä¸çŸ¥è¯†åº“æ‘„å–æ¨¡å—ï¼ˆå¯é€‰ï¼‰
try:
    from backend.advanced_report import generate_advanced_report
except Exception:
    generate_advanced_report = None  # type: ignore
try:
    from backend.knowledge_bases.kb_ingest import ingest_json_to_faiss, flatten_json
except Exception:
    ingest_json_to_faiss = None  # type: ignore
    flatten_json = None  # type: ignore

INTEGRATED_REPORT_PATH = Path("/workspace/Agent/AI_Agent_Complete/sglang_analysis_b8_i512_o64/integrated_performance_report.md")

class NSysToNCUAnalyzer:
    """é›†æˆ nsys å’Œ ncu çš„åˆ†æå·¥å…·

    ç»Ÿä¸€è¾“å‡ºç›®å½•:
        é»˜è®¤ä½¿ç”¨ /workspace/Agent/AI_Agent_Complete ä½œä¸ºæ ¹è·¯å¾„ä¸‹çš„ integrated_analysis å­ç›®å½•ï¼Œ
        ä¾¿äº Agent è¯»å–æ‰€æœ‰ç”Ÿæˆçš„æŠ¥å‘Šå’Œä¸­é—´äº§ç‰©ã€‚
    """
    DEFAULT_BASE_DIR = Path("/workspace/Agent/AI_Agent_Complete")

    def __init__(self, output_dir: str = "integrated_analysis"):
        # å¦‚æœç”¨æˆ·ä¼ å…¥çš„æ˜¯ç»å¯¹è·¯å¾„åˆ™ä½¿ç”¨åŸå€¼ï¼Œå¦åˆ™æ‹¼æ¥åˆ°é»˜è®¤åŸºè·¯å¾„ä¸‹
        base = self.DEFAULT_BASE_DIR
        if output_dir.startswith('/'):
            self.output_dir = Path(output_dir)
        else:
            self.output_dir = base / output_dir
        self.output_dir.mkdir(exist_ok=True)
        self.hot_kernels = []
        self.nsys_stats = {}
        self.ncu_results = {}
        
    def step1_nsys_analysis(self, target_command: List[str], profile_name: str = "overview") -> str:
        """ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨nsysè¿›è¡Œå…¨å±€æ€§èƒ½åˆ†æ"""
        
        nsys_profile = self.output_dir / f"{profile_name}.nsys-rep"
        
        # æ„å»ºnsyså‘½ä»¤
        nsys_cmd = [
            'nsys', 'profile',
            '-o', str(nsys_profile.with_suffix('')),  # nsysä¼šè‡ªåŠ¨æ·»åŠ .nsys-rep
            '-t', 'cuda,nvtx,osrt',
            '--cuda-memory-usage=true',
            '--force-overwrite=true'
        ] + target_command
        
        print("ğŸš€ æ­¥éª¤1: è¿è¡Œnsyså…¨å±€æ€§èƒ½åˆ†æ...")
        print(f"å‘½ä»¤: {' '.join(nsys_cmd)}")
        
        try:
            # è®© nsys çš„è¾“å‡ºç›´æ¥æ‰“å°åˆ°ç»ˆç«¯ï¼Œä¾¿äºå®æ—¶æŸ¥çœ‹ï¼ˆä¸å† capture_outputï¼‰
            # result = subprocess.run(nsys_cmd, check=True)
            result = subprocess.run(nsys_cmd, capture_output=True, text=True, check=True)
            print(f"âœ… nsysåˆ†æå®Œæˆ: {nsys_profile}")
            return str(nsys_profile)
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ nsysåˆ†æå¤±è´¥: {e.stderr}")
            raise
    
    def step2_extract_hot_kernels(self, nsys_file: str, 
                                  top_k: int = 10, 
                                  min_duration_ms: float = 0.1) -> List[Dict]:
        """ç¬¬äºŒæ­¥ï¼šä»nsysç»“æœä¸­æå–çƒ­ç‚¹kernels"""
        
        print("ğŸ” æ­¥éª¤2: ä»nsysç»“æœæå–çƒ­ç‚¹kernels...")
        
        # ä½¿ç”¨æˆ‘ä»¬çš„nsysè§£æå™¨
        parser = NsysParser(nsys_file)
        parser.parse()
        
        if not parser.kernels:
            print("âš ï¸  æœªå‘ç°CUDA kernels")
            return []
        
        # åˆ†ækernels
        analyzer = NsysAnalyzer(parser)
        self.nsys_stats = analyzer.analyze()
        
        # æå–çƒ­ç‚¹kernels
        hot_kernels = []
        
        # æ–¹æ³•1: æŒ‰æ‰§è¡Œæ—¶é—´æ’åº
        kernels_by_time = sorted(parser.kernels, 
                               key=lambda k: k.duration, reverse=True)
        
        # æ–¹æ³•2: æŒ‰è°ƒç”¨æ¬¡æ•°ç»Ÿè®¡
        kernel_stats = {}
        for kernel in parser.kernels:
            name = kernel.name
            if name not in kernel_stats:
                kernel_stats[name] = {
                    'count': 0,
                    'total_time': 0,
                    'max_time': 0,
                    'avg_time': 0
                }
            
            kernel_stats[name]['count'] += 1
            kernel_stats[name]['total_time'] += kernel.duration * 1000  # è½¬ä¸ºms
            kernel_stats[name]['max_time'] = max(kernel_stats[name]['max_time'], 
                                                kernel.duration * 1000)
        
        # è®¡ç®—å¹³å‡æ—¶é—´
        for name, stats in kernel_stats.items():
            stats['avg_time'] = stats['total_time'] / stats['count']
        
        # æŒ‰æ€»æ‰§è¡Œæ—¶é—´æ’åºï¼Œæå–top-k
        sorted_kernels = sorted(kernel_stats.items(), 
                              key=lambda x: x[1]['total_time'], reverse=True)
        
        for kernel_name, stats in sorted_kernels[:top_k]:
            # è¿‡æ»¤æ‰æ‰§è¡Œæ—¶é—´å¤ªçŸ­çš„kernels
            if stats['avg_time'] >= min_duration_ms:
                hot_kernels.append({
                    'name': kernel_name,
                    'total_time_ms': stats['total_time'],
                    'avg_time_ms': stats['avg_time'],
                    'count': stats['count'],
                    'max_time_ms': stats['max_time']
                })
        
        self.hot_kernels = hot_kernels
        
        print(f"ğŸ”¥ è¯†åˆ«åˆ° {len(hot_kernels)} ä¸ªçƒ­ç‚¹kernels:")
        for i, kernel in enumerate(hot_kernels[:5], 1):
            # kernel å¯èƒ½ä¸º dictï¼Œä¹Ÿå¯èƒ½ä¸ºå…¶å®ƒç±»å‹ï¼Œåšé˜²æŠ¤å¤„ç†
            try:
                name_str = kernel['name'] if isinstance(kernel, dict) else kernel
            except Exception:
                name_str = kernel

            # å¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œé˜²æ­¢éå­—ç¬¦ä¸²ç±»å‹å¯¼è‡´åˆ‡ç‰‡é”™è¯¯
            name_str = str(name_str)
            name_short = name_str[:60]
            total = kernel.get('total_time_ms', 0) if isinstance(kernel, dict) else 0
            avg = kernel.get('avg_time_ms', 0) if isinstance(kernel, dict) else 0
            count = kernel.get('count', 0) if isinstance(kernel, dict) else 0

            print(f"  {i}. {name_short}... (æ€»è®¡: {total:.2f}ms, å¹³å‡: {avg:.3f}ms, è°ƒç”¨: {count}æ¬¡)")
        
        # ä¿å­˜çƒ­ç‚¹kernelsåˆ°æ–‡ä»¶
        hot_kernels_file = self.output_dir / "hot_kernels.json"
        with open(hot_kernels_file, 'w', encoding='utf-8') as f:
            json.dump(hot_kernels, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ çƒ­ç‚¹kernelsåˆ—è¡¨å·²ä¿å­˜: {hot_kernels_file}")
        return hot_kernels
    
    def step3_ncu_targeted_analysis(self, target_command: List[str], 
                                   kernels_to_analyze: List[Dict],
                                   max_kernels: int = 5) -> List[str]:
        """ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨ncuå¯¹çƒ­ç‚¹kernelsè¿›è¡Œæ·±åº¦åˆ†æ"""
        
        print("âš¡ æ­¥éª¤3: ä½¿ç”¨ncuæ·±åº¦åˆ†æçƒ­ç‚¹kernels...")
        
        ncu_results = []

        # é™åˆ¶åˆ†ææ•°é‡
        kernels_to_analyze = kernels_to_analyze[:max_kernels]
        
        for i, kernel_info in enumerate(kernels_to_analyze):
            kernel_name = str(kernel_info.get('name', 'kernel')).strip()

            # æ¸…ç†kernelåç§°ï¼Œç”¨äºæ–‡ä»¶å
            safe_name = re.sub(r'[^\w\-_]', '_', kernel_name)[:50]
            ncu_profile = self.output_dir / f"ncu_kernel_{i}_{safe_name}"
            
            # print(f"ğŸ¯ æ­£åœ¨åˆ†ækernel {i+1}/{len(kernels_to_analyze)}: {kernel_name[:60]}...")

             # æ„å»ºncuå‘½ä»¤
            ncu_cmd = [
                'ncu',
                '--kernel-name', kernel_name,
                '--set', 'full',  # æ”¶é›†å®Œæ•´æŒ‡æ ‡é›†
                '-o', str(ncu_profile),
                '--force-overwrite'
            ] + target_command
            
            print(f"ğŸ¯ æ­£åœ¨åˆ†ækernel {i+1}/{len(kernels_to_analyze)}: {kernel_name[:60]}...")
            
            try:
                # è¿è¡Œncu
                print(ncu_cmd)
                # result = subprocess.run(ncu_cmd, capture_output=True, text=True, 
                #                        check=True)  # 5åˆ†é’Ÿè¶…æ—¶
                result = subprocess.run(ncu_cmd)  # 5åˆ†é’Ÿè¶…æ—¶
                ncu_file = str(ncu_profile) + ".ncu-rep"
                if Path(ncu_file).exists():
                    print(f"âœ… kernelåˆ†æå®Œæˆ: {ncu_file}")
                    ncu_results.append(ncu_file)
                    
                    # ç«‹å³å¯¼å‡ºä¸ºCSVä»¥ä¾¿åˆ†æ
                    self._export_ncu_to_csv(ncu_file)
                else:
                    print(f"âš ï¸  NCUæ–‡ä»¶æœªç”Ÿæˆ: {ncu_file}")
                    
            except subprocess.TimeoutExpired:
                print(f"â° kernelåˆ†æè¶…æ—¶: {kernel_name[:60]}")
            except subprocess.CalledProcessError as e:
                print(f"âŒ kernelåˆ†æå¤±è´¥: {kernel_name[:60]} - {e.stderr}")
            except Exception as e:
                print(f"âŒ æ„å¤–é”™è¯¯: {e}")
        
        return ncu_results

    def step3_ncu_global_focus(self, target_command: List[str], hot_kernels: List[Dict], top_focus: int = 5,
                               set_name: str = 'compute', launch_limit: Optional[int] = None) -> Tuple[Optional[str], Dict[str, Dict]]:
        """æ›¿ä»£å®šå‘åˆ†æï¼šä¸€æ¬¡å…¨é‡ ncu é‡‡é›†ï¼Œç„¶åä»…é’ˆå¯¹ nsys å‘ç°çš„å‰ top_focus ä¸ªçƒ­ç‚¹ kernel æå–ä¸å½’å¹¶æŒ‡æ ‡ã€‚

        è¿”å›: (å…¨é‡ ncu æŠ¥å‘Šè·¯å¾„æˆ– None, focus_metrics dict)
        focus_metrics ç»“æ„ (é”®ä¸ºçƒ­ç‚¹ kernel åŸå):
            {
              kernel_display_name: {
                  'kernels_analyzed': int,
                  'gpu_utilization': {...},
                  'memory_analysis': {...},
                  'bottleneck_summary': [...]
              }
            }
        """
        if not hot_kernels:
            print("âš ï¸ æ— çƒ­ç‚¹ kernelï¼Œè·³è¿‡å…¨é‡ NCU é‡‡é›†")
            return None, {}
        # è¿è¡Œä¸€æ¬¡å…¨é‡é‡‡é›†
        full_rep = self.full_ncu_capture(target_command, profile_name='ncu_full_capture_global', set_name=set_name, launch_limit=launch_limit)
        if not full_rep:
            return None, {}
        csv_file = full_rep.replace('.ncu-rep', '.csv')
        if not Path(csv_file).exists() or Path(csv_file).stat().st_size == 0:
            print("âš ï¸ å…¨é‡é‡‡é›† CSV ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œæ— æ³•æå–ç„¦ç‚¹å†…æ ¸æŒ‡æ ‡")
            return full_rep, {}
        # è§£æ CSV
        try:
            parser = NCUParser(csv_file)
            parser.parse()
        except Exception as e:
            print(f"âš ï¸ å…¨é‡ CSV è§£æå¤±è´¥: {e}")
            return full_rep, {}
        # æ„å»ºç„¦ç‚¹åˆ†æ
        focus = {}
        # å»ºç«‹å¿«é€Ÿåˆ—è¡¨
        metrics_list = parser.kernels  # List[KernelMetrics]
        def _match_entries(target: str) -> List[Any]:
            t_low = target.lower()
            matched = [km for km in metrics_list if t_low in km.name.lower() or km.name.lower() in t_low]
            # è‹¥æ— ç›´æ¥åŒ…å«åŒ¹é…ï¼Œå°è¯•æŒ‰åˆ†è¯å…¬å…±å­ä¸² >=5 char
            if not matched:
                # ç®€å•åˆ‡å‰²éå­—æ¯æ•°å­—
                import re
                tokens = [tok for tok in re.split(r'[^A-Za-z0-9_]+', t_low) if len(tok) >= 5]
                if tokens:
                    for tok in tokens:
                        part = [km for km in metrics_list if tok in km.name.lower()]
                        matched.extend(part)
            # å»é‡
            uniq = []
            seen = set()
            for m in matched:
                if id(m) not in seen:
                    seen.add(id(m)); uniq.append(m)
            return uniq[:50]  # é˜²æ­¢è¿‡å¤š
        def _avg(vals: List[Optional[float]]) -> Optional[float]:
            nums = [v for v in vals if isinstance(v, (int, float))]
            return sum(nums)/len(nums) if nums else None
        focus_targets = hot_kernels[:top_focus]
        for hk in focus_targets:
            kname = str(hk.get('name',''))
            entries = _match_entries(kname)
            if not entries:
                continue
            sm_eff = _avg([e.sm_efficiency for e in entries])
            occ = _avg([e.achieved_occupancy for e in entries])
            dram = _avg([e.dram_bandwidth for e in entries])
            l2 = _avg([e.l2_hit_rate for e in entries])
            warp_eff = _avg([e.warp_execution_efficiency for e in entries])
            tensor_active = _avg([e.tensor_active for e in entries])
            # ç“¶é¢ˆåˆ¤å®š (å¯å‘å¼)
            bottlenecks = []
            def add_bottleneck(cond: bool, desc: str, severity: str):
                if cond:
                    bottlenecks.append({'type': 'heuristic', 'severity': severity, 'description': desc})
            add_bottleneck(sm_eff is not None and sm_eff < 40, 'SMæ•ˆç‡åä½', 'high')
            add_bottleneck(dram is not None and dram < 150, 'å†…å­˜å¸¦å®½å¯èƒ½å—é™', 'medium')
            add_bottleneck(occ is not None and occ < 25, 'Occupancyè¾ƒä½', 'medium')
            add_bottleneck(warp_eff is not None and warp_eff < 70, 'Warpæ‰§è¡Œæ•ˆç‡ä¸€èˆ¬', 'low')
            focus[kname] = {
                'kernels_analyzed': len(entries),
                'gpu_utilization': {
                    'average_sm_efficiency': sm_eff,
                    'achieved_occupancy': occ,
                    'tensor_core_active': tensor_active,
                },
                'memory_analysis': {
                    'bandwidth_stats': {
                        'average_bandwidth': dram,
                        'l2_hit_rate': l2,
                    }
                },
                'bottleneck_summary': bottlenecks
            }
        print(f"ğŸ” å…¨é‡é‡‡é›†ä¸­å·²ç”Ÿæˆ {len(focus)} ä¸ªç„¦ç‚¹å†…æ ¸çš„èšåˆæŒ‡æ ‡")
        return full_rep, focus
    
    def _export_ncu_to_csv(self, ncu_file: str) -> Optional[str]:
        """å¯¼å‡ºncuç»“æœä¸ºCSVæ ¼å¼"""
        csv_file = ncu_file.replace('.ncu-rep', '.csv')
        
        export_cmd = [
            'ncu', '--csv',
            '--log-file', csv_file,
            '--import', ncu_file
        ]
        
        try:
            subprocess.run(export_cmd, capture_output=True, text=True, check=True)
            if Path(csv_file).exists():
                return csv_file
        except:
            pass
        
        return None
    
    def step4_comprehensive_analysis(self, ncu_files: List[str], focus_metrics: Optional[Dict[str, Dict]] = None) -> Dict:
        """ç¬¬å››æ­¥ï¼šç»¼åˆåˆ†ænsyså’Œncuç»“æœ"""
        
        print("ğŸ“Š æ­¥éª¤4: ç»¼åˆåˆ†æç»“æœ...")
        
        comprehensive_results = {
            'timestamp': datetime.now().isoformat(),
            'nsys_overview': self.nsys_stats,
            'hot_kernels_count': len(self.hot_kernels),
            'ncu_detailed_analysis': {},
            'ncu_focus_analysis': focus_metrics or {}
        }
        
        # åˆ†ææ¯ä¸ªncuç»“æœ
        # è‹¥æä¾›ç„¦ç‚¹èšåˆæŒ‡æ ‡ï¼Œåˆ™ä¸å¿…å¯¹å…¨é‡ ncu_full_capture_global é€æ–‡ä»¶åšæ ‡å‡†åˆ†æï¼ˆä»å¯ä¿ç•™ targeted æ–‡ä»¶åˆ†æï¼‰
        for ncu_file in ncu_files:
            csv_file = ncu_file.replace('.ncu-rep', '.csv')
            
            if Path(csv_file).exists():
                try:
                    # ä½¿ç”¨æˆ‘ä»¬çš„ncuåˆ†æå™¨
                    parser = NCUParser(csv_file)
                    parser.parse()
                    
                    analyzer = NCUAnalyzer(parser)
                    stats = analyzer.analyze()
                    
                    kernel_name = Path(ncu_file).stem
                    comprehensive_results['ncu_detailed_analysis'][kernel_name] = {
                        'kernels_analyzed': len(parser.kernels),
                        'bottlenecks_found': len(analyzer.bottlenecks),
                        'gpu_utilization': stats.get('gpu_utilization', {}),
                        'memory_analysis': stats.get('memory_analysis', {}),
                        'bottleneck_summary': [
                            {
                                'type': b.type,
                                'severity': b.severity,
                                'description': b.description
                            }
                            for b in analyzer.bottlenecks[:3]  # åªå–å‰3ä¸ª
                        ]
                    }
                    
                    # ç”Ÿæˆè¯¦ç»†çš„å¯è§†åŒ–æŠ¥å‘Š
                    visualizer = NCUVisualizer(parser, analyzer)
                    vis_output_dir = self.output_dir / f"visualization_{kernel_name}"
                    visualizer.output_dir = vis_output_dir
                    vis_output_dir.mkdir(exist_ok=True)
                    visualizer.create_visualizations()
                    
                except Exception as e:
                    print(f"âš ï¸  åˆ†æ {ncu_file} å¤±è´¥: {e}")
        
        # ä¿å­˜ç»¼åˆåˆ†æç»“æœ
        results_file = self.output_dir / "comprehensive_analysis.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“‹ ç»¼åˆåˆ†æç»“æœå·²ä¿å­˜: {results_file}")
        return comprehensive_results
    
    def generate_final_report(self, comprehensive_results: Dict) -> str:
        """ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Šï¼Œä¿ç•™å·²æœ‰çš„ NCU æŠ¥å‘Šå—"""
        report_file = self.output_dir / "integrated_performance_report.md"
        start_tag = "<!-- NCU_REPORT_START -->"
        end_tag = "<!-- NCU_REPORT_END -->"

        existing_ncu_block = ""
        if report_file.exists():
            old = report_file.read_text(encoding='utf-8')
            import re
            m = re.search(f"{start_tag}.*?{end_tag}", old, flags=re.DOTALL)
            if m:
                existing_ncu_block = m.group(0)

        lines = []
        lines.append("# é›†æˆæ€§èƒ½åˆ†ææŠ¥å‘Š\n")
        lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        # nsys æ¦‚è§ˆ
        lines.append("## ğŸ” Nsys å…¨å±€æ€§èƒ½æ¦‚è§ˆ\n")
        nsys_overview = comprehensive_results.get('nsys_overview', {})
        if 'kernel_analysis' in nsys_overview:
            ks = nsys_overview['kernel_analysis']
            lines.append(f"- æ€»kernelsæ•°é‡: {ks.get('total_kernels', 0)}")
            lines.append(f"- æ€»kernelæ‰§è¡Œæ—¶é—´: {ks.get('total_kernel_time', 0):.2f} ms")
            lines.append(f"- å¹³å‡kernelæ‰§è¡Œæ—¶é—´: {ks.get('avg_kernel_time', 0):.3f} ms\n")

        # çƒ­ç‚¹ kernels
        lines.append(f"## ğŸ”¥ è¯†åˆ«çš„çƒ­ç‚¹Kernels ({comprehensive_results.get('hot_kernels_count', 0)}ä¸ª)\n")
        for i, kernel in enumerate(self.hot_kernels[:10], 1):
            name = kernel.get('name', '')[:80]
            lines.append(f"{i}. {name}")
            lines.append(f"   - æ€»æ‰§è¡Œæ—¶é—´: {kernel.get('total_time_ms',0):.2f} ms")
            lines.append(f"   - å¹³å‡æ‰§è¡Œæ—¶é—´: {kernel.get('avg_time_ms',0):.3f} ms")
            lines.append(f"   - è°ƒç”¨æ¬¡æ•°: {kernel.get('count',0)}\n")

        # NCU æ·±åº¦ï¼ˆå ä½ï¼ŒçœŸæ­£å†…å®¹ç”± ncu_parser æ’å…¥å—ä¿ç•™ï¼‰
        lines.append("## âš¡ NCU æ·±åº¦åˆ†æç»“æœ\n")
        if existing_ncu_block:
            lines.append("ï¼ˆä¿ç•™å·²æœ‰ NCU æŠ¥å‘Šå—ï¼‰\n")
        else:
            lines.append("ï¼ˆå°šæœªç”Ÿæˆ NCU æŠ¥å‘Šï¼Œè¿è¡Œ ncu_parser.py åä¼šè‡ªåŠ¨æ’å…¥ï¼‰\n")

        # ç„¦ç‚¹èšåˆ
        focus_analysis = comprehensive_results.get('ncu_focus_analysis', {})
        if focus_analysis:
            lines.append("## ğŸ¯ ç„¦ç‚¹å†…æ ¸èšåˆæŒ‡æ ‡\n")
            for kname, a in focus_analysis.items():
                lines.append(f"### {kname}")
                gu = a.get('gpu_utilization', {})
                mem = a.get('memory_analysis', {}).get('bandwidth_stats', {})
                if gu:
                    lines.append(f"- å¹³å‡SMæ•ˆç‡: {gu.get('average_sm_efficiency','N/A')}")
                    lines.append(f"- Occupancy: {gu.get('achieved_occupancy','N/A')}")
                if mem:
                    lines.append(f"- å¹³å‡å¸¦å®½: {mem.get('average_bandwidth','N/A')} GB/s")
                    lines.append(f"- L2å‘½ä¸­ç‡: {mem.get('l2_hit_rate','N/A')}%")
                bsum = a.get('bottleneck_summary', [])
                if bsum:
                    lines.append("- ä¸»è¦ç“¶é¢ˆ:")
                    for b in bsum:
                        lines.append(f"  - {b['description']} ({b['severity']})")
                lines.append("")

        # ä¼˜åŒ–å»ºè®®
        lines.append("## ğŸ’¡ ä¼˜åŒ–å»ºè®®\n")
        lines.append("- å…³æ³¨çƒ­ç‚¹kernelçš„è°ƒåº¦ä¸å¹¶è¡Œé‡å ")
        lines.append("- é’ˆå¯¹ä½SMæ•ˆç‡kernelä¼˜åŒ–ç®—æ³•/æ‰¹æ¬¡")
        lines.append("- é’ˆå¯¹ä½å¸¦å®½/ä½å‘½ä¸­ç‡kernelä¼˜åŒ–å†…å­˜è®¿é—®\n")

        # åˆå¹¶æ–‡æœ¬
        report_text = "\n".join(lines).rstrip() + "\n"
        # è‹¥æœ‰æ—§ NCU å—ï¼Œé™„åŠ åœ¨æœ«å°¾ï¼ˆä¿æŒæ ‡è®°å®Œæ•´ï¼‰
        if existing_ncu_block:
            report_text += "\n" + existing_ncu_block + "\n"

        report_file.write_text(report_text, encoding='utf-8')
        print(f"ğŸ“„ æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ(ä¿ç•™ NCU å—): {report_file}")
        return str(report_file)

def create_sglang_analysis_workflow():
    """åˆ›å»ºSGlangä¸“ç”¨çš„åˆ†æå·¥ä½œæµ"""
    DEFAULT_MODEL_DIR = os.getenv('SGLANG_MODEL_PATH') or os.getenv('MODEL_PATH') or '/workspace/models/'

    def run_sglang_integrated_analysis(model_path: Optional[str] = None, 
                                      batch_size: int = 8,
                                      input_len: int = 512, 
                                      output_len: int = 64):
        """è¿è¡ŒSGlangçš„é›†æˆåˆ†æ

        å‚æ•°:
            model_path: æ¨¡å‹è·¯å¾„ï¼Œè‹¥æœªæä¾›åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ SGLANG_MODEL_PATH / MODEL_PATHï¼Œæœ€åå›é€€ /workspace/models/
        """
        if not model_path:
            model_path = DEFAULT_MODEL_DIR.rstrip('/')
            print(f"â„¹ï¸ æœªæä¾› model_pathï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {model_path}")
        # æ„å»ºSGlangå‘½ä»¤
        sglang_cmd = [
            'python', '-m', 'sglang.bench_one_batch',
            '--model-path', model_path,
            '--batch-size', str(batch_size),
            '--input-len', str(input_len),
            '--output-len', str(output_len),
            '--load-format', 'dummy'
        ]
        
        # åˆ›å»ºåˆ†æå™¨
        analyzer = NSysToNCUAnalyzer(f"sglang_analysis_b{batch_size}_i{input_len}_o{output_len}")
        
        # æ­¥éª¤1: nsyså…¨å±€åˆ†æ
        nsys_file = analyzer.step1_nsys_analysis(sglang_cmd, "sglang_overview")
        
        # æ­¥éª¤2: æå–çƒ­ç‚¹kernels
        hot_kernels = analyzer.step2_extract_hot_kernels(nsys_file, top_k=8)
        
        if not hot_kernels:
            print("âŒ æœªå‘ç°çƒ­ç‚¹kernelsï¼Œåˆ†æç»ˆæ­¢")
            return
        
        # æ­¥éª¤3: ncuæ·±åº¦åˆ†æï¼ˆé™åˆ¶åˆ†ææ•°é‡ï¼‰
        ncu_files = analyzer.step3_ncu_targeted_analysis(sglang_cmd, hot_kernels, max_kernels=3)
        
        # æ­¥éª¤4: ç»¼åˆåˆ†æ
        results = analyzer.step4_comprehensive_analysis(ncu_files)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        report_file = analyzer.generate_final_report(results)
        
        print(f"\nğŸ‰ SGlangé›†æˆåˆ†æå®Œæˆ!")
        print(f"ğŸ“ åˆ†æç»“æœç›®å½•: {analyzer.output_dir}")
        print(f"ğŸ“„ åˆ†ææŠ¥å‘Š: {report_file}")
        
        return analyzer.output_dir
    
    return run_sglang_integrated_analysis

# --- è¾…åŠ©å‡½æ•°: å°†é«˜é˜¶æŠ¥å‘Š Markdown ç²—ç•¥ç»“æ„åŒ–ä¸º JSON ---
def _extract_advanced_json(md_text: str) -> Dict[str, Any]:  # type: ignore
    sections: Dict[str, Any] = {}
    current = None
    for line in md_text.splitlines():
        if line.startswith('#'):
            # è·å–æ ‡é¢˜
            title = line.strip('# ').strip()
            current = title
            sections[current] = []
        else:
            if current is not None:
                sections[current].append(line)
    # ç®€å•æŠ½å–ä»»åŠ¡åˆ—è¡¨ä¸åˆ†ç±»
    tasks = []
    for k, v in sections.items():
        if 'ä»»åŠ¡åˆ—è¡¨' in k or 'ç»†ç²’åº¦' in k:
            tasks.extend([ln for ln in v if ln.strip().startswith('- ')])
    summary = sections.get('6. æ€»ç»“ (Summary)', [])
    return {
        'sections': list(sections.keys()),
        'tasks_lines': tasks,
        'summary': '\n'.join(summary[:10]),
        'raw_length': len(md_text)
    }

class NCUParser:
    """NCU æŠ¥å‘Šè§£æå™¨ï¼ˆæ”¯æŒæ–°æ—§æ ¼å¼ï¼‰"""

    def __init__(self, file_path: str):
        self.file_path = Path(file_path)
        self.kernels = []  # type: List[KernelMetrics]
        self.metadata = {}
        self._is_legacy = False  # æ ‡è®°æ˜¯å¦ä¸ºæ—§ç‰ˆå®½è¡¨æ ¼å¼

    def parse(self) -> None:
        """ä¸»è§£æå‡½æ•°"""
        if not self.file_path.exists():
            print(f"âŒ NCU æ–‡ä»¶ä¸å­˜åœ¨: {self.file_path}")
            return
        
        # å°è¯•è§£æä¸º CSV æ ¼å¼
        if str(self.file_path).endswith('.csv'):
            self._parse_csv()
        else:
            # å°è¯•æ–°ç‰ˆ JSON æ ¼å¼
            json_file = self.file_path.with_suffix('.json')
            if json_file.exists():
                self._parse_json(json_file)
            else:
                print("âš ï¸ æ— æ³•è¯†åˆ«çš„æ–‡ä»¶æ ¼å¼æˆ–æ–‡ä»¶ç¼ºå¤± (éœ€æ‰‹åŠ¨æ£€æŸ¥):", self.file_path)
    
    def _parse_csv(self) -> None:
        """è§£æ CSV æ ¼å¼çš„ NCU æŠ¥å‘Š"""
        try:
            # å°è¯•è¯»å–ä¸ºé•¿è¡¨æ ¼å¼
            df = pd.read_csv(self.file_path)
            print(f"ğŸ“Š æ£€æµ‹åˆ°é•¿è¡¨æ ¼å¼ï¼Œè¡Œæ•°: {len(df)}")
            self._parse_csv_kernels(df)
            self.metadata['total_kernels'] = len(self.kernels)
            return
        except Exception as e:
            print(f"âš ï¸ é•¿è¡¨æ ¼å¼è§£æå¤±è´¥: {e}")
        
        try:
            # å°è¯•è¯»å–ä¸ºå®½è¡¨æ ¼å¼
            df = pd.read_csv(self.file_path, header=None)
            print(f"ğŸ“Š æ£€æµ‹åˆ°å®½è¡¨æ ¼å¼ï¼Œè¡Œæ•°: {len(df)}")
            self._parse_csv_wide(df)
            self.metadata['total_kernels'] = len(self.kernels)
            return
        except Exception as e:
            print(f"âš ï¸ å®½è¡¨æ ¼å¼è§£æå¤±è´¥: {e}")
        
        print("âŒ CSV æ–‡ä»¶è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
    
    def _parse_csv_kernels(self, df: pd.DataFrame) -> None:
        """å…¼å®¹ Nsight Compute å¯¼å‡ºçš„é•¿è¡¨/å®½è¡¨ CSV"""
        kernel_name_col = None
        for cand in ['Kernel Name','KernelName','Name']:
            if cand in df.columns:
                kernel_name_col = cand
                break
        if kernel_name_col is None:
            print("âš ï¸ æœªæ‰¾åˆ° Kernel Name åˆ—ï¼Œé€€å›é€šç”¨è§£æ")
            return

        # é•¿è¡¨æ£€æµ‹
        is_long = {'Section Name','Metric Name','Metric Value'}.issubset(df.columns)

        if is_long:
            # æ¸…ç†æ•°å€¼
            df['Metric Value'] = df['Metric Value'].astype(str).str.replace(',', '', regex=False)
            df['Metric Value'] = pd.to_numeric(df['Metric Value'], errors='coerce')

            for kname, kdf in df.groupby(kernel_name_col):
                km = KernelMetrics(name=str(kname))

                def get(section, metric):
                    sel = kdf[(kdf['Section Name'] == section) & (kdf['Metric Name'] == metric)]['Metric Value']
                    return None if sel.empty else float(sel.mean())

                # åŸºç¡€å½¢çŠ¶ï¼ˆåªå–ç¬¬ä¸€æ¬¡å‡ºç°çš„ Block / Gridï¼‰
                try:
                    b = str(kdf['Block Size'].dropna().iloc[0])
                    g = str(kdf['Grid Size'].dropna().iloc[0])
                    def parse_shape(s):
                        s = s.strip().strip('()')
                        parts = [int(p.strip()) for p in s.split(',')]
                        return tuple(parts)
                    km.block_size = parse_shape(b)
                    km.grid_size = parse_shape(g)
                except Exception:
                    pass

                # æ˜ å°„æ ¸å¿ƒæŒ‡æ ‡
                km.sm_efficiency = get('Compute Workload Analysis','SM Busy') \
                                   or get('GPU Speed Of Light Throughput','Compute (SM) Throughput')

                km.achieved_occupancy = get('Occupancy','Achieved Occupancy')
                km.theoretical_occupancy = get('Occupancy','Theoretical Occupancy')

                # æ—¶é•¿(us)
                dur_us = get('GPU Speed Of Light Throughput','Duration')
                if dur_us is not None:
                    km.duration = dur_us / 1000.0  # ms

                # å¸¦å®½ä¸å‘½ä¸­ç‡
                bw_gbps = get('Memory Workload Analysis','Memory Throughput')  # Gbyte/s
                if bw_gbps is None:
                    bw_gbps = get('Memory Workload Analysis','DRAM Throughput')  # å¯èƒ½æ˜¯ %
                km.dram_bandwidth = bw_gbps
                km.l2_hit_rate = get('Memory Workload Analysis','L2 Hit Rate')
                l1 = get('Memory Workload Analysis','L1/TEX Hit Rate')
                km.l1_hit_rate = l1

                # å¯„å­˜å™¨ä¸å…±äº«å†…å­˜ï¼ˆåŠ¨æ€+é™æ€ï¼‰
                regs = get('Launch Statistics','Registers Per Thread')
                if regs is not None:
                    km.registers_per_thread = int(regs)
                dyn_shm = get('Launch Statistics','Dynamic Shared Memory Per Block')
                sta_shm = get('Launch Statistics','Static Shared Memory Per Block')
                if dyn_shm is not None or sta_shm is not None:
                    dyn_bytes = (dyn_shm or 0) * 1024.0  # Kbyte -> byte
                    sta_bytes = sta_shm or 0
                    km.shared_memory_per_block = int(dyn_bytes + sta_bytes)

                # Warp æ‰§è¡Œæ•ˆç‡ï¼ˆç²—ç•¥ï¼šAvg. Not Predicated Off Threads /32ï¼‰
                not_off = get('Warp State Statistics','Avg. Not Predicated Off Threads Per Warp')
                if not_off is not None:
                    km.warp_execution_efficiency = min(100.0, (not_off / 32.0) * 100.0)

                self.kernels.append(km)

            print(f"âœ… é•¿è¡¨è§£æå®Œæˆ: {len(self.kernels)} kernels")
            return

        # å®½è¡¨æ—§é€»è¾‘ï¼ˆä¿ç•™ï¼‰
        for kernel_name in df[kernel_name_col].unique():
            row = df[df[kernel_name_col] == kernel_name].iloc[0]
            km = KernelMetrics(name=str(kernel_name))
            mapping = {
                'SM Efficiency':'sm_efficiency',
                'Achieved Occupancy':'achieved_occupancy',
                'Theoretical Occupancy':'theoretical_occupancy',
                'DRAM Bandwidth':'dram_bandwidth',
                'L2 Hit Rate':'l2_hit_rate',
                'L1 Hit Rate':'l1_hit_rate',
                'Duration':'duration',
                'Registers Per Thread':'registers_per_thread'
            }
            for col, attr in mapping.items():
                if col in df.columns and pd.notna(row[col]):
                    setattr(km, attr, row[col])
            self.kernels.append(km)
        print(f"âœ… å®½è¡¨è§£æå®Œæˆ: {len(self.kernels)} kernels")

class NCUReporter:
    """NCU æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, parser: NCUParser, analyzer: NCUAnalyzer, output_dir: str):
        self.parser = parser
        self.analyzer = analyzer
        self.output_dir = output_dir

    def _safe_pct(self, v):
        try:
            return f"{float(v):.1f}%"
        except:
            return "N/A"

    def _safe_num(self, v, unit=""):
        try:
            return f"{float(v):.2f}{unit}"
        except:
            return "N/A"

    def generate_report(self) -> str:
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Šå¹¶å†™å…¥é›†æˆ Markdown"""
        stats = self.analyzer.stats.get('gpu_utilization', {})
        lines = []
        lines.append("### NCU åˆ†æç»“æœ (è‡ªåŠ¨æ’å…¥)\n")
        lines.append(f"- Kernel æ•°é‡: {len(self.parser.kernels)}")
        if stats:
            lines.append(f"- å¹³å‡ SM Busy: {self._safe_pct(stats.get('average_sm_efficiency'))}")
            lines.append(f"- SM Busy æœ€ä½: {self._safe_pct(stats.get('min_sm_efficiency'))}")
            lines.append(f"- ä½äº50% çš„ Kernel æ•°: {stats.get('kernels_below_50_percent',0)}")
        # ç®€è¦åˆ—å‡ºå‰è‹¥å¹² kernel
        lines.append("\n#### å…³é”® Kernel æŒ‡æ ‡ (å‰10)")
        for k in self.parser.kernels[:10]:
            lines.append(
                f"- {k.name[:80]} | SM Busy: {self._safe_pct(k.sm_efficiency)} | Occ: {self._safe_pct(k.achieved_occupancy)} | L2 Hit: {self._safe_pct(k.l2_hit_rate)} | BW: {self._safe_num(k.dram_bandwidth,'')}"
            )

        report_text = "\n".join(lines)

        # å†™å…¥å•ç‹¬æ–‡ä»¶ï¼ˆä¿ç•™åŸè¡Œä¸ºï¼‰
        out_path = Path(self.output_dir) / "ncu_report.txt"
        out_path.parent.mkdir(exist_ok=True, parents=True)
        out_path.write_text(report_text, encoding='utf-8')

        # é›†æˆå†™å…¥ integrated_performance_report.md
        self._update_integrated_markdown(report_text)

        print(f"ğŸ“„ NCUæŠ¥å‘Šå·²ç”Ÿæˆ: {out_path}")
        print(f"ğŸ“ å·²æ›´æ–°é›†æˆæŠ¥å‘Š: {INTEGRATED_REPORT_PATH}")
        return report_text

    def _update_integrated_markdown(self, ncu_section: str):
        start_tag = "<!-- NCU_REPORT_START -->"
        end_tag   = "<!-- NCU_REPORT_END -->"
        block = f"{start_tag}\n{ncu_section}\n{end_tag}\n"

        if INTEGRATED_REPORT_PATH.exists():
            content = INTEGRATED_REPORT_PATH.read_text(encoding='utf-8')
            if start_tag in content and end_tag in content:
                # æ›¿æ¢æ—§å—
                content = re.sub(
                    f"{start_tag}.*?{end_tag}",
                    block,
                    content,
                    flags=re.DOTALL
                )
            else:
                # è¿½åŠ 
                content += "\n" + block
        else:
            content = "# é›†æˆæ€§èƒ½åˆ†ææŠ¥å‘Š\n\n" + block
        INTEGRATED_REPORT_PATH.write_text(content, encoding='utf-8')

def main():
    parser = argparse.ArgumentParser(description='é›†æˆ nsys å’Œ ncu çš„æ€§èƒ½åˆ†æå·¥å…·',
                                     epilog='ç¤ºä¾‹: python nsys_to_ncu_analyzer.py -- python -m sglang.bench_one_batch --model-path /path --batch-size 8 --input-len 512 --output-len 64 --load-format dummy')
    parser.add_argument('command', nargs='*', help='è¦åˆ†æçš„å‘½ä»¤ (å¦‚æœªä½¿ç”¨ --raw-cmd, å¯ç”¨ "--" åˆ†éš”)')
    parser.add_argument('--output-dir', default='integrated_analysis', help='è¾“å‡ºç›®å½• (é»˜è®¤æ ¹: /workspace/Agent/AI_Agent_Complete)')
    parser.add_argument('--top-k', type=int, default=10, help='æå–çš„çƒ­ç‚¹kernelæ•°é‡')
    parser.add_argument('--max-ncu-kernels', type=int, default=5, help='ncuåˆ†æçš„æœ€å¤§kernelæ•°é‡')
    parser.add_argument('--min-duration', type=float, default=0.1, help='æœ€å°kernelæ‰§è¡Œæ—¶é—´(ms)')
    parser.add_argument('--full-ncu', action='store_true', help='æ‰§è¡Œä¸€æ¬¡ä¸åšè¿‡æ»¤çš„å…¨é‡ NCU é‡‡é›† (ä¸çƒ­ç‚¹åˆ†æå¹¶è¡Œæˆ–æ›¿ä»£)')
    parser.add_argument('--full-ncu-set', default='compute', help='å…¨é‡é‡‡é›†ä½¿ç”¨çš„ NCU æŒ‡æ ‡é›†åˆ (--set å€¼)')
    parser.add_argument('--full-ncu-launch-limit', type=int, default=None, help='é™åˆ¶å…¨é‡é‡‡é›†çš„ kernel æ¬¡æ•° (--launch-count)')
    parser.add_argument('--save-hot-kernels', action='store_true', help='ä¿å­˜çƒ­ç‚¹ kernel åç§°åˆ° hot_kernels.txt (çº¯æ–‡æœ¬)')

    # é«˜é˜¶æŠ¥å‘Šç›¸å…³å‚æ•°
    parser.add_argument('--advanced-report', action='store_true', help='ç”Ÿæˆé«˜é˜¶ä¼˜åŒ–å»ºè®®æŠ¥å‘Š (advanced_performance_report.md)')
    parser.add_argument('--advanced-detailed', action='store_true', help='é«˜é˜¶æŠ¥å‘ŠåŒ…å«è¯¦ç»†æŒ‡æ ‡å¿«ç…§ä¸ç»†ç²’åº¦ Kernel ä»»åŠ¡')
    parser.add_argument('--advanced-json', action='store_true', help='åŒæ—¶å¯¼å‡ºé«˜é˜¶æŠ¥å‘Šä¸º JSON (advanced_performance_report.json) ä»¥ä¾›çŸ¥è¯†åº“æ‘„å–')
    parser.add_argument('--ingest-advanced', action='store_true', help='å°†é«˜é˜¶æŠ¥å‘Š JSON å†™å…¥çŸ¥è¯†åº“ (éœ€è¦ embedding ç¯å¢ƒå¯ç”¨)')
    parser.add_argument('--kb-path', type=str, default='knowledge_store', help='çŸ¥è¯†åº“å­˜å‚¨ç›®å½• (FAISS)')
    
    # SGlangç‰¹æ®Šå‚æ•°
    parser.add_argument('--sglang-model', type=str, help='SGlangæ¨¡å‹è·¯å¾„ (é»˜è®¤: ç¯å¢ƒå˜é‡ SGLANG_MODEL_PATH / MODEL_PATH æˆ– /workspace/models/)')
    parser.add_argument('--force-model-path', type=str, help='å¼ºåˆ¶è¦†ç›–ç›®æ ‡å‘½ä»¤ä¸­çš„ --model-path å‚æ•°ä¸ºæŒ‡å®šè·¯å¾„ (å¯ç”¨ç¯å¢ƒå˜é‡ FORCE_MODEL_PATH)')
    parser.add_argument('--sglang-batch', type=int, default=8, help='SGlangæ‰¹æ¬¡å¤§å°')
    parser.add_argument('--sglang-input-len', type=int, default=512, help='SGlangè¾“å…¥é•¿åº¦')
    parser.add_argument('--sglang-output-len', type=int, default=64, help='SGlangè¾“å‡ºé•¿åº¦')
    
    # å…è®¸æœªçŸ¥å‚æ•°ä¿ç•™ç»™ç›®æ ‡å‘½ä»¤
    known_args, unknown_tail = parser.parse_known_args()
    if getattr(known_args, 'command', None):
        base_cmd = known_args.command
    else:
        base_cmd = []
    # å¦‚æœç”¨æˆ·ä½¿ç”¨ -- åˆ†éš”å½¢å¼: python script.py -- <target command parts>
    # unknown_tail å°±æ˜¯åç»­çš„çœŸå®å‘½ä»¤å‚æ•°
    target_command = []
    if unknown_tail:
        target_command = unknown_tail
    else:
        target_command = base_cmd
    # å¯é€‰å¼ºåˆ¶æ¨¡å‹è·¯å¾„æ›¿æ¢
    force_model_path = known_args.force_model_path or os.getenv('FORCE_MODEL_PATH')
    if force_model_path:
        def _rewrite_model_path(cmd: List[str]) -> List[str]:
            out = []
            skip_next = False
            for i, token in enumerate(cmd):
                if skip_next:
                    skip_next = False
                    continue
                if token == '--model-path':
                    out.append('--model-path')
                    out.append(force_model_path)
                    skip_next = True  # è·³è¿‡åŸè·¯å¾„
                    # åŸè·¯å¾„ä¸¢å¼ƒ
                elif token.startswith('--model-path='):
                    out.append(f'--model-path={force_model_path}')
                else:
                    out.append(token)
            return out
        before = ' '.join(target_command)
        target_command = _rewrite_model_path(target_command)
        after = ' '.join(target_command)
        if before != after:
            print(f"ğŸ”§ å·²å¼ºåˆ¶è¦†ç›– --model-path: {force_model_path}")
        else:
            # å¦‚æœåŸå‘½ä»¤æœªåŒ…å« --model-pathï¼Œç›´æ¥è¿½åŠ 
            target_command += ['--model-path', force_model_path]
            print(f"ğŸ”§ åŸå‘½ä»¤æœªåŒ…å« --model-pathï¼Œå·²è¿½åŠ : {force_model_path}")

    if not target_command:
        print('âŒ æœªæä¾›å¾…åˆ†æçš„ç›®æ ‡å‘½ä»¤ã€‚ç¤ºä¾‹: python nsys_to_ncu_analyzer.py -- python -m sglang.bench_one_batch --model-path ...')
        return
    args = known_args
    
    try:
        # SGlang ä¸“ç”¨è·¯å¾„é»˜è®¤å¤„ç†
        sglang_workflow = create_sglang_analysis_workflow()
        if args.sglang_model or os.getenv('SGLANG_MODEL_PATH') or os.getenv('MODEL_PATH'):
            sglang_workflow(
                args.sglang_model or os.getenv('SGLANG_MODEL_PATH') or os.getenv('MODEL_PATH'),
                args.sglang_batch,
                args.sglang_input_len,
                args.sglang_output_len
            )
        else:
            # é€šç”¨åˆ†æ
            analyzer = NSysToNCUAnalyzer(args.output_dir)
            
            # æ­¥éª¤1-4
            nsys_file = analyzer.step1_nsys_analysis(target_command)
            hot_kernels = analyzer.step2_extract_hot_kernels(nsys_file, args.top_k, args.min_duration)
            if args.save_hot_kernels and hot_kernels:
                # å·²åœ¨ step2 å†…éƒ¨ç”Ÿæˆ hot_kernels.txtï¼Œè¿™é‡Œåªåšæç¤º
                print("ğŸ“ --save-hot-kernels å·²å¯ç”¨ï¼Œhot_kernels.txt å·²ç”Ÿæˆã€‚")

            full_capture_file = None
            if args.full_ncu:
                full_capture_file = analyzer.full_ncu_capture(
                    target_command,
                    profile_name='ncu_full_capture',
                    set_name=args.full_ncu_set,
                    launch_limit=args.full_ncu_launch_limit
                )

            ncu_files = []
            if hot_kernels:
                targeted = analyzer.step3_ncu_targeted_analysis(target_command, hot_kernels, args.max_ncu_kernels)
                ncu_files.extend(targeted)
            else:
                print('âŒ æœªå‘ç°ç¬¦åˆæ¡ä»¶çš„çƒ­ç‚¹kernels (è·³è¿‡å®šå‘ NCU)')

            if full_capture_file:
                ncu_files.append(full_capture_file)

            advanced_json_obj = None
            if ncu_files:
                results = analyzer.step4_comprehensive_analysis(ncu_files)
                analyzer.generate_final_report(results)
                # ç”Ÿæˆé«˜é˜¶æŠ¥å‘Š
                if args.advanced_report and generate_advanced_report:
                    try:
                        adv_path = generate_advanced_report(analyzer.output_dir, detailed=args.advanced_detailed)
                        print(f"ğŸ§  é«˜é˜¶ä¼˜åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {adv_path}")
                        if args.advanced_json:
                            # å°†ç”Ÿæˆçš„ markdown è½¬ä¸ºç®€å•ç»“æ„åŒ– JSON (æå–éƒ¨åˆ†æ®µè½)
                            md_text = Path(adv_path).read_text(encoding='utf-8')
                            advanced_json_obj = _extract_advanced_json(md_text)
                            json_path = analyzer.output_dir / 'advanced_performance_report.json'
                            json_path.write_text(json.dumps(advanced_json_obj, ensure_ascii=False, indent=2), encoding='utf-8')
                            print(f"ğŸ“¦ é«˜é˜¶æŠ¥å‘Š JSON å·²å¯¼å‡º: {json_path} â¡ï¸ å¯ç”¨äºçŸ¥è¯†åº“æ‘„å–")
                        # å¯é€‰æ‘„å–çŸ¥è¯†åº“
                        if args.ingest_advanced and advanced_json_obj and ingest_json_to_faiss and flatten_json:
                            try:
                                texts = [json.dumps(advanced_json_obj, ensure_ascii=False)]
                                ingest_json_to_faiss(json.dumps(advanced_json_obj, ensure_ascii=False), kb_path=args.kb_path)
                                print("ğŸ“¥ å·²å°è¯•å°†é«˜é˜¶æŠ¥å‘Šå†™å…¥çŸ¥è¯†åº“å‘é‡åº“")
                            except Exception as e:
                                print(f"âš ï¸ é«˜é˜¶æŠ¥å‘ŠçŸ¥è¯†åº“æ‘„å–å¤±è´¥: {e}")
                    except Exception as e:
                        print(f"âš ï¸ é«˜é˜¶æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
                elif args.advanced_report and not generate_advanced_report:
                    print("âš ï¸ advanced_report æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡é«˜é˜¶æŠ¥å‘Šç”Ÿæˆ")
            else:
                print('âš ï¸ æœªç”Ÿæˆä»»ä½• NCU æŠ¥å‘Š, ç»“æŸã€‚')
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­åˆ†æ")
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

