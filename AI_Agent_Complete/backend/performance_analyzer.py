import asyncio
import os
import re
from pathlib import Path
import sys
from typing import Any, Dict, List, Optional, Tuple, Union

from offline_llm_v3 import get_offline_qwen_client
from utils.nsys_to_ncu_analyzer import create_sglang_analysis_workflow


class PerformanceAnalyzer:
    """
    ä»åŸ agent_core.py è¿ç§»å‡ºæ¥çš„æ€§èƒ½åˆ†ææ¨¡å—ï¼Œå°è£…æˆclassã€‚
    æ–¹ä¾¿æ€§èƒ½åˆ†æçš„ç®¡ç†ã€‚
    å°½é‡ä¸æ”¹å˜åŸæœ‰é€»è¾‘ä¸è¾“å‡ºå†…å®¹ï¼Œåªåšæœ€å°å°è£…ä¸ä¾èµ–æ³¨å…¥ã€‚

    ä¾èµ–ï¼š
    - llm_client: ç”¨äºç”Ÿæˆè¡¨æ ¼/å»ºè®®ï¼ˆä¿æŒåŸå®ç°ï¼‰
    - workflow_factory: create_sglang_analysis_workflow (å‡½æ•°)
    """

    def __init__(
        self,
        llm_client: Any,
        workflow_factory: Any,
        results_dir: Union[str, Path] = "results",
    ):
        self.llm_client = llm_client

        # SGLang workflow å‡½æ•°
        self.workflow_factory = workflow_factory

        # è¾“å‡ºç›®å½•
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True, parents=True)

        # åˆ†æç»“æœç¼“å­˜
        self.last_analysis_dir: Optional[str] = None
        self.last_analysis_dirs: List[str] = []
        self.last_analysis_reports: List[str] = []
        self.last_analysis_table: Optional[str] = None
        self.last_analysis_suggestions: Optional[str] = None

    async def run_analysis(
        self, model_path: str, analysis_type: str, params: Dict
    ) -> str:
        """
        æ‰§è¡Œæ€§èƒ½åˆ†æ
        æ³¨æ„ï¼šanalysis_type å‚æ•°ä¸å‚ä¸é€»è¾‘åˆ†æ”¯ï¼ˆä¿æŒåŸä»£ç è¡Œä¸ºï¼‰ã€‚
        """

        results = []

        # å‚æ•°æå–ï¼ˆåªå–ç¬¬ä¸€ç»„å‚æ•°ï¼›analysis_type ä¸ç”Ÿæ•ˆï¼‰
        batch_sizes = params.get("batch_size", [1])
        input_lens = params.get("input_len", [128])
        output_lens = params.get("output_len", [1])

        # åªåˆ†æç¬¬1ç»„å‚æ•°ï¼ˆé¿å…æ—¶é—´è¿‡é•¿ï¼‰
        batch_size = batch_sizes[0] if isinstance(batch_sizes, list) else batch_sizes
        input_len = input_lens[0] if isinstance(input_lens, list) else input_lens
        output_len = output_lens[0] if isinstance(output_lens, list) else output_lens

        try:
            # åˆ›å»ºåˆ†æå·¥ä½œæµ
            analysis_workflow = self.workflow_factory()
            workflow_output = await asyncio.get_event_loop().run_in_executor(
                None,
                analysis_workflow,
                str(model_path),
                batch_size,
                input_len,
                output_len,
            )

            run_records: List[Tuple[str, Path]] = []

            # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸ºåˆ—è¡¨ï¼ˆå¤„ç†å¤šå¡æˆ–å¤šæ¬¡è¿è¡Œçš„æƒ…å†µï¼‰
            if isinstance(workflow_output, list):
                for idx, item in enumerate(workflow_output):
                    output_path = (
                        item.get("dir") or item.get("path")
                        if isinstance(item, dict)
                        else str(item)
                    )
                    gpu_label = (
                        str(item.get("gpu", idx))
                        if isinstance(item, dict)
                        else str(idx)
                    )
                    if output_path:
                        run_records.append((gpu_label, Path(output_path)))
            elif workflow_output:
                run_records.append(("0", Path(str(workflow_output))))

            if not run_records:
                results.append("âš ï¸ **åˆ†æå·²å®Œæˆï¼Œä½†æœªæ‰¾åˆ°è¾“å‡ºç›®å½•**")
                return "\n".join(results)

            report_infos = []
            missing_reports = []
            for idx, (gpu_label, output_dir) in enumerate(run_records):
                report_path = output_dir / "integrated_performance_report.md"
                if report_path.exists():
                    report_text = report_path.read_text(encoding="utf-8")
                    report_infos.append(
                        {
                            "gpu": gpu_label,
                            "dir": output_dir,
                            "report": report_path,
                            "text": report_text,
                            "index": idx,
                        }
                    )
                else:
                    missing_reports.append(output_dir)

            if not report_infos:
                dir_lines = "\n".join(f"  â€¢ {path}" for _, path in run_records)
                results.append(
                    f"""
                    âš ï¸ **åˆ†æå·²å®Œæˆï¼Œä½†æœªç”ŸæˆæŠ¥å‘Šæ–‡ä»¶**

                    ğŸ“ ç»“æœç›®å½•:
                    {dir_lines}
                    ğŸ’¡ è¯·æ£€æŸ¥ç›®å½•ä¸­çš„å…¶ä»–è¾“å‡ºæ–‡ä»¶
                """
                )
                return "\n".join(results)

            primary_info = report_infos[0]
            self.last_analysis_dir = str(primary_info["dir"])
            self.last_analysis_reports = [str(info["report"]) for info in report_infos]
            summary = self._extract_report_summary(primary_info["text"])

            try:
                loop = asyncio.get_event_loop()
                if len(report_infos) > 1:
                    table_markdown = self._generate_multi_gpu_table(
                        [info["text"] for info in report_infos],
                        [info["gpu"] for info in report_infos],
                    )
                else:
                    table_markdown = await loop.run_in_executor(
                        None, self._generate_report_table, primary_info["text"]
                    )
            except Exception as table_exc:
                table_markdown = f"âš ï¸ è¡¨æ ¼ç”Ÿæˆå¤±è´¥: {table_exc}"

            self.last_analysis_table = table_markdown

            suggestions = self._generate_optimization_suggestions(report_infos)
            self.last_analysis_suggestions = suggestions if suggestions else None

            dir_lines = "\n".join(
                f"  â€¢ {self._format_gpu_label(info['gpu'], info['index'])}: {info['dir']}"
                for info in report_infos
            )

            missing_lines = ""
            if missing_reports:
                missing_lines = "\n".join(f"  â€¢ {path}" for path in missing_reports)
                missing_lines = f"\nâš ï¸ æœªæ‰¾åˆ°ä»¥ä¸‹ç›®å½•çš„æŠ¥å‘Šæ–‡ä»¶:\n{missing_lines}\n"

            results.append(
                f"""
                âœ… **åˆ†æå®Œæˆ!**

                ğŸ“ **ç»“æœç›®å½•**:
                {dir_lines}
                ğŸ“„ **æŠ¥å‘Šæ–‡ä»¶**: {primary_info["report"]}
                {missing_lines}
                {summary}

                ğŸ“Œ **çƒ­ç‚¹Kernelè¡¨æ ¼é¢„è§ˆ**:
                {table_markdown}

                ğŸ’¡ **ä¼˜åŒ–å»ºè®®**:
                {suggestions or "æš‚æœªç”Ÿæˆä¼˜åŒ–å»ºè®®"}

                ğŸ” **è¯¦ç»†æŠ¥å‘Š**: è¯·æŸ¥çœ‹ {primary_info["report"]}
                ğŸ“Š **å¯è§†åŒ–å›¾è¡¨**: è¯·æŸ¥çœ‹å¯¹åº”ç»“æœç›®å½•ä¸­çš„å›¾ç‰‡æ–‡ä»¶
            """
            )

        except Exception as e:
            import traceback

            error_detail = traceback.format_exc()
            results.append(
                f"""
                âŒ **åˆ†ææ‰§è¡Œå¤±è´¥**

                é”™è¯¯ä¿¡æ¯: {str(e)}

                è¯¦ç»†é”™è¯¯:
                ```
                {error_detail}
                ```

                ğŸ’¡ **å¸¸è§é—®é¢˜è§£å†³**:
                1. ç¡®ä¿å·²å®‰è£… nsys å’Œ ncu å·¥å…·
                2. ç¡®ä¿ SGlang å·²æ­£ç¡®å®‰è£…
                3. ç¡®ä¿æ¨¡å‹æ–‡ä»¶è·¯å¾„æ­£ç¡®
                4. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜
            """
            )

        return "\n".join(results)

    def _extract_report_summary(self, report_content: str) -> str:
        """ä»æŠ¥å‘Šä¸­æå–å…³é”®æ‘˜è¦ä¿¡æ¯"""

        lines = report_content.split("\n")
        summary_lines = []

        # æå–å…³é”®ç»Ÿè®¡ä¿¡æ¯
        for i, line in enumerate(lines):
            if "æ€»kernelsæ•°é‡" in line or "æ€»kernelæ‰§è¡Œæ—¶é—´" in line:
                summary_lines.append(line)
            elif "ğŸ”¥ è¯†åˆ«çš„çƒ­ç‚¹Kernels" in line:
                # æå–å‰3ä¸ªçƒ­ç‚¹kernel
                summary_lines.append("\n**ğŸ”¥ çƒ­ç‚¹Kernels (Top 3):**")
                for j in range(i + 1, min(i + 10, len(lines))):
                    if lines[j].strip() and lines[j].startswith(("1.", "2.", "3.")):
                        summary_lines.append(lines[j][:100])
                break

        return "\n".join(summary_lines) if summary_lines else ""

    def _generate_multi_gpu_table(
        self, report_texts: List[str], gpu_labels: List[str]
    ) -> str:
        """ç”Ÿæˆå¤šGPUæŠ¥å‘Šè¡¨æ ¼ï¼ˆæ³¨æ„ï¼šåŸå®ç°å¹¶æœªçœŸæ­£å¯¹é½å¤š GPU æ•°æ®ï¼‰"""

        if not report_texts:
            return "âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„æŠ¥å‘Šå†…å®¹"

        entries = self._parse_kernel_entries_from_report(report_texts[0])
        header = (
            "| Kernel | " + " | ".join([f"{lbl} Duration" for lbl in gpu_labels]) + " |"
        )
        sep = "|---" * (len(gpu_labels) + 1) + "|"
        rows = []
        for entry in entries[:5]:  # Top 5
            rows.append(
                f"| {entry['name']} | {entry['duration']} |"
                + " ... |" * (len(gpu_labels) - 1)
            )
        return f"{header}\n{sep}\n" + "\n".join(rows)

    def _parse_kernel_entries_from_report(
        self, report_text: str
    ) -> List[Dict[str, str]]:
        entries: List[Dict[str, str]] = []
        lines = report_text.splitlines()
        idx = 0
        total_lines = len(lines)
        while idx < total_lines:
            raw_line = lines[idx]
            if raw_line.strip().startswith("äºŒã€"):
                break
            match = re.match(r"^\s*\d+\.\s+(.*)$", raw_line)
            if match:
                name = match.group(1).strip()
                duration = ""
                ratio = ""
                idx += 1
                while idx < total_lines:
                    line = lines[idx].strip()
                    if line.startswith("- æ‰§è¡Œæ—¶é—´"):
                        dur_match = re.search(r"([0-9.]+)\s*ms", line)
                        if dur_match:
                            duration = dur_match.group(1)
                    elif line.startswith("- æ—¶é—´å æ¯”"):
                        ratio_match = re.search(r"([0-9.]+)\s*%", line)
                        if ratio_match:
                            ratio = ratio_match.group(1)
                    elif re.match(r"^\s*\d+\.", lines[idx]) or line.startswith("äºŒã€"):
                        break
                    idx += 1
                entries.append({"name": name, "duration": duration, "ratio": ratio})
            else:
                idx += 1
        return entries

    @staticmethod
    def _collect_ncu_csv_snippets(
        output_dir: Path, limit: int = 1500
    ) -> List[Tuple[str, str]]:
        """
        ä»è¾“å‡ºç›®å½•æ”¶é›† NCU ç”Ÿæˆçš„ CSV æ–‡ä»¶ç‰‡æ®µï¼Œç”¨äºè¾…åŠ© LLM åˆ†æã€‚
        Args:
            output_dir: ç»“æœç›®å½•
            limit: æ¯ä¸ªæ–‡ä»¶è¯»å–çš„å­—ç¬¦æ•°é™åˆ¶ï¼ˆé˜²æ­¢ Prompt çˆ†ç‚¸ï¼‰
        """
        
        snippets: List[Tuple[str, str]] = []
        if not output_dir.exists():
            return snippets

        # æŸ¥æ‰¾ ncu_kernel*.csv æ–‡ä»¶
        for csv_path in sorted(output_dir.glob("*.csv")):
            # ç®€å•è¿‡æ»¤ï¼Œåªçœ‹å¯èƒ½æœ‰ç”¨çš„æ•°æ®æ–‡ä»¶
            if (
                "ncu" not in csv_path.name.lower()
                and "profile" not in csv_path.name.lower()
            ):
                continue

            try:
                # åªè¯»å–å‰ N ä¸ªå­—ç¬¦
                raw = csv_path.read_text(encoding="utf-8", errors="ignore")
                snippet = raw[:limit]
                if snippet.strip():
                    snippets.append((csv_path.name, snippet))
            except Exception:
                continue

        return snippets

    def _generate_optimization_suggestions(
        self, report_infos: List[Dict[str, Any]]
    ) -> str:
        """
        åŸºäºåˆ†ææŠ¥å‘Šå’ŒåŸå§‹ CSV æ•°æ®ç”Ÿæˆä¼˜åŒ–å»ºè®®ã€‚ï¼ˆåŸæ ·è¿ç§»ï¼šä»è°ƒç”¨ llm_clientï¼‰
        """
        if not report_infos:
            return ""

        # 1. å‡†å¤‡ä¸Šä¸‹æ–‡æ•°æ®
        context_parts = []

        for info in report_infos:
            gpu_lbl = self._format_gpu_label(info["gpu"], info["index"])
            context_parts.append(
                f"=== æŠ¥å‘Š ({gpu_lbl}) ===\n{info.get('text', '')[:2000]}"
            )

            output_dir = Path(info["dir"])
            csv_data = self._collect_ncu_csv_snippets(output_dir)
            if csv_data:
                for name, content in csv_data:
                    context_parts.append(
                        f"=== åŸå§‹æ•°æ® ({gpu_lbl}/{name}) ===\n{content}\n..."
                    )

        full_context = "\n\n".join(context_parts)

        # 2. æ„å»º Prompt
        prompt = f"""
        ä½ æ˜¯ä¸€ä½ CUDA æ€§èƒ½ä¼˜åŒ–ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹æä¾›çš„ã€æ€§èƒ½åˆ†ææŠ¥å‘Šã€‘å’Œã€åŸå§‹é‡‡æ ·æ•°æ®ã€‘ï¼Œç»™å‡ºå…·ä½“çš„ä¼˜åŒ–å»ºè®®ã€‚

        ### åˆ†ææ•°æ®
        {full_context}

        ### ä½ çš„ä»»åŠ¡
        è¯·åˆ†æä¸Šè¿°æ•°æ®ï¼Œå¹¶è¾“å‡ºä¸€æ®µ Markdown æ ¼å¼çš„ä¼˜åŒ–å»ºè®®ã€‚

        è¦æ±‚ï¼š
        1. **è¯†åˆ«ç“¶é¢ˆ**ï¼šæŒ‡å‡ºæœ€è€—æ—¶çš„ Kernel åŠå…¶å¯èƒ½çš„åŸå› ï¼ˆå¦‚ Memory Bound, Compute Bound, Latency Boundï¼‰ã€‚
        2. **å…·ä½“å»ºè®®**ï¼š
           - å¦‚æœæ˜¯ Memory Boundï¼Œå»ºè®®æ£€æŸ¥æ˜¾å­˜åˆå¹¶è®¿é—®ã€Shared Memory ä½¿ç”¨ç­‰ã€‚
           - å¦‚æœæ˜¯ Compute Boundï¼Œå»ºè®®æ£€æŸ¥ Tensor Core åˆ©ç”¨ç‡ã€‚
           - å¦‚æœå‘ç°å¤§é‡å° Kernelï¼Œå»ºè®®è€ƒè™‘ Kernel Fusionã€‚
        3. **ç®€æ´ä¸“ä¸š**ï¼šä¸è¦åºŸè¯ï¼Œç›´æ¥åˆ—å‡º Top 3 å»ºè®®ã€‚

        ### ä¼˜åŒ–å»ºè®®ï¼š
        """.strip()

        # 3. è°ƒç”¨ LLM
        try:
            suggestion = self.llm_client.generate(
                prompt, max_tokens=1024, mode="conversation"
            )
            return suggestion
        except Exception as e:
            return f"âš ï¸ æ— æ³•ç”Ÿæˆä¼˜åŒ–å»ºè®®: {e}"

    @staticmethod
    def _generate_report_table(report_text: str) -> str:
        """
        [Legacy] é™æ€æ–¹æ³•ï¼Œä¾›å¤–éƒ¨æ—§ä»£ç è°ƒç”¨ã€‚ï¼ˆåŸæ ·è¿ç§»ï¼‰
        æ³¨æ„ï¼šåŸå®ç°ä¼šåœ¨å†…éƒ¨é‡æ–°æ„é€  clientï¼ˆä¿æŒä¸æ”¹ï¼‰
        ç”¨report_to_tableæ–¹æ³•ç”Ÿæˆè¡¨æ ¼ã€‚
        """
        from offline_llm_v3 import get_offline_qwen_client

        model_path = Path(
            os.getenv(
                "QWEN_LOCAL_MODEL_PATH",
                "/workspaces/ai-agent/AI_Agent_Complete/.models/Qwen3-4B-Instruct-2507",
            )
        )

        try:
            client = get_offline_qwen_client(model_path)
            return client.report_to_table(report_text)
        except Exception as e:
            return f"âš ï¸ è¡¨æ ¼ç”Ÿæˆå¤±è´¥ (Legacy Mode): {e}"

    @staticmethod
    def _format_gpu_label(label: str, index: int) -> str:
        if not label:
            return f"GPU{index}"
        normalized = label.strip()
        if not normalized:
            return f"GPU{index}"
        if normalized.lower().startswith("gpu"):
            return normalized.upper()
        return f"GPU{normalized}"


if __name__ == "__main__":
    # 1. åˆ›å»ºæµ‹è¯•æ¨¡å‹
    test_model_path = "/workspace/models/Llama-2-7b-hf"

    # 2. åˆå§‹åŒ– LLM Client (ä½¿ç”¨çœŸå®å®¢æˆ·ç«¯ï¼Œä½†ä¸ä¾èµ–å®é™…ç”Ÿæˆç»“æœ)
    # æ³¨æ„: è¿™é‡Œä½¿ç”¨ offline_llm_v3 çš„å®¢æˆ·ç«¯ï¼Œä½†å®é™…æµ‹è¯•ä¸ä¾èµ–å…¶è¾“å‡º
    offline_qwen_path = Path("/workspace/models/Qwen3-30B-A3B-Instruct-2507")
    llm_client = get_offline_qwen_client(offline_qwen_path)

    # 3. åˆ›å»º PerformanceAnalyzer å®ä¾‹
    analyzer = PerformanceAnalyzer(
        llm_client=llm_client,
        workflow_factory=create_sglang_analysis_workflow,
        results_dir="test_results",
    )

    # 4. æ‰§è¡Œæ€§èƒ½åˆ†æï¼ˆä½¿ç”¨æµ‹è¯•æ¨¡å‹ï¼‰
    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹æ‰§è¡Œæ€§èƒ½åˆ†ææµ‹è¯• (ä½¿ç”¨çœŸå® sglang åˆ†ææµç¨‹)")
    print(f"æµ‹è¯•æ¨¡å‹è·¯å¾„: {test_model_path}")
    print("=" * 60)

    try:
        # æ‰§è¡Œåˆ†æï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰
        result = asyncio.run(
            analyzer.run_analysis(
                model_path=test_model_path,
                analysis_type="all",
                params={"batch_size": 1, "input_len": 128, "output_len": 1},
            )
        )

        print("\n" + "=" * 60)
        print("âœ… åˆ†æå®Œæˆ! ç»“æœæ‘˜è¦:")
        print("=" * 60)
        print(result)

        # 5. éªŒè¯ç»“æœç›®å½•
        if analyzer.last_analysis_dir:
            print(f"\nğŸ” ç»“æœç›®å½•: {analyzer.last_analysis_dir}")
            print(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {analyzer.last_analysis_reports[0]}")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°åˆ†æç»“æœç›®å½• (å¯èƒ½åˆ†æå¤±è´¥)")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}")
        print("ğŸ’¡ è¯·æ£€æŸ¥ä»¥ä¸‹äº‹é¡¹:")
        print("1. ç¡®ä¿å·²å®‰è£… sglang å’Œ nsys/ncu å·¥å…·")
        print("2. ç¡®ä¿æµ‹è¯•æ¨¡å‹è·¯å¾„å­˜åœ¨ (å½“å‰: test_model)")
        print("3. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ GPU å†…å­˜")
        sys.exit(1)
