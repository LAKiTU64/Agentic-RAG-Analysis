#!/usr/bin/env python3
"""
åŸºäºLangChainçš„AI Agent LLMæ€§èƒ½åˆ†æå™¨

é›†æˆLangChainæ¡†æ¶ï¼Œæä¾›æ›´æ™ºèƒ½çš„å¯¹è¯ã€å·¥å…·è°ƒç”¨å’Œå·¥ä½œæµç¨‹ç®¡ç†
"""

# Path setup for imports
import sys
from pathlib import Path
_current_dir = Path(__file__).parent
sys.path.insert(0, str(_current_dir.parent / 'original_version'))
sys.path.insert(0, str(_current_dir.parent.parent / 'TOOLS' / 'Auto_Anlyze_tool'))

import json
import asyncio
from typing import Dict, List, Optional, Any, Type
from datetime import datetime
from pathlib import Path
import traceback

# LangChain imports
from langchain.agents import AgentType, initialize_agent, AgentExecutor
from langchain.agents.tools import BaseTool
from langchain.schema import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory
from langchain.callbacks.manager import CallbackManagerForToolRun
from langchain.tools import tool
from langchain.chains import ConversationChain, LLMChain
from langchain.prompts import PromptTemplate
from langchain.schema import OutputParserException
from pydantic import BaseModel, Field

# å¯¼å…¥ç°æœ‰çš„AI Agentç»„ä»¶
from ai_agent_analyzer import AIAgentAnalyzer, PromptParser, ConfigGenerator, AnalysisRequest
from web_agent_backend import ConfigFileParser

# å°è¯•å¯¼å…¥OpenAIï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿ
try:
    from langchain.llms import OpenAI
    from langchain.chat_models import ChatOpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

class MockLLM:
    """æ¨¡æ‹ŸLLMç±»ï¼Œç”¨äºæ²¡æœ‰OpenAI APIæ—¶çš„æµ‹è¯•"""
    
    def __init__(self, *args, **kwargs):
        pass
    
    def __call__(self, prompt: str) -> str:
        return self._generate_mock_response(prompt)
    
    def _generate_mock_response(self, prompt: str) -> str:
        """åŸºäºæç¤ºç”Ÿæˆæ¨¡æ‹Ÿå›ç­”"""
        if "åˆ†æ" in prompt or "analyze" in prompt.lower():
            return "æˆ‘å°†ä¸ºæ‚¨è¿›è¡ŒLLMæ€§èƒ½åˆ†æã€‚è¯·æä¾›æ¨¡å‹åç§°å’Œåˆ†æå‚æ•°ã€‚"
        elif "é…ç½®" in prompt or "config" in prompt.lower():
            return "æˆ‘å·²è§£ææ‚¨çš„é…ç½®æ–‡ä»¶ï¼Œå¹¶ç”Ÿæˆäº†ç›¸åº”çš„å»ºè®®ã€‚"
        elif "å»ºè®®" in prompt or "recommend" in prompt.lower():
            return "åŸºäºåˆ†æç»“æœï¼Œæˆ‘å»ºè®®ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆç‡ã€‚"
        else:
            return "æˆ‘æ˜¯LLMæ€§èƒ½åˆ†æåŠ©æ‰‹ï¼Œå¯ä»¥å¸®æ‚¨åˆ†ææ¨¡å‹æ€§èƒ½ã€è§£æé…ç½®æ–‡ä»¶ã€æä¾›ä¼˜åŒ–å»ºè®®ã€‚"

# LangChainå·¥å…·å®šä¹‰
class PromptAnalysisTool(BaseTool):
    """æç¤ºè¯åˆ†æå·¥å…·"""
    
    name = "prompt_analyzer"
    description = """
    åˆ†æç”¨æˆ·è¾“å…¥çš„è‡ªç„¶è¯­è¨€æç¤ºï¼Œæå–LLMæ€§èƒ½åˆ†æéœ€æ±‚ã€‚
    è¾“å…¥ï¼šç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æè¿°
    è¾“å‡ºï¼šç»“æ„åŒ–çš„åˆ†æè¯·æ±‚å‚æ•°
    
    ç¤ºä¾‹è¾“å…¥ï¼š'åˆ†ællama-7bæ¨¡å‹ï¼Œbatch_size=8,16'
    """
    
    def __init__(self):
        super().__init__()
        self.parser = PromptParser()
    
    def _run(
        self, 
        query: str, 
        run_manager: Optional[CallbackManagerForToolRun] = None
    ) -> str:
        """è¿è¡Œæç¤ºè¯åˆ†æ"""
        try:
            request = self.parser.parse_prompt(query)
            result = {
                "status": "success",
                "model_name": request.model_name,
                "analysis_type": request.analysis_type,
                "batch_size": request.batch_size,
                "input_len": request.input_len,
                "output_len": request.output_len,
                "script_type": request.script_type
            }
            return json.dumps(result, ensure_ascii=False)
        except Exception as e:
            return json.dumps({
                "status": "error",
                "message": f"æç¤ºè¯è§£æå¤±è´¥: {str(e)}"
            }, ensure_ascii=False)

class ConfigAnalysisTool(BaseTool):
    """é…ç½®æ–‡ä»¶åˆ†æå·¥å…·"""
    
    name = "config_analyzer"
    description = """
    åˆ†æä¸Šä¼ çš„JSON/YAMLé…ç½®æ–‡ä»¶ï¼Œæå–æ¨¡å‹ä¿¡æ¯å’Œåˆ†æå‚æ•°ã€‚
    è¾“å…¥ï¼šé…ç½®æ–‡ä»¶å†…å®¹(JSONæ ¼å¼å­—ç¬¦ä¸²)
    è¾“å‡ºï¼šè§£æçš„é…ç½®ä¿¡æ¯å’Œæ™ºèƒ½å»ºè®®
    """
    
    def __init__(self):
        super().__init__()
        self.parser = ConfigFileParser()
    
    def _run(
        self, 
        config_content: str, 
        run_manager: Optional[CallbackManagerForToolRun] = None
    ) -> str:
        """è¿è¡Œé…ç½®æ–‡ä»¶åˆ†æ"""
        try:
            # è§£æé…ç½®æ–‡ä»¶
            parsed_info = self.parser.parse_json_config(config_content)
            
            result = {
                "status": "success",
                "model_info": parsed_info["model_info"],
                "analysis_params": parsed_info["analysis_params"],
                "hardware_info": parsed_info["hardware_info"],
                "suggestions": parsed_info["suggestions"]
            }
            return json.dumps(result, ensure_ascii=False, indent=2)
        except Exception as e:
            return json.dumps({
                "status": "error", 
                "message": f"é…ç½®æ–‡ä»¶åˆ†æå¤±è´¥: {str(e)}"
            }, ensure_ascii=False)

class PerformanceAnalysisTool(BaseTool):
    """æ€§èƒ½åˆ†ææ‰§è¡Œå·¥å…·"""
    
    name = "performance_analyzer"
    description = """
    æ‰§è¡ŒLLMæ€§èƒ½åˆ†æï¼Œæ”¯æŒnsyså…¨å±€åˆ†æã€ncuæ·±åº¦åˆ†ææˆ–é›†æˆåˆ†æã€‚
    è¾“å…¥ï¼šåˆ†æè¯·æ±‚å‚æ•°(JSONæ ¼å¼)
    è¾“å‡ºï¼šåˆ†æç»“æœå’ŒæŠ¥å‘Šè·¯å¾„
    """
    
    def __init__(self):
        super().__init__()
        self.analyzer = AIAgentAnalyzer()
    
    def _run(
        self, 
        analysis_params: str, 
        run_manager: Optional[CallbackManagerForToolRun] = None
    ) -> str:
        """è¿è¡Œæ€§èƒ½åˆ†æ"""
        try:
            # è§£æåˆ†æå‚æ•°
            params = json.loads(analysis_params)
            
            # æ„å»ºæç¤ºè¯
            prompt = self._build_prompt_from_params(params)
            
            # æ‰§è¡Œåˆ†æï¼ˆè¿™é‡Œç®€åŒ–ä¸ºæ¨¡æ‹Ÿï¼‰
            result = {
                "status": "success",
                "analysis_type": params.get("analysis_type", "auto"),
                "model_name": params.get("model_name", "unknown"),
                "message": "æ€§èƒ½åˆ†æå·²å¯åŠ¨",
                "output_dir": f"analysis_{params.get('model_name', 'model')}_{datetime.now().strftime('%H%M%S')}",
                "estimated_time": "5-10åˆ†é’Ÿ"
            }
            return json.dumps(result, ensure_ascii=False, indent=2)
            
        except Exception as e:
            return json.dumps({
                "status": "error",
                "message": f"æ€§èƒ½åˆ†æå¯åŠ¨å¤±è´¥: {str(e)}"
            }, ensure_ascii=False)
    
    def _build_prompt_from_params(self, params: Dict) -> str:
        """ä»å‚æ•°æ„å»ºæç¤ºè¯"""
        model_name = params.get("model_name", "unknown")
        analysis_type = params.get("analysis_type", "auto")
        batch_size = params.get("batch_size", [8])
        
        if isinstance(batch_size, list):
            batch_str = ",".join(map(str, batch_size))
        else:
            batch_str = str(batch_size)
        
        return f"åˆ†ææ¨¡å‹ {model_name}ï¼Œä½¿ç”¨ {analysis_type} åˆ†æï¼Œbatch_size: {batch_str}"

class OptimizationAdvisorTool(BaseTool):
    """æ€§èƒ½ä¼˜åŒ–å»ºè®®å·¥å…·"""
    
    name = "optimization_advisor"
    description = """
    åŸºäºåˆ†æç»“æœæä¾›æ€§èƒ½ä¼˜åŒ–å»ºè®®å’Œæœ€ä½³å®è·µã€‚
    è¾“å…¥ï¼šåˆ†æç»“æœæ•°æ®(JSONæ ¼å¼)
    è¾“å‡ºï¼šè¯¦ç»†çš„ä¼˜åŒ–å»ºè®®å’Œæ“ä½œæ­¥éª¤
    """
    
    def _run(
        self, 
        analysis_results: str, 
        run_manager: Optional[CallbackManagerForToolRun] = None
    ) -> str:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        try:
            results = json.loads(analysis_results)
            
            # ç”ŸæˆåŸºäºç»“æœçš„å»ºè®®
            suggestions = self._generate_optimization_suggestions(results)
            
            result = {
                "status": "success",
                "optimization_suggestions": suggestions,
                "priority_actions": [
                    "æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨ç‡",
                    "ä¼˜åŒ–batch_sizeè®¾ç½®",
                    "è€ƒè™‘ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ",
                    "åˆ†ækernelæ‰§è¡Œæ•ˆç‡"
                ]
            }
            return json.dumps(result, ensure_ascii=False, indent=2)
            
        except Exception as e:
            return json.dumps({
                "status": "error",
                "message": f"ç”Ÿæˆä¼˜åŒ–å»ºè®®å¤±è´¥: {str(e)}"
            }, ensure_ascii=False)
    
    def _generate_optimization_suggestions(self, results: Dict) -> List[str]:
        """åŸºäºç»“æœç”Ÿæˆå»ºè®®"""
        suggestions = []
        
        model_name = results.get("model_name", "")
        analysis_type = results.get("analysis_type", "")
        
        # åŸºäºæ¨¡å‹ç±»å‹çš„å»ºè®®
        if "7b" in model_name.lower():
            suggestions.append("ğŸ¯ 7Bæ¨¡å‹ä¼˜åŒ–ï¼šæ¨èbatch_size=8-16ï¼Œä½¿ç”¨FP16ç²¾åº¦")
        elif "13b" in model_name.lower():
            suggestions.append("ğŸ¯ 13Bæ¨¡å‹ä¼˜åŒ–ï¼šè€ƒè™‘tensorå¹¶è¡Œï¼Œbatch_size=4-8")
        
        # åŸºäºåˆ†æç±»å‹çš„å»ºè®®
        if analysis_type == "nsys":
            suggestions.append("ğŸ“Š NSysåˆ†æå»ºè®®ï¼šå…³æ³¨timelineä¸­çš„ç©ºéš™ï¼Œä¼˜åŒ–kernelå¯åŠ¨é—´éš”")
        elif analysis_type == "ncu":
            suggestions.append("ğŸ”¬ NCUåˆ†æå»ºè®®ï¼šæ£€æŸ¥SMæ•ˆç‡å’Œå†…å­˜å¸¦å®½åˆ©ç”¨ç‡")
        
        # é€šç”¨å»ºè®®
        suggestions.extend([
            "ğŸ’¾ å†…å­˜ä¼˜åŒ–ï¼šå¯ç”¨gradient checkpointingå‡å°‘å†…å­˜ä½¿ç”¨",
            "âš¡ è®¡ç®—ä¼˜åŒ–ï¼šä½¿ç”¨FlashAttentionåŠ é€Ÿæ³¨æ„åŠ›è®¡ç®—",
            "ğŸ”„ Pipelineä¼˜åŒ–ï¼šè€ƒè™‘ä½¿ç”¨æµæ°´çº¿å¹¶è¡Œæé«˜ååé‡"
        ])
        
        return suggestions

class LangChainAgent:
    """åŸºäºLangChainçš„AI Agentï¼Œæ”¯æŒå¯é€‰FAISSå‘é‡æ£€ç´¢å¢å¼º (RAG)ã€‚

    å‚æ•°:
        use_openai: æ˜¯å¦ä½¿ç”¨ OpenAI Chat æ¨¡å‹
        api_key: OpenAI API Key
        enable_faiss: æ˜¯å¦å¯ç”¨ FAISS æ£€ç´¢å¢å¼º
        faiss_embedding_model: ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹åç§°
        faiss_index_dir: å·²ä¿å­˜çš„å‘é‡ç´¢å¼•ç›®å½• (ä¸ºç©ºåˆ™éœ€è¿è¡Œæ„å»º)
    """
    
    def __init__(self, use_openai: bool = False, api_key: Optional[str] = None,
                 enable_faiss: bool = False,
                 faiss_embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
                 faiss_index_dir: Optional[str] = None):
        self.use_openai = use_openai and HAS_OPENAI
        self.api_key = api_key
        self.enable_faiss = enable_faiss
        self.faiss_embedding_model = faiss_embedding_model
        self.faiss_index_dir = Path(faiss_index_dir) if faiss_index_dir else None
        self._faiss_store = None
        self._faiss_available = False
        
        # åˆå§‹åŒ–LLM
        if self.use_openai and api_key:
            self.llm = ChatOpenAI(
                temperature=0.1,
                openai_api_key=api_key,
                model_name="gpt-3.5-turbo"
            )
        else:
            self.llm = MockLLM()
        
        # åˆå§‹åŒ–å·¥å…·
        self.tools = [
            PromptAnalysisTool(),
            ConfigAnalysisTool(),
            PerformanceAnalysisTool(),
            OptimizationAdvisorTool()
        ]
        
        # åˆå§‹åŒ–è®°å¿†
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
        
        # åˆ›å»ºAgent
        if self.use_openai:
            self.agent = initialize_agent(
                tools=self.tools,
                llm=self.llm,
                agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,
                memory=self.memory,
                verbose=True,
                handle_parsing_errors=True
            )
        else:
            # ä½¿ç”¨ç®€åŒ–çš„å·¥å…·è°ƒç”¨é€»è¾‘
            self.agent = None
        
        # åˆ›å»ºä¸“é—¨çš„æç¤ºæ¨¡æ¿
    self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„LLMæ€§èƒ½åˆ†æåŠ©æ‰‹ã€‚ä½ å¯ä»¥ï¼š

1. åˆ†æç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è¯·æ±‚ï¼Œæå–åˆ†æéœ€æ±‚
2. è§£æé…ç½®æ–‡ä»¶ï¼Œæä¾›ä¼˜åŒ–å»ºè®®
3. æ‰§è¡Œæ€§èƒ½åˆ†æä»»åŠ¡
4. åŸºäºç»“æœæä¾›ä¸“ä¸šçš„ä¼˜åŒ–å»ºè®®

è¯·æ ¹æ®ç”¨æˆ·è¾“å…¥é€‰æ‹©åˆé€‚çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚å§‹ç»ˆä¿æŒä¸“ä¸šã€å‹å¥½çš„è¯­è°ƒã€‚

å¯ç”¨å·¥å…·ï¼š
- prompt_analyzer: åˆ†æç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è¯·æ±‚
- config_analyzer: åˆ†æé…ç½®æ–‡ä»¶
- performance_analyzer: æ‰§è¡Œæ€§èƒ½åˆ†æ
- optimization_advisor: æä¾›ä¼˜åŒ–å»ºè®®
"""
    
    async def process_message(self, message: str, context: Dict = None) -> Dict:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
        try:
            # ç”Ÿæˆå¢å¼ºæŠ¥å‘Šè§¦å‘: "å®Œæ•´æŠ¥å‘Š" / "å¢å¼ºæŠ¥å‘Š" / "enriched report"
            lower_msg = message.lower()
            if any(k in lower_msg for k in ["å®Œæ•´æŠ¥å‘Š", "å¢å¼ºæŠ¥å‘Š", "enriched report"]):
                try:
                    from backend.report_generator import generate_enriched_report
                    # é»˜è®¤ä½¿ç”¨æœ€è¿‘ä¸€æ¬¡åˆ†æç›®å½•: é€šè¿‡é…ç½®æˆ–ä¸Šä¸‹æ–‡ä¼ å…¥ output_dir
                    # å¦‚æœ context æŒ‡å®š output_dir, ä½¿ç”¨å®ƒ; å¦åˆ™å°è¯• config ä¸­çš„ last_analysis_dir
                    target_dir = None
                    if context and isinstance(context, dict):
                        target_dir = context.get('output_dir')
                    if not target_dir and hasattr(self, 'last_analysis_dir'):
                        target_dir = getattr(self, 'last_analysis_dir')
                    if not target_dir:
                        # å›é€€é»˜è®¤ä¸»ç›®å½•
                        target_dir = "/workspace/Agent/AI_Agent_Complete"
                    from pathlib import Path
                    enriched_path = generate_enriched_report(Path(target_dir))
                    response_text = f"ğŸ“˜ å·²ç”Ÿæˆå¢å¼ºç‰ˆæ€§èƒ½æŠ¥å‘Š\nè·¯å¾„: {enriched_path}\næ‚¨å¯ä»¥æŸ¥çœ‹è¯¥ Markdown æ–‡ä»¶ä»¥è·å–è¯¦ç»†çš„ç†è®ºæ”¯æ’‘ä¸ä¼˜åŒ–å»ºè®®ã€‚"
                    return {
                        "status": "success",
                        "response": response_text,
                        "timestamp": datetime.now().isoformat()
                    }
                except Exception as rep_e:
                    return {
                        "status": "error",
                        "response": f"ç”Ÿæˆå¢å¼ºæŠ¥å‘Šå¤±è´¥: {rep_e}",
                        "timestamp": datetime.now().isoformat()
                    }
            # å¦‚æœå¯ç”¨äº† FAISSï¼Œå°è¯•åŠ è½½æˆ–æ„å»ºç´¢å¼•
                try:
                    # æ¨¡å‹ä¿¡æ¯æå–ä¸ç†è®ºé¢„å– (åªåšä¸€æ¬¡å¿«é€Ÿè§£æï¼Œä¸é˜»å¡ä¸»æµç¨‹)
                    from backend.model_intel import extract_model_info, prefetch_theory_snippets
                    model_info = extract_model_info(message)
                    theory_prefetch = None
                    if model_info.get('detected') and self.enable_faiss:
                        theory_prefetch = prefetch_theory_snippets(model_info)
                except Exception:
                    model_info = {"detected": False}
                    theory_prefetch = None

                try:
                    if self.enable_faiss:
                        self._init_faiss_store()

            if self.agent:
                # ä½¿ç”¨LangChain Agent
                response = await self._process_with_langchain(message, context)
            else:
                # ä½¿ç”¨ç®€åŒ–é€»è¾‘
                response = await self._process_with_simple_logic(message, context)
            
            return {
                "status": "success",
                "response": response,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                "status": "error",
                "response": f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }
    
    async def _process_with_langchain(self, message: str, context: Dict = None) -> str:
        """ä½¿ç”¨LangChainå¤„ç†æ¶ˆæ¯"""
        try:
            # æ·»åŠ ä¸Šä¸‹æ–‡åˆ°æ¶ˆæ¯
            if context:
                message = f"ä¸Šä¸‹æ–‡: {json.dumps(context, ensure_ascii=False)}\n\nç”¨æˆ·æ¶ˆæ¯: {message}"
            
            # è°ƒç”¨Agent
            result = self.agent.run(message)
            return result
            
        except Exception as e:
            return f"LangChainå¤„ç†å‡ºé”™: {str(e)}"
    
    async def _process_with_simple_logic(self, message: str, context: Dict = None) -> str:
        """ä½¿ç”¨ç®€åŒ–é€»è¾‘å¤„ç†æ¶ˆæ¯"""
        message_lower = message.lower()
        
        # åˆ¤æ–­æ„å›¾å¹¶è°ƒç”¨ç›¸åº”å·¥å…·
        # è‡ªåŠ¨æ€§èƒ½åˆ†ææ„å›¾: åŒ…å«"åˆ†æ"å¹¶æ£€æµ‹åˆ°æ¨¡å‹
        if ("åˆ†æ" in message or "analyze" in message_lower) and 'model_path' in message_lower or model_info.get('detected'):
            # å°è¯•æå– batch/input/output å‚æ•°
            batch_match = re.search(r"batch[_ ]?size[=ï¼š: ]?(\d+)", message_lower)
            input_match = re.search(r"input[_ ]?len[gth]*[=ï¼š: ]?(\d+)", message_lower)
            output_match = re.search(r"output[_ ]?len[gth]*[=ï¼š: ]?(\d+)", message_lower)
            batch_size = int(batch_match.group(1)) if batch_match else 8
            input_len = int(input_match.group(1)) if input_match else 2048
            output_len = int(output_match.group(1)) if output_match else 1024
            # è¿‘ä¼¼æå–æ¨¡å‹è·¯å¾„ (å« /workspace/models æˆ– å« qwen/llama å­—æ®µ)
            model_path = None
            path_candidates = re.findall(r"/workspace/models[\w/\-_.]+", message)
            if path_candidates:
                model_path = path_candidates[0]
            if not model_path and model_info.get('raw_match'):
                # ç”¨æˆ·å¯èƒ½åªå†™äº† qwen3-32bï¼Œæ‹¼æ¥é»˜è®¤å‰ç¼€ (å‡è®¾è·¯å¾„è§„åˆ™)
                family = model_info.get('family')
                size = int(model_info.get('size_b') or 0)
                if family and size:
                    # ç®€åŒ–æ˜ å°„: /workspace/models/<Family>/<Family><Size>B
                    model_path = f"/workspace/models/{family.capitalize()}/{family}{size}B"
            # è‹¥ä»æ— è·¯å¾„åˆ™æç¤º
            if not model_path:
                return "è¯·æä¾›æ¨¡å‹è·¯å¾„ (ä¾‹å¦‚ /workspace/models/Qwen/Qwen3-32B) ä»¥å¯åŠ¨è‡ªåŠ¨åˆ†æã€‚"
            # è°ƒç”¨ä¸€ä½“åŒ–æ€§èƒ½åˆ†æå·¥å…·
            try:
                from backend.tools.perf_tools import RunIntegratedPerfAnalysisTool
                tool = RunIntegratedPerfAnalysisTool()
                payload = json.dumps({
                    "model_path": model_path,
                    "batch_size": batch_size,
                    "input_len": input_len,
                    "output_len": output_len,
                    "max_kernels": 3
                }, ensure_ascii=False)
                result_json = tool._run(payload)
                return result_json
            except Exception as e:
                return f"è‡ªåŠ¨åˆ†æè§¦å‘å¤±è´¥: {e}"

        if "åˆ†æ" in message or "analyze" in message_lower:
            # ä½¿ç”¨æç¤ºè¯åˆ†æå·¥å…·
            tool = self.tools[0]  # PromptAnalysisTool
            result = tool._run(message)
            result_data = json.loads(result)
            
            if result_data["status"] == "success":
                return f"""âœ… æˆ‘å·²åˆ†ææ‚¨çš„è¯·æ±‚ï¼š

ğŸ¤– **æ¨¡å‹**: {result_data['model_name']}
ğŸ”¬ **åˆ†æç±»å‹**: {result_data['analysis_type']}
ğŸ“Š **æ‰¹æ¬¡å¤§å°**: {result_data['batch_size']}
ğŸ“ **è¾“å…¥é•¿åº¦**: {result_data['input_len']}

æ¥ä¸‹æ¥æˆ‘å°†ä¸ºæ‚¨é…ç½®åˆ†æå‚æ•°å¹¶å¼€å§‹æ‰§è¡Œã€‚æ‚¨éœ€è¦æˆ‘ç«‹å³å¼€å§‹åˆ†æå—ï¼Ÿ"""
            else:
                return f"âŒ åˆ†æè¯·æ±‚è§£æå¤±è´¥: {result_data['message']}"
        
        elif "é…ç½®" in message or "config" in message_lower:
            if context and "config_content" in context:
                # ä½¿ç”¨é…ç½®åˆ†æå·¥å…·
                tool = self.tools[1]  # ConfigAnalysisTool
                result = tool._run(context["config_content"])
                result_data = json.loads(result)
                
                if result_data["status"] == "success":
                    suggestions = result_data["suggestions"]
                    return f"""ğŸ“„ **é…ç½®æ–‡ä»¶åˆ†æå®Œæˆ**

ğŸ¤– **æ¨¡å‹ä¿¡æ¯**: å·²è¯†åˆ«æ¨¡å‹å‚æ•°
âš™ï¸ **åˆ†æå‚æ•°**: å·²æå–é…ç½®
ğŸ’¡ **æ™ºèƒ½å»ºè®®**:
{chr(10).join(f'â€¢ {s}' for s in suggestions[:5])}

åŸºäºæ‚¨çš„é…ç½®ï¼Œæˆ‘æ¨èè¿›è¡Œé›†æˆæ€§èƒ½åˆ†æã€‚è¦å¼€å§‹å—ï¼Ÿ"""
                else:
                    return f"âŒ é…ç½®æ–‡ä»¶åˆ†æå¤±è´¥: {result_data['message']}"
        
        elif "å»ºè®®" in message or "ä¼˜åŒ–" in message or "recommend" in message_lower:
            # ä½¿ç”¨ä¼˜åŒ–å»ºè®®å·¥å…·
            tool = self.tools[3]  # OptimizationAdvisorTool
            mock_results = json.dumps({"model_name": "general", "analysis_type": "auto"})
            result = tool._run(mock_results)
            result_data = json.loads(result)
            
            if result_data["status"] == "success":
                suggestions = result_data["optimization_suggestions"]
                return f"""ğŸ’¡ **æ€§èƒ½ä¼˜åŒ–å»ºè®®**

{chr(10).join(suggestions)}

ğŸ¯ **ä¼˜å…ˆè¡ŒåŠ¨**:
{chr(10).join(f'â€¢ {a}' for a in result_data['priority_actions'])}

éœ€è¦æˆ‘è¯¦ç»†è§£é‡Šä»»ä½•ä¸€é¡¹å»ºè®®å—ï¼Ÿ"""
        
        # å¦‚æœå¯ç”¨ FAISS ä¸”å­˜åœ¨â€œæ£€ç´¢â€æˆ–â€œæŸ¥è¯¢â€æ„å›¾
        if self.enable_faiss and any(k in message_lower for k in ["æ£€ç´¢", "æŸ¥è¯¢", "search", "retrieve", "å‘é‡", "çŸ¥è¯†åº“"]):
            if not self._faiss_available:
                return "âš ï¸ å‘é‡çŸ¥è¯†åº“å°šæœªåˆå§‹åŒ–ï¼Œå…ˆæä¾›åŸå§‹æ–‡æœ¬æˆ–æŒ‡å®šç´¢å¼•ç›®å½•ã€‚"
            results = self.query_faiss(message)
            if not results:
                return "ğŸ” æœªåœ¨å‘é‡çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"
            formatted = "\n".join([
                f"â€¢ (score={r['score']:.4f}) {r['content'][:120]}" for r in results
            ])
            theory_extra = ""
            if model_info.get('detected') and theory_prefetch and theory_prefetch.get('snippets'):
                merged = []
                for q, snips in theory_prefetch['snippets'].items():
                    if snips:
                        merged.append(f"Q: {q}\n  - " + "\n  - ".join(s[:100] for s in snips[:2]))
                if merged:
                    theory_extra = "\n\nğŸ“ é¢„å–æ¨¡å‹ç›¸å…³ç†è®º:\n" + "\n".join(merged[:3])
            return f"ğŸ“š æ£€ç´¢åˆ°ä»¥ä¸‹ç›¸å…³ç‰‡æ®µ:\n{formatted}{theory_extra}\n\nè¯·ç»§ç»­æé—®æˆ–è¦æ±‚ç”Ÿæˆæ€»ç»“ã€‚"

        # é»˜è®¤å“åº”
        return f"""ğŸ‘‹ æ‚¨å¥½ï¼æˆ‘æ˜¯LLMæ€§èƒ½åˆ†æåŠ©æ‰‹ã€‚

æˆ‘å¯ä»¥å¸®æ‚¨ï¼š
ğŸ” **åˆ†æè¯·æ±‚** - è§£ææ‚¨çš„æ€§èƒ½åˆ†æéœ€æ±‚
ğŸ“ **é…ç½®è§£æ** - åˆ†æä¸Šä¼ çš„é…ç½®æ–‡ä»¶
âš¡ **æ€§èƒ½åˆ†æ** - æ‰§è¡Œnsys/ncuåˆ†æ
ğŸ’¡ **ä¼˜åŒ–å»ºè®®** - æä¾›ä¸“ä¸šçš„æ€§èƒ½ä¼˜åŒ–å»ºè®®

è¯·å‘Šè¯‰æˆ‘æ‚¨éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Œæˆ–è€…ç›´æ¥è¯´å‡ºåˆ†æéœ€æ±‚ï¼Œæ¯”å¦‚ï¼š
"åˆ†ællama-7bæ¨¡å‹çš„æ€§èƒ½"
"å¯¹qwen-14bè¿›è¡Œncuæ·±åº¦åˆ†æ"
"""
    
    def add_uploaded_file(self, file_content: str, filename: str) -> Dict:
        """æ·»åŠ ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸Šä¸‹æ–‡"""
        try:
            # è§£ææ–‡ä»¶å†…å®¹
            tool = self.tools[1]  # ConfigAnalysisTool
            result = tool._run(file_content)
            result_data = json.loads(result)
            
            if result_data["status"] == "success":
                return {
                    "status": "success",
                    "message": f"å·²æˆåŠŸè§£ææ–‡ä»¶ {filename}",
                    "suggestions": result_data["suggestions"]
                }
            else:
                return {
                    "status": "error",
                    "message": f"æ–‡ä»¶è§£æå¤±è´¥: {result_data['message']}"
                }
                
        except Exception as e:
            return {
                "status": "error",
                "message": f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"
            }
    
    def get_memory_summary(self) -> str:
        """è·å–å¯¹è¯è®°å¿†æ‘˜è¦"""
        if hasattr(self.memory, 'buffer'):
            messages = self.memory.buffer[-5:]  # æœ€è¿‘5æ¡æ¶ˆæ¯
            return "\n".join([f"{msg.type}: {msg.content}" for msg in messages])
        return "æ— å¯¹è¯å†å²"

    # ================= FAISS é›†æˆç›¸å…³ =================
    def _init_faiss_store(self) -> None:
        """åˆå§‹åŒ–æˆ–åŠ è½½ FAISS å‘é‡åº“ã€‚"""
        try:
            from backend.knowledge_bases.faiss_store import load_index, build_index  # åŠ¨æ€å¯¼å…¥é¿å…æ— ä¾èµ–æ—¶å¤±è´¥
        except ImportError:
            print("âš ï¸ æœªæ‰¾åˆ° faiss_store æ¨¡å—æˆ–ç›¸å…³ä¾èµ–ï¼Œå‘é‡æ£€ç´¢åŠŸèƒ½ç¦ç”¨ã€‚")
            self._faiss_available = False
            return
        try:
            if self.faiss_index_dir and self.faiss_index_dir.exists():
                self._faiss_store = load_index(self.faiss_index_dir, model_name=self.faiss_embedding_model)
                self._faiss_available = True
                print(f"âœ… å·²åŠ è½½ FAISS ç´¢å¼•: {self.faiss_index_dir}")
            else:
                # å»¶è¿Ÿæ„å»ºä¸€ä¸ªç©ºç´¢å¼•ï¼ˆéœ€è¦åç»­æ·»åŠ æ–‡æœ¬ï¼‰
                self._faiss_store = None
                self._faiss_available = False
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ FAISS ç´¢å¼•å¤±è´¥: {e}")
            self._faiss_available = False

    def add_texts_to_faiss(self, texts: List[str]) -> str:
        """å‘ FAISS çŸ¥è¯†åº“æ·»åŠ åŸå§‹æ–‡æœ¬å¹¶é‡å»ºç´¢å¼•ã€‚"""
        if not self.enable_faiss:
            return "âŒ æœªå¯ç”¨ FAISS æ£€ç´¢åŠŸèƒ½"
        try:
            from backend.knowledge_bases.faiss_store import build_index, save_index
            self._faiss_store = build_index(texts, model_name=self.faiss_embedding_model)
            # è‡ªåŠ¨ä¿å­˜åˆ°é»˜è®¤ç›®å½• (è‹¥æœªæŒ‡å®šåˆ™åˆ›å»ºä¸´æ—¶ç›®å½•)
            target_dir = self.faiss_index_dir or Path("/workspace/Agent/AI_Agent_Complete/faiss_index")
            save_index(self._faiss_store, target_dir)
            self.faiss_index_dir = target_dir
            self._faiss_available = True
            return f"âœ… å·²æ„å»ºå¹¶ä¿å­˜å‘é‡ç´¢å¼•åˆ°: {target_dir}"
        except Exception as e:
            return f"âŒ æ„å»º FAISS ç´¢å¼•å¤±è´¥: {e}"

    def query_faiss(self, question: str, top_k: int = 4) -> List[Dict[str, Any]]:
        """åœ¨ FAISS å‘é‡åº“ä¸­æ‰§è¡Œæ£€ç´¢ã€‚"""
        if not self._faiss_available or self._faiss_store is None:
            return []
        try:
            from backend.knowledge_bases.faiss_store import query
            return query(self._faiss_store, question, top_k=top_k)
        except Exception:
            return []

    def reload_faiss_index(self) -> str:
        """é‡æ–°åŠ è½½ç£ç›˜ä¸Šçš„ FAISS ç´¢å¼• (ç”¨äºå‰ç«¯ä¸Šä¼ æ–°çŸ¥è¯†ååˆ·æ–°)ã€‚"""
        if not self.enable_faiss:
            return "âŒ æœªå¯ç”¨ FAISS æ£€ç´¢åŠŸèƒ½"
        if not self.faiss_index_dir or not self.faiss_index_dir.exists():
            return "âŒ ç´¢å¼•ç›®å½•ä¸å­˜åœ¨ï¼Œæ— æ³•åŠ è½½"
        try:
            from backend.knowledge_bases.faiss_store import load_index
            self._faiss_store = load_index(self.faiss_index_dir, model_name=self.faiss_embedding_model)
            self._faiss_available = True
            return f"âœ… å·²åˆ·æ–°ç´¢å¼•: {self.faiss_index_dir}"
        except Exception as e:
            return f"âŒ åˆ·æ–°ç´¢å¼•å¤±è´¥: {e}"

# ä½¿ç”¨ç¤ºä¾‹å‡½æ•°
async def test_langchain_agent():
    """æµ‹è¯•LangChain Agent"""
    print("ğŸ§ª æµ‹è¯•LangChain Agent...")
    
    # åˆ›å»ºAgent
    agent = LangChainAgent(use_openai=False)
    
    # æµ‹è¯•æ¶ˆæ¯
    test_messages = [
        "åˆ†æ llama-7b æ¨¡å‹ï¼Œbatch_size=8,16",
        "æˆ‘éœ€è¦æ€§èƒ½ä¼˜åŒ–å»ºè®®",
        "å¦‚ä½•æé«˜GPUåˆ©ç”¨ç‡ï¼Ÿ"
    ]
    
    for message in test_messages:
        print(f"\nğŸ‘¤ ç”¨æˆ·: {message}")
        result = await agent.process_message(message)
        print(f"ğŸ¤– AI Agent: {result['response']}")

if __name__ == "__main__":
    asyncio.run(test_langchain_agent())


