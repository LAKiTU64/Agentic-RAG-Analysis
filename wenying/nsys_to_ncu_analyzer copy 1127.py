#!/usr/bin/env python3
"""
NVIDIA æ€§èƒ½åˆ†æé›†æˆå·¥å…·
å…ˆç”¨ nsys è¯†åˆ«çƒ­ç‚¹kernelsï¼Œå†ç”¨ ncu æ·±åº¦åˆ†æ

å·¥ä½œæµç¨‹ï¼š
1. nsys profile -> è·å–å…¨å±€æ€§èƒ½overview  
2. æå–çƒ­ç‚¹kernelåç§°
3. ncu profile -> é’ˆå¯¹çƒ­ç‚¹kernelsæ·±åº¦åˆ†æ
4. ç»¼åˆåˆ†ææŠ¥å‘Š

ä½œè€…: xjw
ç‰ˆæœ¬: 1.0
"""

import os
import sys
import json
import subprocess
import argparse
import re
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime

# å¯¼å…¥æˆ‘ä»¬çš„åˆ†æå·¥å…·
sys.path.append(str(Path(__file__).parent))
from nsys_parser import NsysParser, NsysAnalyzer
from ncu_parser import NCUParser, NCUAnalyzer, NCUVisualizer, NCUReporter

# å¼•å…¥é«˜é˜¶æŠ¥å‘Šä¸çŸ¥è¯†åº“æ‘„å–æ¨¡å—ï¼ˆå¯é€‰ï¼‰
try:
    from backend.advanced_report import generate_advanced_report
except Exception:
    generate_advanced_report = None  # type: ignore
try:
    from backend.knowledge_bases.kb_ingest import ingest_json_to_faiss, flatten_json
except Exception:
    ingest_json_to_faiss = None  # type: ignore
    flatten_json = None  # type: ignore

class NSysToNCUAnalyzer:
    """é›†æˆ nsys å’Œ ncu çš„åˆ†æå·¥å…·

    ç»Ÿä¸€è¾“å‡ºç›®å½•:
        é»˜è®¤ä½¿ç”¨ /workspace/Agent/AI_Agent_Complete ä½œä¸ºæ ¹è·¯å¾„ä¸‹çš„ integrated_analysis å­ç›®å½•ï¼Œ
        ä¾¿äº Agent è¯»å–æ‰€æœ‰ç”Ÿæˆçš„æŠ¥å‘Šå’Œä¸­é—´äº§ç‰©ã€‚
    """
    DEFAULT_BASE_DIR = Path("/workspace/Agent/AI_Agent_Complete")

    def __init__(self, output_dir: str = "integrated_analysis"):
        # å¦‚æœç”¨æˆ·ä¼ å…¥çš„æ˜¯ç»å¯¹è·¯å¾„åˆ™ä½¿ç”¨åŸå€¼ï¼Œå¦åˆ™æ‹¼æ¥åˆ°é»˜è®¤åŸºè·¯å¾„ä¸‹
        base = self.DEFAULT_BASE_DIR
        if output_dir.startswith('/'):
            self.output_dir = Path(output_dir)
        else:
            self.output_dir = base / output_dir
        self.output_dir.mkdir(exist_ok=True)
        self.hot_kernels = []
        self.nsys_stats = {}
        self.ncu_results = {}
        
    def step1_nsys_analysis(self, target_command: List[str], profile_name: str = "overview") -> str:
        """ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨nsysè¿›è¡Œå…¨å±€æ€§èƒ½åˆ†æ"""
        
        nsys_profile = self.output_dir / f"{profile_name}.nsys-rep"
        
        # æ„å»ºnsyså‘½ä»¤
        nsys_cmd = [
            'nsys', 'profile',
            '-o', str(nsys_profile.with_suffix('')),  # nsysä¼šè‡ªåŠ¨æ·»åŠ .nsys-rep
            '-t', 'cuda,nvtx,osrt',
            '--cuda-memory-usage=true',
            '--force-overwrite=true'
        ] + target_command
        
        print("ğŸš€ æ­¥éª¤1: è¿è¡Œnsyså…¨å±€æ€§èƒ½åˆ†æ...")
        print(f"å‘½ä»¤: {' '.join(nsys_cmd)}")
        
        try:
            result = subprocess.run(nsys_cmd, capture_output=True, text=True, check=True)
            print(f"âœ… nsysåˆ†æå®Œæˆ: {nsys_profile}")
            return str(nsys_profile)
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ nsysåˆ†æå¤±è´¥: {e.stderr}")
            raise
    
    def step2_extract_hot_kernels(self, nsys_file: str, 
                                  top_k: int = 10, 
                                  min_duration_ms: float = 0.1) -> List[Dict]:
        """ç¬¬äºŒæ­¥ï¼šä»nsysç»“æœä¸­æå–çƒ­ç‚¹kernels"""
        
        print("ğŸ” æ­¥éª¤2: ä»nsysç»“æœæå–çƒ­ç‚¹kernels...")
        
        # ä½¿ç”¨æˆ‘ä»¬çš„nsysè§£æå™¨
        parser = NsysParser(nsys_file)
        parser.parse()
        
        if not parser.kernels:
            print("âš ï¸  æœªå‘ç°CUDA kernels")
            return []
        
        # åˆ†ækernels
        analyzer = NsysAnalyzer(parser)
        self.nsys_stats = analyzer.analyze()
        
        # æå–çƒ­ç‚¹kernels
        hot_kernels = []
        
        # æ–¹æ³•1: æŒ‰æ‰§è¡Œæ—¶é—´æ’åº
        kernels_by_time = sorted(parser.kernels, 
                               key=lambda k: k.duration, reverse=True)
        
        # æ–¹æ³•2: æŒ‰è°ƒç”¨æ¬¡æ•°ç»Ÿè®¡
        kernel_stats = {}
        for kernel in parser.kernels:
            name = kernel.name
            if name not in kernel_stats:
                kernel_stats[name] = {
                    'count': 0,
                    'total_time': 0,
                    'max_time': 0,
                    'avg_time': 0
                }
            
            kernel_stats[name]['count'] += 1
            kernel_stats[name]['total_time'] += kernel.duration * 1000  # è½¬ä¸ºms
            kernel_stats[name]['max_time'] = max(kernel_stats[name]['max_time'], 
                                                kernel.duration * 1000)
        
        # è®¡ç®—å¹³å‡æ—¶é—´
        for name, stats in kernel_stats.items():
            stats['avg_time'] = stats['total_time'] / stats['count']
        
        # æŒ‰æ€»æ‰§è¡Œæ—¶é—´æ’åºï¼Œæå–top-k
        sorted_kernels = sorted(kernel_stats.items(), 
                              key=lambda x: x[1]['total_time'], reverse=True)
        
        for kernel_name, stats in sorted_kernels[:top_k]:
            # è¿‡æ»¤æ‰æ‰§è¡Œæ—¶é—´å¤ªçŸ­çš„kernels
            if stats['avg_time'] >= min_duration_ms:
                hot_kernels.append({
                    'name': kernel_name,
                    'total_time_ms': stats['total_time'],
                    'avg_time_ms': stats['avg_time'],
                    'count': stats['count'],
                    'max_time_ms': stats['max_time']
                })
        
        self.hot_kernels = hot_kernels
        
        print(f"ğŸ”¥ è¯†åˆ«åˆ° {len(hot_kernels)} ä¸ªçƒ­ç‚¹kernels:")
        for i, kernel in enumerate(hot_kernels[:5], 1):
            # kernel å¯èƒ½ä¸º dictï¼Œä¹Ÿå¯èƒ½ä¸ºå…¶å®ƒç±»å‹ï¼Œåšé˜²æŠ¤å¤„ç†
            try:
                name_str = kernel['name'] if isinstance(kernel, dict) else kernel
            except Exception:
                name_str = kernel

            # å¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œé˜²æ­¢éå­—ç¬¦ä¸²ç±»å‹å¯¼è‡´åˆ‡ç‰‡é”™è¯¯
            name_str = str(name_str)
            name_short = name_str[:60]
            total = kernel.get('total_time_ms', 0) if isinstance(kernel, dict) else 0
            avg = kernel.get('avg_time_ms', 0) if isinstance(kernel, dict) else 0
            count = kernel.get('count', 0) if isinstance(kernel, dict) else 0

            print(f"  {i}. {name_short}... (æ€»è®¡: {total:.2f}ms, å¹³å‡: {avg:.3f}ms, è°ƒç”¨: {count}æ¬¡)")
        
        # ä¿å­˜çƒ­ç‚¹kernelsåˆ°æ–‡ä»¶
        hot_kernels_file = self.output_dir / "hot_kernels.json"
        with open(hot_kernels_file, 'w', encoding='utf-8') as f:
            json.dump(hot_kernels, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ çƒ­ç‚¹kernelsåˆ—è¡¨å·²ä¿å­˜: {hot_kernels_file}")
        # çº¯æ–‡æœ¬åˆ—è¡¨ï¼ˆä»…åç§°ï¼‰ï¼Œä¾¿äºå‰ç«¯ç›´æ¥è¯»å–
        try:
            txt_list_path = self.output_dir / "hot_kernels.txt"
            with open(txt_list_path, 'w', encoding='utf-8') as f_txt:
                for hk in hot_kernels:
                    name_val = hk.get('name') if isinstance(hk, dict) else str(hk)
                    f_txt.write(str(name_val).strip() + '\n')
            print(f"ğŸ“ å·²ç”Ÿæˆçº¯æ–‡æœ¬çƒ­ç‚¹ kernel åç§°åˆ—è¡¨: {txt_list_path}")
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆçº¯æ–‡æœ¬çƒ­ç‚¹åˆ—è¡¨å¤±è´¥: {e}")
        # JSON å¯¼å‡º + åç§°å¢å¼º
        try:
            json_path = parser.export_to_json(self.output_dir / f"{Path(nsys_file).stem}.json")
            json_names = parser.extract_kernel_names_from_json(json_path, limit=800)
            gpu_candidates = parser.filter_gpu_kernel_candidates(json_names)
            self._augment_hot_kernels_with_json_names(gpu_candidates)
        except Exception as e:
            print(f"âš ï¸ JSON kernel åå¢å¼ºå¤±è´¥: {e}")
        # CSV kernel summary å¯¼å‡ºï¼ˆæ›´å¯é åç§°æ¥æºï¼‰
        try:
            csv_summary = parser.export_kernel_summary_csv(nsys_file, self.output_dir / f"{Path(nsys_file).stem}_kernels")
            if csv_summary:
                summary_rows = parser.parse_kernel_summary_csv(csv_summary)
                self._replace_with_csv_kernel_names(summary_rows)
        except Exception as e:
            print(f"âš ï¸ CSV kernel æ±‡æ€»è·å–å¤±è´¥: {e}")
        return hot_kernels

    def _augment_hot_kernels_with_json_names(self, gpu_candidates: List[str]):
        """å¦‚æœçƒ­ç‚¹ kernel åéƒ½æ˜¯æ•°å­—æˆ– __unnamed_ï¼Œå°è¯•ç”¨çœŸå®å€™é€‰åæ›¿æ¢å‰ N ä¸ªä»¥ä¾¿ ncu åŒ¹é…ã€‚
        ä¿ç•™åŸå­—æ®µ 'original_name'.
        """
        if not self.hot_kernels or not gpu_candidates:
            return
        # åˆ¤æ–­éœ€è¦å¢å¼ºçš„æ¯”ä¾‹
        def is_placeholder(name: str):
            return name.isdigit() or name.startswith('__unnamed_')
        placeholders = [hk for hk in self.hot_kernels if is_placeholder(str(hk.get('name','')))]
        if not placeholders:
            return
        replace_count = min(len(placeholders), len(gpu_candidates))
        for i in range(replace_count):
            hk = placeholders[i]
            candidate = gpu_candidates[i]
            hk['original_name'] = hk['name']
            hk['name'] = candidate
        print(f"ğŸ” å·²ç”¨ {replace_count} ä¸ª JSON å€™é€‰åç§°æ›¿æ¢å ä½/æ•°å­— hotspot kernel åï¼Œç¤ºä¾‹: {[(hk.get('original_name'), hk['name']) for hk in placeholders[:3]]}")

    def _replace_with_csv_kernel_names(self, summary_rows: List[Dict]):
        """æ ¹æ® kernel summary CSV ä¸­çš„çœŸå®åç§°ï¼Œå¯¹çƒ­ç‚¹åˆ—è¡¨è¿›è¡Œç²¾å‡†æ›¿æ¢ã€‚
        ä¼˜å…ˆåŒ¹é…å ä½/æ•°å­—åï¼Œæˆ–æ—¶é—´æ˜¾è‘—çš„éçœŸå®åã€‚
        """
        if not summary_rows:
            return
        real_names = [r['name'] for r in summary_rows if r['name']]
        if not real_names:
            return
        # æ„å»ºä¸€ä¸ªè¿­ä»£å™¨
        idx = 0
        for hk in self.hot_kernels:
            name = str(hk.get('name',''))
            if self._is_placeholder_name(name):
                if idx < len(real_names):
                    hk['original_name'] = name
                    hk['name'] = real_names[idx]
                    hk['csv_substituted'] = True
                    idx += 1
        if idx > 0:
            print(f"ğŸ§¬ å·²ç”¨ CSV æ±‡æ€»ä¸­çš„çœŸå®åç§°æ›¿æ¢ {idx} ä¸ª hotspot kernel: {[hk['name'] for hk in self.hot_kernels[:idx]]}")
    
    def step3_ncu_targeted_analysis(self, target_command: List[str], 
                                   kernels_to_analyze: List[Dict],
                                   max_kernels: int = 5) -> List[str]:
        """ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨ncuå¯¹çƒ­ç‚¹kernelsè¿›è¡Œæ·±åº¦åˆ†æ"""
        
        print("âš¡ æ­¥éª¤3: ä½¿ç”¨ncuæ·±åº¦åˆ†æçƒ­ç‚¹kernels...")
        
        ncu_results = []

        # è‹¥çƒ­ç‚¹kernelåç§°å¯ç–‘ï¼ˆæ•°å­—/unnamedï¼‰ï¼Œå…ˆç”¨ ncu --list-kernels è‡ªåŠ¨å‘ç°çœŸå®åç§°
        if any(self._is_placeholder_name(str(k.get('name',''))) for k in kernels_to_analyze):
            print("ğŸ” æ£€æµ‹åˆ°å ä½/æ•°å­—kernelåï¼Œè§¦å‘ ncu --list-kernels è¿›è¡ŒçœŸå®åç§°å‘ç°...")
            discovered = self.list_kernels_with_ncu(target_command)
            selected = self._select_real_kernels(discovered, max_kernels)
            print(f"ğŸ§­ é€‰æ‹©ç”¨äºæ·±åº¦åˆ†æçš„çœŸå®kernelåç§°: {selected}")
            # æ›¿æ¢ kernels_to_analyze ä¸­çš„ name å­—æ®µ
            for i, real_name in enumerate(selected):
                if i < len(kernels_to_analyze):
                    kernels_to_analyze[i]['discovered'] = True
                    kernels_to_analyze[i]['original_name'] = kernels_to_analyze[i]['name']
                    kernels_to_analyze[i]['name'] = real_name
                else:
                    # å¦‚æœçƒ­ç‚¹åˆ—è¡¨ä¸å¤Ÿï¼Œè¡¥å……
                    kernels_to_analyze.append({'name': real_name, 'discovered': True, 'total_time_ms': 0, 'avg_time_ms': 0, 'count': 0})

        # é™åˆ¶åˆ†ææ•°é‡
        kernels_to_analyze = kernels_to_analyze[:max_kernels]
        
        for i, kernel_info in enumerate(kernels_to_analyze):
            kernel_name = str(kernel_info.get('name', 'kernel')).strip()

            # æ¸…ç†kernelåç§°ï¼Œç”¨äºæ–‡ä»¶å
            safe_name = re.sub(r'[^\w\-_]', '_', kernel_name)[:50]
            ncu_profile = self.output_dir / f"ncu_kernel_{i}_{safe_name}"
            
            print(f"ğŸ¯ æ­£åœ¨åˆ†ækernel {i+1}/{len(kernels_to_analyze)}: {kernel_name[:60]}...")

            def attempt_profile(attempt_cmd: List[str], attempt_tag: str) -> Optional[str]:
                """å°è£…ä¸€æ¬¡ ncu å°è¯•ï¼Œè¿”å› .ncu-rep è·¯å¾„æˆ– None"""
                try:
                    res = subprocess.run(attempt_cmd)
                    if res.returncode != 0:
                        # æ‰“å°stderrçš„ä¸€éƒ¨åˆ†ä¾¿äºè°ƒè¯•
                        snippet = (res.stderr or '')[:200].replace('\n', ' ')
                        print(f"âš ï¸ å°è¯• {attempt_tag} å¤±è´¥(returncode={res.returncode}): {snippet}")
                        return None
                    ncu_file = str(ncu_profile) + '.ncu-rep'
                    if Path(ncu_file).exists():
                        print(f"âœ… æˆåŠŸç”Ÿæˆ NCU æŠ¥å‘Š ({attempt_tag}): {ncu_file}")
                        return ncu_file
                    else:
                        print(f"âš ï¸ å°è¯• {attempt_tag} æœªç”Ÿæˆ .ncu-rep æ–‡ä»¶: {ncu_file}")
                except subprocess.TimeoutExpired:
                    print(f"â° å°è¯• {attempt_tag} è¶…æ—¶")
                except Exception as e:
                    print(f"âŒ å°è¯• {attempt_tag} å¼‚å¸¸: {e}")
                return None

            # æ„å»ºå¤šå±‚å›é€€ç­–ç•¥ï¼š
            # 1) åŸå§‹åç§° + demangled åŸºå‡†
            # 2) æ­£åˆ™å‰ç¼€åŒ¹é… (å‡å°‘è¿‡é•¿åç§°ç²¾ç¡®åŒ¹é…å¤±è´¥æ¦‚ç‡)
            # 3) å»æ‰è¿‡æ»¤ (é‡‡é›†æ‰€æœ‰å†…æ ¸ï¼Œé™åˆ¶ launch-count ä»¥é™å¼€é”€)
            attempts = []
            # 1 åŸå§‹ç²¾ç¡®åŒ¹é…ï¼ˆdemangledï¼‰
            attempts.append({
                'tag': 'exact-demangled',
                'cmd': ['ncu', '--kernel-name-base', 'demangled', '--kernel-name', kernel_name,
                        '--rename-kernels=0', '--set', 'full', '-o', str(ncu_profile), '--force-overwrite'] + target_command
            })
            # 2 æ­£åˆ™å‰ç¼€ï¼ˆå–å‰ 60 å¯è§å­—ç¬¦ï¼Œå»é™¤å¼•å·ï¼Œåªä¿ç•™å®‰å…¨å­—ç¬¦ï¼‰
            prefix_raw = re.sub(r'"', '', kernel_name)[:60]
            # é€‚åº¦è£å‰ªåˆ°ç¬¬ä¸€ä¸ªå³æ‹¬å·æˆ–æ¨¡æ¿ç»“æŸç¬¦ï¼Œé¿å…è¿‡é•¿
            m_end = re.search(r'[)>]$', prefix_raw)
            prefix_clean = prefix_raw
            # è½¬ä¹‰æ­£åˆ™ç‰¹æ®Šå­—ç¬¦
            prefix_regex = re.sub(r'([\\.^$|?*+\[\](){}])', r'\\\1', prefix_clean)
            attempts.append({
                'tag': 'regex-prefix',
                'cmd': ['ncu', '--kernel-name-base', 'demangled', '--kernel-name', f'regex:^{prefix_regex}',
                        '--rename-kernels=0', '--set', 'full', '-o', str(ncu_profile), '--force-overwrite'] + target_command
            })
            # 3 æ— è¿‡æ»¤ï¼ˆå¯èƒ½è¾ƒå¤šæ•°æ®ï¼Œä½¿ç”¨åŸºç¡€æŒ‡æ ‡é›† + launch-count é™åˆ¶ï¼‰
            attempts.append({
                'tag': 'unfiltered-basic',
                'cmd': ['ncu', '--launch-count', '50', '--set', 'compute', '-o', str(ncu_profile), '--force-overwrite'] + target_command
            })

            produced = None
            for att in attempts:
                print(f"ğŸ” NCU å°è¯•: {att['tag']}")
                produced = attempt_profile(att['cmd'], att['tag'])
                if produced:
                    break

            if produced:
                ncu_results.append(produced)
                self._export_ncu_to_csv(produced)
            else:
                print(f"âŒ æ‰€æœ‰å°è¯•å‡æœªç”Ÿæˆ NCU æŠ¥å‘Š: {kernel_name[:80]}")
        
        return ncu_results

    def step3_ncu_global_focus(self, target_command: List[str], hot_kernels: List[Dict], top_focus: int = 5,
                               set_name: str = 'compute', launch_limit: Optional[int] = None) -> Tuple[Optional[str], Dict[str, Dict]]:
        """æ›¿ä»£å®šå‘åˆ†æï¼šä¸€æ¬¡å…¨é‡ ncu é‡‡é›†ï¼Œç„¶åä»…é’ˆå¯¹ nsys å‘ç°çš„å‰ top_focus ä¸ªçƒ­ç‚¹ kernel æå–ä¸å½’å¹¶æŒ‡æ ‡ã€‚

        è¿”å›: (å…¨é‡ ncu æŠ¥å‘Šè·¯å¾„æˆ– None, focus_metrics dict)
        focus_metrics ç»“æ„ (é”®ä¸ºçƒ­ç‚¹ kernel åŸå):
            {
              kernel_display_name: {
                  'kernels_analyzed': int,
                  'gpu_utilization': {...},
                  'memory_analysis': {...},
                  'bottleneck_summary': [...]
              }
            }
        """
        if not hot_kernels:
            print("âš ï¸ æ— çƒ­ç‚¹ kernelï¼Œè·³è¿‡å…¨é‡ NCU é‡‡é›†")
            return None, {}
        # è¿è¡Œä¸€æ¬¡å…¨é‡é‡‡é›†
        full_rep = self.full_ncu_capture(target_command, profile_name='ncu_full_capture_global', set_name=set_name, launch_limit=launch_limit)
        if not full_rep:
            return None, {}
        csv_file = full_rep.replace('.ncu-rep', '.csv')
        if not Path(csv_file).exists() or Path(csv_file).stat().st_size == 0:
            print("âš ï¸ å…¨é‡é‡‡é›† CSV ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œæ— æ³•æå–ç„¦ç‚¹å†…æ ¸æŒ‡æ ‡")
            return full_rep, {}
        # è§£æ CSV
        try:
            parser = NCUParser(csv_file)
            parser.parse()
        except Exception as e:
            print(f"âš ï¸ å…¨é‡ CSV è§£æå¤±è´¥: {e}")
            return full_rep, {}
        # æ„å»ºç„¦ç‚¹åˆ†æ
        focus = {}
        # å»ºç«‹å¿«é€Ÿåˆ—è¡¨
        metrics_list = parser.kernels  # List[KernelMetrics]
        def _match_entries(target: str) -> List[Any]:
            t_low = target.lower()
            matched = [km for km in metrics_list if t_low in km.name.lower() or km.name.lower() in t_low]
            # è‹¥æ— ç›´æ¥åŒ…å«åŒ¹é…ï¼Œå°è¯•æŒ‰åˆ†è¯å…¬å…±å­ä¸² >=5 char
            if not matched:
                # ç®€å•åˆ‡å‰²éå­—æ¯æ•°å­—
                import re
                tokens = [tok for tok in re.split(r'[^A-Za-z0-9_]+', t_low) if len(tok) >= 5]
                if tokens:
                    for tok in tokens:
                        part = [km for km in metrics_list if tok in km.name.lower()]
                        matched.extend(part)
            # å»é‡
            uniq = []
            seen = set()
            for m in matched:
                if id(m) not in seen:
                    seen.add(id(m)); uniq.append(m)
            return uniq[:50]  # é˜²æ­¢è¿‡å¤š
        def _avg(vals: List[Optional[float]]) -> Optional[float]:
            nums = [v for v in vals if isinstance(v, (int, float))]
            return sum(nums)/len(nums) if nums else None
        focus_targets = hot_kernels[:top_focus]
        for hk in focus_targets:
            kname = str(hk.get('name',''))
            entries = _match_entries(kname)
            if not entries:
                continue
            sm_eff = _avg([e.sm_efficiency for e in entries])
            occ = _avg([e.achieved_occupancy for e in entries])
            dram = _avg([e.dram_bandwidth for e in entries])
            l2 = _avg([e.l2_hit_rate for e in entries])
            warp_eff = _avg([e.warp_execution_efficiency for e in entries])
            tensor_active = _avg([e.tensor_active for e in entries])
            # ç“¶é¢ˆåˆ¤å®š (å¯å‘å¼)
            bottlenecks = []
            def add_bottleneck(cond: bool, desc: str, severity: str):
                if cond:
                    bottlenecks.append({'type': 'heuristic', 'severity': severity, 'description': desc})
            add_bottleneck(sm_eff is not None and sm_eff < 40, 'SMæ•ˆç‡åä½', 'high')
            add_bottleneck(dram is not None and dram < 150, 'å†…å­˜å¸¦å®½å¯èƒ½å—é™', 'medium')
            add_bottleneck(occ is not None and occ < 25, 'Occupancyè¾ƒä½', 'medium')
            add_bottleneck(warp_eff is not None and warp_eff < 70, 'Warpæ‰§è¡Œæ•ˆç‡ä¸€èˆ¬', 'low')
            focus[kname] = {
                'kernels_analyzed': len(entries),
                'gpu_utilization': {
                    'average_sm_efficiency': sm_eff,
                    'achieved_occupancy': occ,
                    'tensor_core_active': tensor_active,
                },
                'memory_analysis': {
                    'bandwidth_stats': {
                        'average_bandwidth': dram,
                        'l2_hit_rate': l2,
                    }
                },
                'bottleneck_summary': bottlenecks
            }
        print(f"ğŸ” å…¨é‡é‡‡é›†ä¸­å·²ç”Ÿæˆ {len(focus)} ä¸ªç„¦ç‚¹å†…æ ¸çš„èšåˆæŒ‡æ ‡")
        return full_rep, focus

    def full_ncu_capture(self, target_command: List[str], profile_name: str = "ncu_full_capture",
                          set_name: str = "compute", launch_limit: Optional[int] = None,
                          timeout: int = 1200) -> Optional[str]:
        """æ‰§è¡Œä¸€æ¬¡ä¸åš kernel è¿‡æ»¤çš„å®Œæ•´ NCU é‡‡é›†ã€‚

        å‚æ•°:
            target_command: åŸå§‹å¾…åˆ†æå‘½ä»¤ (['python', 'script.py', ...])
            profile_name: è¾“å‡ºæŠ¥å‘ŠåŸºå
            set_name: ä½¿ç”¨çš„ NCU æŒ‡æ ‡é›†åˆ (--set)ã€‚å¯é€‰: 'compute', 'full' ç­‰
            launch_limit: ä½¿ç”¨ --launch-count é™åˆ¶é‡‡é›†çš„ kernel æ¬¡æ•° (é™ä½é•¿ä»»åŠ¡å¼€é”€)
            timeout: è¶…æ—¶æ—¶é—´ (ç§’)

        è¡Œä¸º:
            ç”Ÿæˆ <profile_name>.ncu-rep åŠå¯¹åº”çš„ CSV/JSON (è‹¥å¯èƒ½)
            è¾“å‡ºè·¯å¾„ä½äºç»Ÿä¸€çš„ self.output_dir ä¸‹ã€‚
        """
        ncu_profile_base = self.output_dir / profile_name
        ncu_rep = str(ncu_profile_base) + '.ncu-rep'
        cmd = ['ncu', '--set', set_name, '-o', str(ncu_profile_base), '--force-overwrite']
        if launch_limit:
            cmd += ['--launch-count', str(launch_limit)]
        # ä¸åŠ  --kernel-name è¿‡æ»¤, æ•è·å…¨éƒ¨å¯è§å†…æ ¸
        cmd += target_command
        print(f"ğŸŒ€ å…¨é‡ NCU é‡‡é›†: {' '.join(cmd)}")
        try:
            res = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)
            if res.returncode != 0:
                print(f"âš ï¸ å…¨é‡é‡‡é›†å¤±è´¥(returncode={res.returncode}): {(res.stderr or '')[:300].replace('\n',' ')}")
                return None
            if not Path(ncu_rep).exists():
                print(f"âš ï¸ æœªç”Ÿæˆ ncu æŠ¥å‘Šæ–‡ä»¶: {ncu_rep}")
                return None
            print(f"âœ… å…¨é‡ NCU é‡‡é›†å®Œæˆ: {ncu_rep}")
            # å°è¯•å¯¼å‡º CSV
            self._export_ncu_to_csv(ncu_rep)
            return ncu_rep
        except subprocess.TimeoutExpired:
            print("â³ å…¨é‡ NCU é‡‡é›†è¶…æ—¶")
        except Exception as e:
            print(f"âŒ å…¨é‡ NCU é‡‡é›†å¼‚å¸¸: {e}")
        return None

    def list_kernels_with_ncu(self, target_command: List[str]) -> List[str]:
        """è¿è¡Œ ncu --list-kernels ä»¥è·å–å®é™…å¯åˆ†æçš„ kernel åç§°åˆ—è¡¨"""
        cmd = ['ncu', '--list-kernels'] + target_command
        print(f"ğŸ§ª è¿è¡Œ: {' '.join(cmd)}")
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
            output = result.stdout + '\n' + result.stderr
        except Exception as e:
            print(f"âŒ list-kernels å¤±è´¥: {e}")
            return []

        # è§£æè¾“å‡ºï¼šæ¯è¡Œå¯èƒ½åŒ…å« kernel åç§°ã€‚æˆ‘ä»¬è¿‡æ»¤å‡ºå« 'Kernel', 'cuda', 'cutlass', 'flash', 'aten', 'cublas', 'gemm', 'matmul', 'triton'
        lines = [l.strip() for l in output.splitlines() if l.strip()]
        kernels = []
        import re
        pattern = re.compile(r'(Kernel|cuda|cutlass|flash|aten|cublas|gemm|matmul|triton)', re.IGNORECASE)
        for line in lines:
            # å¸¸è§æ ¼å¼: index + name æˆ– ç›´æ¥ name
            # æ’é™¤å¤ªçŸ­è¡Œ
            if len(line) < 4:
                continue
            if pattern.search(line):
                # å»æ‰å‰å¯¼ç¼–å·æˆ–è£…é¥°ç¬¦
                cleaned = re.sub(r'^\s*\d+\s*[:\-]?\s*', '', line)
                kernels.append(cleaned)
        # å»é‡ä¿æŒé¡ºåº
        seen = set(); uniq = []
        for k in kernels:
            if k not in seen:
                seen.add(k); uniq.append(k)
        print(f"ğŸ“‹ list-kernels è·å¾—å€™é€‰ {len(uniq)} ä¸ª (å‰10): {uniq[:10]}")
        return uniq

    def _select_real_kernels(self, discovered: List[str], max_kernels: int) -> List[str]:
        """æ ¹æ®ä¼˜å…ˆçº§ä»å‘ç°çš„ kernel åç§°åˆ—è¡¨ä¸­æŒ‘é€‰ç”¨äºåˆ†æçš„åç§°"""
        if not discovered:
            return []
        priority_patterns = [
            'FlashAttn', 'flash', 'cutlass', 'triton', 'gemm', 'matmul', 'cublas', 'aten', 'reduce', 'norm'
        ]
        scored = []
        for name in discovered:
            low = name.lower()
            score = 0
            for idx, pat in enumerate(priority_patterns):
                if pat.lower() in low:
                    score += (100 - idx)  # earlier pattern higher score
            # é•¿åº¦å’ŒåŒ…å« Kernel å­—æ ·åŠ ä¸€ç‚¹åˆ†
            if 'kernel' in low:
                score += 5
            if len(name) > 30:
                score += 1
            scored.append((score, name))
        # æ’åºï¼Œåˆ†æ•°é«˜çš„é å‰
        scored.sort(reverse=True)
        selected = [n for s, n in scored[:max_kernels]]
        return selected

    def _is_placeholder_name(self, name: str) -> bool:
        # å°†æ•°å­—ã€__unnamed_ ä»¥åŠè‹¥å¹²ä½ä¿¡å· / æ¡†æ¶æ€§åå­—è§†ä¸ºå ä½ã€‚å¢åŠ  __cudart_ å‰ç¼€ï¼Œä»¥ä¾¿åç»­ç”¨ CSV çœŸå® kernel åæ›¿æ¢ã€‚
        return (
            name.isdigit()
            or name.startswith('__unnamed_')
            or name in ('cudafe++', 'sleep', 'python', 'node')
            or name.startswith('__cudart_')
        )
    
    def _export_ncu_to_csv(self, ncu_file: str) -> Optional[str]:
        """å¯¼å‡ºncuç»“æœä¸ºCSVæ ¼å¼"""
        csv_file = ncu_file.replace('.ncu-rep', '.csv')
        
        export_cmd = [
            'ncu', '--csv',
            '--log-file', csv_file,
            '--import', ncu_file
        ]
        
        try:
            subprocess.run(export_cmd, capture_output=True, text=True, check=True)
            if Path(csv_file).exists():
                return csv_file
        except:
            pass
        
        return None
    
    def step4_comprehensive_analysis(self, ncu_files: List[str], focus_metrics: Optional[Dict[str, Dict]] = None) -> Dict:
        """ç¬¬å››æ­¥ï¼šç»¼åˆåˆ†ænsyså’Œncuç»“æœ"""
        
        print("ğŸ“Š æ­¥éª¤4: ç»¼åˆåˆ†æç»“æœ...")
        
        comprehensive_results = {
            'timestamp': datetime.now().isoformat(),
            'nsys_overview': self.nsys_stats,
            'hot_kernels_count': len(self.hot_kernels),
            'ncu_detailed_analysis': {},
            'ncu_focus_analysis': focus_metrics or {}
        }
        
        # åˆ†ææ¯ä¸ªncuç»“æœ
        # è‹¥æä¾›ç„¦ç‚¹èšåˆæŒ‡æ ‡ï¼Œåˆ™ä¸å¿…å¯¹å…¨é‡ ncu_full_capture_global é€æ–‡ä»¶åšæ ‡å‡†åˆ†æï¼ˆä»å¯ä¿ç•™ targeted æ–‡ä»¶åˆ†æï¼‰
        for ncu_file in ncu_files:
            csv_file = ncu_file.replace('.ncu-rep', '.csv')
            print(csv_file)
            if Path(csv_file).exists():
                try:
                    # ä½¿ç”¨æˆ‘ä»¬çš„ncuåˆ†æå™¨
                    parser = NCUParser(csv_file)
                    parser.parse()
                    
                    analyzer = NCUAnalyzer(parser)
                    stats = analyzer.analyze()
                    
                    kernel_name = Path(ncu_file).stem
                    comprehensive_results['ncu_detailed_analysis'][kernel_name] = {
                        'kernels_analyzed': len(parser.kernels),
                        'bottlenecks_found': len(analyzer.bottlenecks),
                        'gpu_utilization': stats.get('gpu_utilization', {}),
                        'memory_analysis': stats.get('memory_analysis', {}),
                        'bottleneck_summary': [
                            {
                                'type': b.type,
                                'severity': b.severity,
                                'description': b.description
                            }
                            for b in analyzer.bottlenecks[:3]  # åªå–å‰3ä¸ª
                        ]
                    }
                    
                    # ç”Ÿæˆè¯¦ç»†çš„å¯è§†åŒ–æŠ¥å‘Š
                    visualizer = NCUVisualizer(parser, analyzer)
                    vis_output_dir = self.output_dir / f"visualization_{kernel_name}"
                    visualizer.output_dir = vis_output_dir
                    vis_output_dir.mkdir(exist_ok=True)
                    visualizer.create_visualizations()
                    
                except Exception as e:
                    print(f"âš ï¸  åˆ†æ {ncu_file} å¤±è´¥: {e}")
        
        # ä¿å­˜ç»¼åˆåˆ†æç»“æœ
        results_file = self.output_dir / "comprehensive_analysis.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“‹ ç»¼åˆåˆ†æç»“æœå·²ä¿å­˜: {results_file}")
        return comprehensive_results
    
    def generate_final_report(self, comprehensive_results: Dict) -> str:
        """ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š"""
        
        report_file = self.output_dir / "integrated_performance_report.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# é›†æˆæ€§èƒ½åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # nsysæ¦‚è§ˆ
            f.write("## ğŸ” Nsys å…¨å±€æ€§èƒ½æ¦‚è§ˆ\n\n")
            nsys_overview = comprehensive_results.get('nsys_overview', {})
            
            if 'kernel_analysis' in nsys_overview:
                kernel_stats = nsys_overview['kernel_analysis']
                f.write(f"- **æ€»kernelsæ•°é‡**: {kernel_stats.get('total_kernels', 0)}\n")
                f.write(f"- **æ€»kernelæ‰§è¡Œæ—¶é—´**: {kernel_stats.get('total_kernel_time', 0):.2f} ms\n")
                f.write(f"- **å¹³å‡kernelæ‰§è¡Œæ—¶é—´**: {kernel_stats.get('avg_kernel_time', 0):.3f} ms\n")
            
            # çƒ­ç‚¹kernels
            f.write(f"\n## ğŸ”¥ è¯†åˆ«çš„çƒ­ç‚¹Kernels ({comprehensive_results.get('hot_kernels_count', 0)}ä¸ª)\n\n")
            for i, kernel in enumerate(self.hot_kernels[:10], 1):
                display_name = kernel.get('name','')[:80]
                original_name = kernel.get('original_name')
                csv_flag = ' (CSVæ›¿æ¢)' if kernel.get('csv_substituted') else ''
                if original_name and original_name != kernel.get('name'):
                    display_name = f"{original_name} â†’ {kernel.get('name')}" + csv_flag
                f.write(f"{i}. **{display_name}**\n")
                f.write(f"   - æ€»æ‰§è¡Œæ—¶é—´: {kernel.get('total_time_ms',0):.2f} ms\n")
                f.write(f"   - å¹³å‡æ‰§è¡Œæ—¶é—´: {kernel.get('avg_time_ms',0):.3f} ms\n") 
                f.write(f"   - è°ƒç”¨æ¬¡æ•°: {kernel.get('count',0)}\n")
                if kernel.get('discovered'):
                    f.write(f"   - åç§°æ¥æº: ncu --list-kernels å‘ç°\n")
                f.write("\n")
            
            # ncuæ·±åº¦åˆ†æ
            f.write("## âš¡ NCU æ·±åº¦åˆ†æç»“æœ\n\n")
            ncu_analysis = comprehensive_results.get('ncu_detailed_analysis', {})
            focus_analysis = comprehensive_results.get('ncu_focus_analysis', {})
            
            for kernel_name, analysis in ncu_analysis.items():
                f.write(f"### {kernel_name}\n\n")
            if focus_analysis:
                f.write("## ğŸ¯ ç„¦ç‚¹å†…æ ¸èšåˆæŒ‡æ ‡ (å…¨é‡é‡‡é›†æå–)\n\n")
                for kname, analysis in focus_analysis.items():
                    f.write(f"### {kname}\n\n")
                    gu = analysis.get('gpu_utilization', {})
                    if gu:
                        f.write(f"- å¹³å‡SMæ•ˆç‡: {gu.get('average_sm_efficiency','N/A')}\n")
                        f.write(f"- Occupancy: {gu.get('achieved_occupancy','N/A')}\n")
                    mem = analysis.get('memory_analysis', {}).get('bandwidth_stats', {})
                    if mem:
                        f.write(f"- å¹³å‡å¸¦å®½: {mem.get('average_bandwidth','N/A')} GB/s\n")
                        f.write(f"- L2å‘½ä¸­ç‡: {mem.get('l2_hit_rate','N/A')}%\n")
                    bsum = analysis.get('bottleneck_summary', [])
                    if bsum:
                        f.write("- ä¸»è¦ç“¶é¢ˆ:\n")
                        for b in bsum:
                            f.write(f"  - {b['description']} ({b['severity']})\n")
                    f.write('\n')
                
                gpu_util = analysis.get('gpu_utilization', {})
                if gpu_util:
                    f.write(f"- **å¹³å‡SMæ•ˆç‡**: {gpu_util.get('average_sm_efficiency', 0):.1f}%\n")
                
                memory_analysis = analysis.get('memory_analysis', {})
                if 'bandwidth_stats' in memory_analysis:
                    bw = memory_analysis['bandwidth_stats']
                    f.write(f"- **å¹³å‡å†…å­˜å¸¦å®½**: {bw.get('average_bandwidth', 0):.1f} GB/s\n")
                
                # ç“¶é¢ˆåˆ†æ
                bottlenecks = analysis.get('bottleneck_summary', [])
                if bottlenecks:
                    f.write(f"- **ä¸»è¦æ€§èƒ½ç“¶é¢ˆ**:\n")
                    for bottleneck in bottlenecks:
                        f.write(f"  - {bottleneck['description']} ({bottleneck['severity']})\n")
                
                f.write("\n")
            
            # ä¼˜åŒ–å»ºè®®
            f.write("## ğŸ’¡ ä¼˜åŒ–å»ºè®®\n\n")
            f.write("### åŸºäºnsysåˆ†æ:\n")
            f.write("- å…³æ³¨ä¸Šè¿°çƒ­ç‚¹kernelsçš„ä¼˜åŒ–\n")
            f.write("- æ£€æŸ¥kernelè°ƒç”¨çš„æ—¶é—´é—´éš™ï¼Œä¼˜åŒ–overlap\n\n")
            
            f.write("### åŸºäºncuåˆ†æ:\n")
            f.write("- å¯¹SMæ•ˆç‡ä½çš„kernelsè¿›è¡Œç®—æ³•ä¼˜åŒ–\n")
            f.write("- å¯¹å†…å­˜å¸¦å®½ä½çš„kernelsä¼˜åŒ–è®¿é—®æ¨¡å¼\n")
            f.write("- æ ¹æ®å…·ä½“ç“¶é¢ˆç±»å‹é‡‡å–é’ˆå¯¹æ€§ä¼˜åŒ–æªæ–½\n")
        
        print(f"ğŸ“„ æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return str(report_file)

def create_sglang_analysis_workflow():
    """åˆ›å»ºSGlangä¸“ç”¨çš„åˆ†æå·¥ä½œæµ"""
    DEFAULT_MODEL_DIR = os.getenv('SGLANG_MODEL_PATH') or os.getenv('MODEL_PATH') or '/workspace/models/'

    def run_sglang_integrated_analysis(model_path: Optional[str] = None, 
                                      batch_size: int = 8,
                                      input_len: int = 512, 
                                      output_len: int = 64):
        """è¿è¡ŒSGlangçš„é›†æˆåˆ†æ

        å‚æ•°:
            model_path: æ¨¡å‹è·¯å¾„ï¼Œè‹¥æœªæä¾›åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ SGLANG_MODEL_PATH / MODEL_PATHï¼Œæœ€åå›é€€ /workspace/models/
        """
        if not model_path:
            model_path = DEFAULT_MODEL_DIR.rstrip('/')
            print(f"â„¹ï¸ æœªæä¾› model_pathï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {model_path}")
        # æ„å»ºSGlangå‘½ä»¤
        sglang_cmd = [
            'python', '-m', 'sglang.bench_one_batch',
            '--model-path', model_path,
            '--batch-size', str(batch_size),
            '--input-len', str(input_len),
            '--output-len', str(output_len),
            '--load-format', 'dummy'
        ]
        
        # åˆ›å»ºåˆ†æå™¨
        analyzer = NSysToNCUAnalyzer(f"sglang_analysis_b{batch_size}_i{input_len}_o{output_len}")
        
        # æ­¥éª¤1: nsyså…¨å±€åˆ†æ
        nsys_file = analyzer.step1_nsys_analysis(sglang_cmd, "sglang_overview")
        
        # æ­¥éª¤2: æå–çƒ­ç‚¹kernels
        hot_kernels = analyzer.step2_extract_hot_kernels(nsys_file, top_k=8)
        
        if not hot_kernels:
            print("âŒ æœªå‘ç°çƒ­ç‚¹kernelsï¼Œåˆ†æç»ˆæ­¢")
            return
        
        # æ­¥éª¤3: ncuæ·±åº¦åˆ†æï¼ˆé™åˆ¶åˆ†ææ•°é‡ï¼‰
        ncu_files = analyzer.step3_ncu_targeted_analysis(sglang_cmd, hot_kernels, max_kernels=3)
        
        # æ­¥éª¤4: ç»¼åˆåˆ†æ
        results = analyzer.step4_comprehensive_analysis(ncu_files)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        report_file = analyzer.generate_final_report(results)
        
        print(f"\nğŸ‰ SGlangé›†æˆåˆ†æå®Œæˆ!")
        print(f"ğŸ“ åˆ†æç»“æœç›®å½•: {analyzer.output_dir}")
        print(f"ğŸ“„ åˆ†ææŠ¥å‘Š: {report_file}")
        
        return analyzer.output_dir
    
    return run_sglang_integrated_analysis

# --- è¾…åŠ©å‡½æ•°: å°†é«˜é˜¶æŠ¥å‘Š Markdown ç²—ç•¥ç»“æ„åŒ–ä¸º JSON ---
def _extract_advanced_json(md_text: str) -> Dict[str, Any]:  # type: ignore
    sections: Dict[str, Any] = {}
    current = None
    for line in md_text.splitlines():
        if line.startswith('#'):
            # è·å–æ ‡é¢˜
            title = line.strip('# ').strip()
            current = title
            sections[current] = []
        else:
            if current is not None:
                sections[current].append(line)
    # ç®€å•æŠ½å–ä»»åŠ¡åˆ—è¡¨ä¸åˆ†ç±»
    tasks = []
    for k, v in sections.items():
        if 'ä»»åŠ¡åˆ—è¡¨' in k or 'ç»†ç²’åº¦' in k:
            tasks.extend([ln for ln in v if ln.strip().startswith('- ')])
    summary = sections.get('6. æ€»ç»“ (Summary)', [])
    return {
        'sections': list(sections.keys()),
        'tasks_lines': tasks,
        'summary': '\n'.join(summary[:10]),
        'raw_length': len(md_text)
    }

def main():
    parser = argparse.ArgumentParser(description='é›†æˆ nsys å’Œ ncu çš„æ€§èƒ½åˆ†æå·¥å…·',
                                     epilog='ç¤ºä¾‹: python nsys_to_ncu_analyzer.py -- python -m sglang.bench_one_batch --model-path /path --batch-size 8 --input-len 512 --output-len 64 --load-format dummy')
    parser.add_argument('command', nargs='*', help='è¦åˆ†æçš„å‘½ä»¤ (å¦‚æœªä½¿ç”¨ --raw-cmd, å¯ç”¨ "--" åˆ†éš”)')
    parser.add_argument('--output-dir', default='integrated_analysis', help='è¾“å‡ºç›®å½• (é»˜è®¤æ ¹: /workspace/Agent/AI_Agent_Complete)')
    parser.add_argument('--top-k', type=int, default=10, help='æå–çš„çƒ­ç‚¹kernelæ•°é‡')
    parser.add_argument('--max-ncu-kernels', type=int, default=5, help='ncuåˆ†æçš„æœ€å¤§kernelæ•°é‡')
    parser.add_argument('--min-duration', type=float, default=0.1, help='æœ€å°kernelæ‰§è¡Œæ—¶é—´(ms)')
    parser.add_argument('--full-ncu', action='store_true', help='æ‰§è¡Œä¸€æ¬¡ä¸åšè¿‡æ»¤çš„å…¨é‡ NCU é‡‡é›† (ä¸çƒ­ç‚¹åˆ†æå¹¶è¡Œæˆ–æ›¿ä»£)')
    parser.add_argument('--full-ncu-set', default='compute', help='å…¨é‡é‡‡é›†ä½¿ç”¨çš„ NCU æŒ‡æ ‡é›†åˆ (--set å€¼)')
    parser.add_argument('--full-ncu-launch-limit', type=int, default=None, help='é™åˆ¶å…¨é‡é‡‡é›†çš„ kernel æ¬¡æ•° (--launch-count)')
    parser.add_argument('--save-hot-kernels', action='store_true', help='ä¿å­˜çƒ­ç‚¹ kernel åç§°åˆ° hot_kernels.txt (çº¯æ–‡æœ¬)')

    # é«˜é˜¶æŠ¥å‘Šç›¸å…³å‚æ•°
    parser.add_argument('--advanced-report', action='store_true', help='ç”Ÿæˆé«˜é˜¶ä¼˜åŒ–å»ºè®®æŠ¥å‘Š (advanced_performance_report.md)')
    parser.add_argument('--advanced-detailed', action='store_true', help='é«˜é˜¶æŠ¥å‘ŠåŒ…å«è¯¦ç»†æŒ‡æ ‡å¿«ç…§ä¸ç»†ç²’åº¦ Kernel ä»»åŠ¡')
    parser.add_argument('--advanced-json', action='store_true', help='åŒæ—¶å¯¼å‡ºé«˜é˜¶æŠ¥å‘Šä¸º JSON (advanced_performance_report.json) ä»¥ä¾›çŸ¥è¯†åº“æ‘„å–')
    parser.add_argument('--ingest-advanced', action='store_true', help='å°†é«˜é˜¶æŠ¥å‘Š JSON å†™å…¥çŸ¥è¯†åº“ (éœ€è¦ embedding ç¯å¢ƒå¯ç”¨)')
    parser.add_argument('--kb-path', type=str, default='knowledge_store', help='çŸ¥è¯†åº“å­˜å‚¨ç›®å½• (FAISS)')
    
    # SGlangç‰¹æ®Šå‚æ•°
    parser.add_argument('--sglang-model', type=str, help='SGlangæ¨¡å‹è·¯å¾„ (é»˜è®¤: ç¯å¢ƒå˜é‡ SGLANG_MODEL_PATH / MODEL_PATH æˆ– /workspace/models/)')
    parser.add_argument('--force-model-path', type=str, help='å¼ºåˆ¶è¦†ç›–ç›®æ ‡å‘½ä»¤ä¸­çš„ --model-path å‚æ•°ä¸ºæŒ‡å®šè·¯å¾„ (å¯ç”¨ç¯å¢ƒå˜é‡ FORCE_MODEL_PATH)')
    parser.add_argument('--sglang-batch', type=int, default=8, help='SGlangæ‰¹æ¬¡å¤§å°')
    parser.add_argument('--sglang-input-len', type=int, default=512, help='SGlangè¾“å…¥é•¿åº¦')
    parser.add_argument('--sglang-output-len', type=int, default=64, help='SGlangè¾“å‡ºé•¿åº¦')
    
    # å…è®¸æœªçŸ¥å‚æ•°ä¿ç•™ç»™ç›®æ ‡å‘½ä»¤
    known_args, unknown_tail = parser.parse_known_args()
    if getattr(known_args, 'command', None):
        base_cmd = known_args.command
    else:
        base_cmd = []
    # å¦‚æœç”¨æˆ·ä½¿ç”¨ -- åˆ†éš”å½¢å¼: python script.py -- <target command parts>
    # unknown_tail å°±æ˜¯åç»­çš„çœŸå®å‘½ä»¤å‚æ•°
    target_command = []
    if unknown_tail:
        target_command = unknown_tail
    else:
        target_command = base_cmd
    # å¯é€‰å¼ºåˆ¶æ¨¡å‹è·¯å¾„æ›¿æ¢
    force_model_path = known_args.force_model_path or os.getenv('FORCE_MODEL_PATH')
    if force_model_path:
        def _rewrite_model_path(cmd: List[str]) -> List[str]:
            out = []
            skip_next = False
            for i, token in enumerate(cmd):
                if skip_next:
                    skip_next = False
                    continue
                if token == '--model-path':
                    out.append('--model-path')
                    out.append(force_model_path)
                    skip_next = True  # è·³è¿‡åŸè·¯å¾„
                    # åŸè·¯å¾„ä¸¢å¼ƒ
                elif token.startswith('--model-path='):
                    out.append(f'--model-path={force_model_path}')
                else:
                    out.append(token)
            return out
        before = ' '.join(target_command)
        target_command = _rewrite_model_path(target_command)
        after = ' '.join(target_command)
        if before != after:
            print(f"ğŸ”§ å·²å¼ºåˆ¶è¦†ç›– --model-path: {force_model_path}")
        else:
            # å¦‚æœåŸå‘½ä»¤æœªåŒ…å« --model-pathï¼Œç›´æ¥è¿½åŠ 
            target_command += ['--model-path', force_model_path]
            print(f"ğŸ”§ åŸå‘½ä»¤æœªåŒ…å« --model-pathï¼Œå·²è¿½åŠ : {force_model_path}")

    if not target_command:
        print('âŒ æœªæä¾›å¾…åˆ†æçš„ç›®æ ‡å‘½ä»¤ã€‚ç¤ºä¾‹: python nsys_to_ncu_analyzer.py -- python -m sglang.bench_one_batch --model-path ...')
        return
    args = known_args
    
    try:
        # SGlang ä¸“ç”¨è·¯å¾„é»˜è®¤å¤„ç†
        sglang_workflow = create_sglang_analysis_workflow()
        if args.sglang_model or os.getenv('SGLANG_MODEL_PATH') or os.getenv('MODEL_PATH'):
            sglang_workflow(
                args.sglang_model or os.getenv('SGLANG_MODEL_PATH') or os.getenv('MODEL_PATH'),
                args.sglang_batch,
                args.sglang_input_len,
                args.sglang_output_len
            )
        else:
            # é€šç”¨åˆ†æ
            analyzer = NSysToNCUAnalyzer(args.output_dir)
            
            # æ­¥éª¤1-4
            nsys_file = analyzer.step1_nsys_analysis(target_command)
            hot_kernels = analyzer.step2_extract_hot_kernels(nsys_file, args.top_k, args.min_duration)
            if args.save_hot_kernels and hot_kernels:
                # å·²åœ¨ step2 å†…éƒ¨ç”Ÿæˆ hot_kernels.txtï¼Œè¿™é‡Œåªåšæç¤º
                print("ğŸ“ --save-hot-kernels å·²å¯ç”¨ï¼Œhot_kernels.txt å·²ç”Ÿæˆã€‚")

            full_capture_file = None
            if args.full_ncu:
                full_capture_file = analyzer.full_ncu_capture(
                    target_command,
                    profile_name='ncu_full_capture',
                    set_name=args.full_ncu_set,
                    launch_limit=args.full_ncu_launch_limit
                )

            ncu_files = []
            if hot_kernels:
                targeted = analyzer.step3_ncu_targeted_analysis(target_command, hot_kernels, args.max_ncu_kernels)
                ncu_files.extend(targeted)
            else:
                print('âŒ æœªå‘ç°ç¬¦åˆæ¡ä»¶çš„çƒ­ç‚¹kernels (è·³è¿‡å®šå‘ NCU)')

            if full_capture_file:
                ncu_files.append(full_capture_file)

            advanced_json_obj = None
            if ncu_files:
                results = analyzer.step4_comprehensive_analysis(ncu_files)
                analyzer.generate_final_report(results)
                # ç”Ÿæˆé«˜é˜¶æŠ¥å‘Š
                if args.advanced_report and generate_advanced_report:
                    try:
                        adv_path = generate_advanced_report(analyzer.output_dir, detailed=args.advanced_detailed)
                        print(f"ğŸ§  é«˜é˜¶ä¼˜åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {adv_path}")
                        if args.advanced_json:
                            # å°†ç”Ÿæˆçš„ markdown è½¬ä¸ºç®€å•ç»“æ„åŒ– JSON (æå–éƒ¨åˆ†æ®µè½)
                            md_text = Path(adv_path).read_text(encoding='utf-8')
                            advanced_json_obj = _extract_advanced_json(md_text)
                            json_path = analyzer.output_dir / 'advanced_performance_report.json'
                            json_path.write_text(json.dumps(advanced_json_obj, ensure_ascii=False, indent=2), encoding='utf-8')
                            print(f"ğŸ“¦ é«˜é˜¶æŠ¥å‘Š JSON å·²å¯¼å‡º: {json_path} â¡ï¸ å¯ç”¨äºçŸ¥è¯†åº“æ‘„å–")
                        # å¯é€‰æ‘„å–çŸ¥è¯†åº“
                        if args.ingest_advanced and advanced_json_obj and ingest_json_to_faiss and flatten_json:
                            try:
                                texts = [json.dumps(advanced_json_obj, ensure_ascii=False)]
                                ingest_json_to_faiss(json.dumps(advanced_json_obj, ensure_ascii=False), kb_path=args.kb_path)
                                print("ğŸ“¥ å·²å°è¯•å°†é«˜é˜¶æŠ¥å‘Šå†™å…¥çŸ¥è¯†åº“å‘é‡åº“")
                            except Exception as e:
                                print(f"âš ï¸ é«˜é˜¶æŠ¥å‘ŠçŸ¥è¯†åº“æ‘„å–å¤±è´¥: {e}")
                    except Exception as e:
                        print(f"âš ï¸ é«˜é˜¶æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
                elif args.advanced_report and not generate_advanced_report:
                    print("âš ï¸ advanced_report æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡é«˜é˜¶æŠ¥å‘Šç”Ÿæˆ")
            else:
                print('âš ï¸ æœªç”Ÿæˆä»»ä½• NCU æŠ¥å‘Š, ç»“æŸã€‚')
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­åˆ†æ")
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

