#!/usr/bin/env python3
"""
NVIDIA æ€§èƒ½åˆ†æé›†æˆå·¥å…·
å…ˆç”¨ nsys è¯†åˆ«çƒ­ç‚¹kernelsï¼Œå†ç”¨ ncu æ·±åº¦åˆ†æ

å·¥ä½œæµç¨‹ï¼š
1. nsys profile -> è·å–å…¨å±€æ€§èƒ½overview  
2. æå–çƒ­ç‚¹kernelåç§°
3. ncu profile -> é’ˆå¯¹çƒ­ç‚¹kernelsæ·±åº¦åˆ†æ
4. ç»¼åˆåˆ†ææŠ¥å‘Š

ä½œè€…: xjw
ç‰ˆæœ¬: 1.0
"""

import os
import sys
import json
import sqlite3
import subprocess
import argparse
import re
import math
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime, timezone

# å¯¼å…¥æˆ‘ä»¬çš„åˆ†æå·¥å…·
sys.path.append(str(Path(__file__).parent))
from nsys_parser import NsysParser, NsysAnalyzer
from ncu_parser import NCUParser, NCUAnalyzer, NCUVisualizer, NCUReporter
try:
    from .roofline_estimator import compute_roofline  # type: ignore
except Exception:
    try:
        from backend.utils.roofline_estimator import compute_roofline  # type: ignore
    except Exception:
        local_dir = Path(__file__).parent
        if str(local_dir) not in sys.path:
            sys.path.insert(0, str(local_dir))
        from roofline_estimator import compute_roofline  # type: ignore

# å¼•å…¥é«˜é˜¶æŠ¥å‘Šä¸çŸ¥è¯†åº“æ‘„å–æ¨¡å—ï¼ˆå¯é€‰ï¼‰
try:
    from backend.advanced_report import generate_advanced_report
except Exception:
    generate_advanced_report = None  # type: ignore
try:
    from backend.knowledge_bases.kb_ingest import ingest_json_to_faiss, flatten_json
except Exception:
    ingest_json_to_faiss = None  # type: ignore
    flatten_json = None  # type: ignore

class NSysToNCUAnalyzer:
    """é›†æˆ nsys å’Œ ncu çš„åˆ†æå·¥å…·

    ç»Ÿä¸€è¾“å‡ºç›®å½•:
        é»˜è®¤ä½¿ç”¨ /workspace/Agent/AI_Agent_Complete ä½œä¸ºæ ¹è·¯å¾„ä¸‹çš„ integrated_analysis å­ç›®å½•ï¼Œ
        ä¾¿äº Agent è¯»å–æ‰€æœ‰ç”Ÿæˆçš„æŠ¥å‘Šå’Œä¸­é—´äº§ç‰©ã€‚
    """
    DEFAULT_BASE_DIR = Path("/workspace/Agent/AI_Agent_Complete")

    def __init__(self, output_dir: str = "integrated_analysis", env: Optional[Dict[str, str]] = None):
        # å¦‚æœç”¨æˆ·ä¼ å…¥çš„æ˜¯ç»å¯¹è·¯å¾„åˆ™ä½¿ç”¨åŸå€¼ï¼Œå¦åˆ™æ‹¼æ¥åˆ°é»˜è®¤åŸºè·¯å¾„ä¸‹
        base = self.DEFAULT_BASE_DIR
        if output_dir.startswith('/'):
            self.output_dir = Path(output_dir)
        else:
            self.output_dir = base / output_dir
        self.output_dir.mkdir(exist_ok=True)
        self.hot_kernels = []
        self.nsys_stats = {}
        self.ncu_results = {}
        # ä¸ºå­è¿›ç¨‹è°ƒç”¨ï¼ˆnsys/ncuï¼‰é¢„å…ˆä¿å­˜ç¯å¢ƒå˜é‡ï¼Œä¾¿äºæ§åˆ¶ GPU ç»‘å®š
        self.env = env or os.environ.copy()
        # é»˜è®¤è¡¥å…… FP16/F16 Tensor Core æŒ‡æ ‡ï¼Œä¾¿äºåç»­åˆ†æ
        self.ncu_metrics = [
            'smsp__inst_executed_pipe_tensor_op_hf.sum',
            'smsp__inst_executed_pipe_fp16.sum',
            'sm__inst_executed_pipe_tensor_op_hf.sum',
            'sm__sass_thread_inst_executed_op_hf.sum',
        ]
        self.roofline_estimate: Optional[Dict[str, Any]] = None
        
    def step1_nsys_analysis(self, target_command: List[str], profile_name: str = "overview") -> str:
        """ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨nsysè¿›è¡Œå…¨å±€æ€§èƒ½åˆ†æ"""
        
        nsys_profile = self.output_dir / f"{profile_name}.nsys-rep"
        
        # æ„å»ºnsyså‘½ä»¤
        nsys_cmd = [
            'nsys', 'profile',
            '-o', str(nsys_profile.with_suffix('')),  # nsysä¼šè‡ªåŠ¨æ·»åŠ .nsys-rep
            '-t', 'cuda,nvtx,osrt',
            '--cuda-memory-usage=true',
            '--force-overwrite=true'
            # '--capture-range=nvtx',
            # '--capture-range-end=stop',
        ] + target_command 
        
        print("ğŸš€ æ­¥éª¤1: è¿è¡Œnsyså…¨å±€æ€§èƒ½åˆ†æ...")
        print(f"å‘½ä»¤: {' '.join(nsys_cmd)}")
        
        try:
            result = subprocess.run(nsys_cmd, capture_output=True, text=True, check=True, env=self.env)
            print(f"âœ… nsysåˆ†æå®Œæˆ: {nsys_profile}")
            return str(nsys_profile)
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ nsysåˆ†æå¤±è´¥: {e.stderr}")
            raise
    
    def step2_extract_hot_kernels(self, nsys_file: str, 
                                  top_k: int = 10, 
                                  min_duration_ms: float = 0.1) -> List[Dict]:
        """ç¬¬äºŒæ­¥ï¼šè§£æ nsys å¹¶ç”Ÿæˆ layer_kernels.csvï¼ˆè·³è¿‡çƒ­ç‚¹æå–ï¼Œç›´æ¥ä½¿ç”¨ NVTX å…³è”çš„ kernelsï¼‰
        ç°åœ¨é»˜è®¤ä»…ä½¿ç”¨ Run[2] çš„å­é›†è¿›è¡Œåç»­åˆ†æä¸æŠ¥å‘Šã€‚
        """
        
        print("ğŸ” æ­¥éª¤2: è§£æ nsys å¹¶ç”Ÿæˆ layer_kernels.csvï¼ˆè·³è¿‡çƒ­ç‚¹æå–ï¼‰...")
        
        parser = NsysParser(nsys_file)
        parser.parse()
        # å¯¼å‡ºç”±ä¸‰è¡¨å…³è”å¾—åˆ°çš„ layer_kernels.csvï¼ˆä½äº .sqlite åŒç›®å½•ï¼‰
        lk_csv = parser.export_kernel_summary_csv(nsys_file, self.output_dir / f"{Path(nsys_file).stem}_kernels")
        target_csv = self.output_dir / "layer_kernels.csv"
        if lk_csv:
            try:
                src = Path(lk_csv)
                if src.resolve() == target_csv.resolve():
                    # æºç›®æ ‡ä¸€è‡´ï¼Œè§†ä¸ºæˆåŠŸ
                    print(f"ğŸ“„ layer_kernels.csv å·²å­˜åœ¨äºè¾“å‡ºç›®å½•: {target_csv}")
                else:
                    # å¼ºåˆ¶è¦†ç›–
                    import shutil, os
                    if target_csv.exists():
                        os.remove(target_csv)
                    shutil.copy2(src, target_csv)
                    print(f"ğŸ“„ å·²ç”Ÿæˆ layer_kernels.csv: {target_csv}")
            except Exception as e:
                print(f"âš ï¸ æ‹·è´ layer_kernels.csv å¤±è´¥: {e}. åŸè·¯å¾„: {lk_csv}")
        else:
            print("âš ï¸ æœªç”Ÿæˆ layer_kernels.csvï¼Œè¯·ç¡®è®¤ NVTX_EVENTS/StringIds/CUPTI è¡¨å­˜åœ¨ä¸”æœ‰ Layer[...] æ ‡ç­¾")

        # ä»…ä¿ç•™ Run[2] çš„å­é›†
        run_tag = "#Run[2]"
        run_csv = self.output_dir / "layer_kernels_run2.csv"
        try:
            import csv
            kept = 0
            with open(target_csv, newline='', encoding='utf-8') as f, open(run_csv, 'w', newline='', encoding='utf-8') as g:
                rdr = csv.DictReader(f)
                w = csv.DictWriter(g, fieldnames=rdr.fieldnames)
                w.writeheader()
                for r in rdr:
                    if run_tag in (r.get("layer") or ""):
                        w.writerow(r)
                        kept += 1
            print(f"ğŸ“„ å·²ç”Ÿæˆä»… Run[2] çš„å­é›†: {run_csv}ï¼ˆ{kept} è¡Œï¼‰")
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆ Run[2] å­é›†å¤±è´¥: {e}")

        # ç”¨ Run[2] å­é›†ç”Ÿæˆç²¾ç®€ JSONï¼ˆä»… nameã€dur_msï¼‰ï¼Œå¹¶ä»¥æ­¤ä½œä¸ºåç»­å”¯ä¸€æ•°æ®æº
        rows_run2 = []
        try:
            import csv, json
            with open(run_csv, newline='', encoding='utf-8') as f:
                rdr = csv.DictReader(f)
                for r in rdr:
                    rows_run2.append({
                        'name': r.get('kernel_name',''),
                        'dur_ms': float(r.get('dur_ms', 0.0) or 0.0)
                    })
            hot_json_path = self.output_dir / "layer_kernels_run2_hot.json"
            hot_json_path.write_text(json.dumps(rows_run2, ensure_ascii=False, indent=2), encoding='utf-8')
            print(f"ğŸ“„ å·²ç”Ÿæˆç²¾ç®€çƒ­ç‚¹æ–‡ä»¶(é¡ºåºä¿ç•™ï¼Œä¸å»é‡): {hot_json_path}")
        except Exception as e:
            print(f"âš ï¸ è¯»å– Run[2] å­é›†å¤±è´¥: {e}")
            rows_run2 = []

        # ç”¨ Run[2] JSON æ›´æ–° nsys æ¦‚è§ˆï¼ˆtotal_time ä¸º dur_ms æ±‚å’Œï¼Œä¸ç»Ÿè®¡ count/maxï¼‰
        total_time_ms = sum(item.get('dur_ms', 0.0) for item in rows_run2)
        total_kernels = len(rows_run2)
        avg_time_ms = (total_time_ms / total_kernels) if total_kernels else 0.0

        analyzer = NsysAnalyzer(parser)
        self.nsys_stats = {
            'kernel_analysis': {
                'total_kernels': total_kernels,
                'total_kernel_time': total_time_ms,
                'avg_kernel_time': avg_time_ms,
            },
            'layer_kernels_rows': rows_run2,           # ç”¨ç²¾ç®€ç»“æ„æ›¿ä»£
            'layer_kernels_source': str(hot_json_path) # æŒ‡å‘ run2_hot.json
        }

        # è®°å½•å…³é”® NVTX èŒƒå›´çš„èµ·æ­¢ä¸æ—¶é•¿ï¼Œä¾¿äºæŠ¥å‘Šå¼•ç”¨
        nvtx_tag = "Layer[0]#Run[2]"
        nvtx_span = self._query_nvtx_range(parser.sqlite_file, nvtx_tag)
        if nvtx_span:
            self.nsys_stats.setdefault('nvtx_ranges', {})[nvtx_tag] = nvtx_span

        # å°†â€œçƒ­ç‚¹â€åˆ—è¡¨è®¾ç½®ä¸º run2_hot.json çš„é¡ºåºåˆ—è¡¨ï¼ˆä¸å»é‡ã€ä¸æ’åºï¼‰
        self.hot_kernels = rows_run2[:]
        print(f"â„¹ï¸ å·²æŒ‰è¦æ±‚ä½¿ç”¨ Run[2] JSONï¼ˆ{len(self.hot_kernels)} æ¡ï¼‰ï¼Œé¡ºåºä¿ç•™ä¸”ä¸å»é‡ã€‚")
        return self.hot_kernels
    
    def step3_ncu_targeted_analysis(self, target_command: List[str], 
                                   kernels_to_analyze: List[Dict],
                                   max_kernels: Optional[int] = None) -> List[str]:
        """ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨ncuå¯¹çƒ­ç‚¹kernelsè¿›è¡Œæ·±åº¦åˆ†æ
        ç°åœ¨é»˜è®¤å¯¹ä¼ å…¥åˆ—è¡¨çš„æ¯ä¸€ä¸ª kernel è¿›è¡Œåˆ†æï¼ˆä¿æŒé¡ºåºï¼‰ï¼Œé™¤éæ˜¾å¼æä¾› max_kernelsã€‚
        """
        
        print("âš¡ æ­¥éª¤3: ä½¿ç”¨ncuæ·±åº¦åˆ†æçƒ­ç‚¹kernels...")
        
        ncu_results = []

        # å…è®¸ä»…åŒ…å« {name, dur_ms} çš„æœ€ç®€ç»“æ„
        # è‹¥æ£€æµ‹åˆ°å ä½/æ•°å­— kernel åï¼Œå°è¯• list-kernels å‘ç°çœŸå®å
        if any(self._is_placeholder_name(str(k.get('name',''))) for k in kernels_to_analyze):
            print("ğŸ” æ£€æµ‹åˆ°å ä½/æ•°å­—kernelåï¼Œè§¦å‘ ncu --list-kernels è¿›è¡ŒçœŸå®åç§°å‘ç°...")
            discovered = self.list_kernels_with_ncu(target_command)
            selected = self._select_real_kernels(discovered, len(kernels_to_analyze) if max_kernels is None else max_kernels)
            print(f"ğŸ§­ é€‰æ‹©ç”¨äºæ·±åº¦åˆ†æçš„çœŸå®kernelåç§°: {selected}")
            # æ›¿æ¢å‰è‹¥é•¿åº¦ä¸è¶³åˆ™è¡¥é½
            for i, real_name in enumerate(selected):
                if i < len(kernels_to_analyze):
                    kernels_to_analyze[i]['name'] = real_name
                    kernels_to_analyze[i]['discovered'] = True
                else:
                    kernels_to_analyze.append({'name': real_name, 'discovered': True})

        # åœ¨æœ€ç»ˆåˆ†æå‰å»é‡ï¼Œé¿å…é‡å¤ kernel é€ æˆå†—ä½™é‡‡é›†
        def _deduplicate(entries: List[Dict]) -> List[Dict]:
            seen = set()
            unique: List[Dict] = []
            duplicates = 0
            for entry in entries:
                name = str(entry.get('name', '')).strip()
                if not name:
                    unique.append(entry)
                    continue
                if name in seen:
                    duplicates += 1
                    continue
                seen.add(name)
                unique.append(entry)
            if duplicates:
                print(f"â™»ï¸  å»é‡åå‡å°‘ {duplicates} ä¸ªé‡å¤ kernel")
            return unique

        kernels_to_analyze = _deduplicate(kernels_to_analyze)

        # åˆ†ææ•°é‡ï¼šé»˜è®¤å…¨éƒ¨ï¼›è‹¥æŒ‡å®š max_kernels åˆ™æˆªæ–­
        if max_kernels is not None:
            kernels_to_analyze = kernels_to_analyze[:max_kernels]
        
        for i, kernel_info in enumerate(kernels_to_analyze):
            kernel_name = str(kernel_info.get('name', 'kernel')).strip()

            # æ¸…ç†kernelåç§°ï¼Œç”¨äºæ–‡ä»¶å
            safe_name = re.sub(r'[^\w\-_]', '_', kernel_name)[:50]
            ncu_profile = self.output_dir / f"ncu_kernel_{i}_{safe_name}"
            
            print(f"ğŸ¯ æ­£åœ¨åˆ†ækernel {i+1}/{len(kernels_to_analyze)}: {kernel_name[:60]}...")

            def attempt_profile(attempt_cmd: List[str], attempt_tag: str) -> Optional[str]:
                """å°è£…ä¸€æ¬¡ ncu å°è¯•ï¼Œè¿”å› .ncu-rep è·¯å¾„æˆ– None"""
                try:
                    res = subprocess.run(attempt_cmd, env=self.env, capture_output=True, text=True)
                    if res.returncode != 0:
                        snippet = (res.stderr or '')[:200].replace('\n', ' ')
                        print(f"âš ï¸ å°è¯• {attempt_tag} å¤±è´¥(returncode={res.returncode}): {snippet}")
                        return None
                    ncu_file = str(ncu_profile) + '.ncu-rep'
                    if Path(ncu_file).exists():
                        print(f"âœ… æˆåŠŸç”Ÿæˆ NCU æŠ¥å‘Š ({attempt_tag}): {ncu_file}")
                        return ncu_file
                    else:
                        print(f"âš ï¸ å°è¯• {attempt_tag} æœªç”Ÿæˆ .ncu-rep æ–‡ä»¶: {ncu_file}")
                except subprocess.TimeoutExpired:
                    print(f"â° å°è¯• {attempt_tag} è¶…æ—¶")
                except Exception as e:
                    print(f"âŒ å°è¯• {attempt_tag} å¼‚å¸¸: {e}")
                return None

            # å›é€€ç­–ç•¥ï¼šç²¾ç¡®åŒ¹é… -> æ­£åˆ™å‰ç¼€ -> æ— è¿‡æ»¤
            attempts = []
            attempts.append({
                'tag': 'exact-demangled',
                'cmd': ['ncu', '--kernel-name-base', 'demangled', '--kernel-name', kernel_name,
                        '--rename-kernels=0', '--set', 'full']
            })
            prefix_raw = re.sub(r'"', '', kernel_name)[:60]
            prefix_regex = re.sub(r'([\\.^$|?*+\[\](){}])', r'\\\1', prefix_raw)
            attempts.append({
                'tag': 'regex-prefix',
                'cmd': ['ncu', '--kernel-name-base', 'demangled', '--kernel-name', f'regex:^{prefix_regex}',
                        '--rename-kernels=0', '--set', 'full']
            })
            attempts.append({
                'tag': 'unfiltered-basic',
                'cmd': ['ncu', '--launch-count', '50', '--set', 'compute']
            })

            for att in attempts:
                metrics_segment = self._metrics_args()
                att['cmd'] = att['cmd'] + metrics_segment + ['-o', str(ncu_profile), '--force-overwrite'] + target_command

            produced = None
            for att in attempts:
                print(f"ğŸ” NCU å°è¯•: {att['tag']}")
                print(att['cmd'])
                produced = attempt_profile(att['cmd'], att['tag'])
                if produced:
                    break

            if produced:
                ncu_results.append(produced)
                self._export_ncu_to_csv(produced)
            else:
                print(f"âŒ æ‰€æœ‰å°è¯•å‡æœªç”Ÿæˆ NCU æŠ¥å‘Š: {kernel_name[:80]}")
        
        return ncu_results


    def list_kernels_with_ncu(self, target_command: List[str]) -> List[str]:
        """è¿è¡Œ ncu --list-kernels ä»¥è·å–å®é™…å¯åˆ†æçš„ kernel åç§°åˆ—è¡¨"""
        cmd = ['ncu', '--list-kernels'] + target_command
        print(f"ğŸ§ª è¿è¡Œ: {' '.join(cmd)}")
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=600, env=self.env)
            output = result.stdout + '\n' + result.stderr
        except Exception as e:
            print(f"âŒ list-kernels å¤±è´¥: {e}")
            return []

        # è§£æè¾“å‡ºï¼šæ¯è¡Œå¯èƒ½åŒ…å« kernel åç§°ã€‚æˆ‘ä»¬è¿‡æ»¤å‡ºå« 'Kernel', 'cuda', 'cutlass', 'flash', 'aten', 'cublas', 'gemm', 'matmul', 'triton'
        lines = [l.strip() for l in output.splitlines() if l.strip()]
        kernels = []
        import re
        pattern = re.compile(r'(Kernel|cuda|cutlass|flash|aten|cublas|gemm|matmul|triton)', re.IGNORECASE)
        for line in lines:
            # å¸¸è§æ ¼å¼: index + name æˆ– ç›´æ¥ name
            # æ’é™¤å¤ªçŸ­è¡Œ
            if len(line) < 4:
                continue
            if pattern.search(line):
                # å»æ‰å‰å¯¼ç¼–å·æˆ–è£…é¥°ç¬¦
                cleaned = re.sub(r'^\s*\d+\s*[:\-]?\s*', '', line)
                kernels.append(cleaned)
        # å»é‡ä¿æŒé¡ºåº
        seen = set(); uniq = []
        for k in kernels:
            if k not in seen:
                seen.add(k); uniq.append(k)
        print(f"ğŸ“‹ list-kernels è·å¾—å€™é€‰ {len(uniq)} ä¸ª (å‰10): {uniq[:10]}")
        return uniq

    def _select_real_kernels(self, discovered: List[str], max_kernels: int) -> List[str]:
        """æ ¹æ®ä¼˜å…ˆçº§ä»å‘ç°çš„ kernel åç§°åˆ—è¡¨ä¸­æŒ‘é€‰ç”¨äºåˆ†æçš„åç§°"""
        if not discovered:
            return []
        priority_patterns = [
            'FlashAttn', 'flash', 'cutlass', 'triton', 'gemm', 'matmul', 'cublas', 'aten', 'reduce', 'norm'
        ]
        scored = []
        for name in discovered:
            low = name.lower()
            score = 0
            for idx, pat in enumerate(priority_patterns):
                if pat.lower() in low:
                    score += (100 - idx)  # earlier pattern higher score
            # é•¿åº¦å’ŒåŒ…å« Kernel å­—æ ·åŠ ä¸€ç‚¹åˆ†
            if 'kernel' in low:
                score += 5
            if len(name) > 30:
                score += 1
            scored.append((score, name))
        # æ’åºï¼Œåˆ†æ•°é«˜çš„é å‰
        scored.sort(reverse=True)
        selected = [n for s, n in scored[:max_kernels]]
        return selected

    def _metrics_args(self) -> List[str]:
        metrics = getattr(self, 'ncu_metrics', [])
        if not metrics:
            return []
        return ['--metrics', ','.join(metrics)]

    def _is_placeholder_name(self, name: str) -> bool:
        # å°†æ•°å­—ã€__unnamed_ ä»¥åŠè‹¥å¹²ä½ä¿¡å· / æ¡†æ¶æ€§åå­—è§†ä¸ºå ä½ã€‚å¢åŠ  __cudart_ å‰ç¼€ï¼Œä»¥ä¾¿åç»­ç”¨ CSV çœŸå® kernel åæ›¿æ¢ã€‚
        return (
            name.isdigit()
            or name.startswith('__unnamed_')
            or name in ('cudafe++', 'sleep', 'python', 'node')
            or name.startswith('__cudart_')
        )
    
    def _query_nvtx_range(self, sqlite_path: Optional[Path], tag: str) -> Optional[Dict[str, Any]]:
        if not sqlite_path or not Path(sqlite_path).exists():
            return None
        pattern = f"%{tag}%"
        sql = (
            "SELECT n.start, n.end, (n.end - n.start) AS span_ns "
            "FROM NVTX_EVENTS n "
            "LEFT JOIN StringIds s ON n.textId = s.id "
            "WHERE (n.text LIKE ? OR s.value LIKE ?) AND n.start IS NOT NULL AND n.end IS NOT NULL "
            "ORDER BY span_ns DESC LIMIT 1"
        )
        try:
            with sqlite3.connect(str(sqlite_path)) as conn:
                row = conn.execute(sql, (pattern, pattern)).fetchone()
        except Exception as exc:
            print(f"âš ï¸ NVTX èŒƒå›´æŸ¥è¯¢å¤±è´¥: {exc}")
            return None
        if not row:
            return None
        start_ns, end_ns, span_ns = row
        if start_ns is None or end_ns is None or span_ns is None:
            return None
        return {
            'start_ns': int(start_ns),
            'end_ns': int(end_ns),
            'duration_ns': int(span_ns),
            'duration_ms': float(span_ns) / 1_000_000.0,
        }

    def _export_ncu_to_csv(self, ncu_file: str) -> Optional[str]:
        """å¯¼å‡ºncuç»“æœä¸ºCSVæ ¼å¼"""
        csv_file = ncu_file.replace('.ncu-rep', '.csv')
        
        export_cmd = [
            'ncu', '--csv',
            '--log-file', csv_file,
            '--import', ncu_file
        ]
        
        try:
            subprocess.run(export_cmd, capture_output=True, text=True, check=True)
            if Path(csv_file).exists():
                return csv_file
        except:
            pass
        
        return None
    
    def step4_comprehensive_analysis(self, ncu_files: List[str], focus_metrics: Optional[Dict[str, Dict]] = None) -> Dict:
        """ç¬¬å››æ­¥ï¼šç»¼åˆåˆ†ænsyså’Œncuç»“æœ"""
        
        print("ğŸ“Š æ­¥éª¤4: ç»¼åˆåˆ†æç»“æœ...")
        
        comprehensive_results = {
            'timestamp': datetime.now().isoformat(),
            'nsys_overview': self.nsys_stats,
            'hot_kernels_count': len(self.hot_kernels),
            'ncu_detailed_analysis': {},
            'ncu_focus_analysis': focus_metrics or {},
            'roofline_estimate': self.roofline_estimate,
        }
        
        # åˆ†ææ¯ä¸ªncuç»“æœ
        # è‹¥æä¾›ç„¦ç‚¹èšåˆæŒ‡æ ‡ï¼Œåˆ™ä¸å¿…å¯¹å…¨é‡ ncu_full_capture_global é€æ–‡ä»¶åšæ ‡å‡†åˆ†æï¼ˆä»å¯ä¿ç•™ targeted æ–‡ä»¶åˆ†æï¼‰
        for ncu_file in ncu_files:
            csv_file = ncu_file.replace('.ncu-rep', '.csv')
            print(csv_file)
            if Path(csv_file).exists():
                try:
                    # ä½¿ç”¨æˆ‘ä»¬çš„ncuåˆ†æå™¨
                    parser = NCUParser(csv_file)
                    parser.parse()
                    
                    analyzer = NCUAnalyzer(parser)
                    stats = analyzer.analyze()
                    
                    kernel_name = Path(ncu_file).stem
                    comprehensive_results['ncu_detailed_analysis'][kernel_name] = {
                        'kernels_analyzed': len(parser.kernels),
                        'bottlenecks_found': len(analyzer.bottlenecks),
                        'gpu_utilization': stats.get('gpu_utilization', {}),
                        'memory_analysis': stats.get('memory_analysis', {}),
                        'bottleneck_summary': [
                            {
                                'type': b.type,
                                'severity': b.severity,
                                'description': b.description
                            }
                            for b in analyzer.bottlenecks[:3]  # åªå–å‰3ä¸ª
                        ]
                    }
                    
                    # ç”Ÿæˆè¯¦ç»†çš„å¯è§†åŒ–æŠ¥å‘Š
                    visualizer = NCUVisualizer(parser, analyzer)
                    vis_output_dir = self.output_dir / f"visualization_{kernel_name}"
                    visualizer.output_dir = vis_output_dir
                    vis_output_dir.mkdir(exist_ok=True)
                    visualizer.create_visualizations()
                    
                except Exception as e:
                    print(f"âš ï¸  åˆ†æ {ncu_file} å¤±è´¥: {e}")
        
        # ä¿å­˜ç»¼åˆåˆ†æç»“æœ
        results_file = self.output_dir / "comprehensive_analysis.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“‹ ç»¼åˆåˆ†æç»“æœå·²ä¿å­˜: {results_file}")
        return comprehensive_results
    
    def generate_final_report(self, comprehensive_results: Dict) -> str:
        """ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š"""
        report_file = self.output_dir / "integrated_performance_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("é›†æˆæ€§èƒ½åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().astimezone().strftime('%Yå¹´%mæœˆ%dæ—¥')}\n\n")
            roofline = comprehensive_results.get('roofline_estimate') or {}
            if roofline:
                def fmt_time(seconds: Optional[float]) -> str:
                    if seconds is None:
                        return "N/A"
                    if isinstance(seconds, float) and math.isinf(seconds):
                        return "âˆ"
                    return f"{seconds * 1000:.3f} ms"

                def fmt_perf(value: float) -> str:
                    return f"{value / 1e12:.2f} TOPS"

                def fmt_mem(value: float) -> str:
                    return f"{value / 1e9:.2f} GB"

                def fmt_ops(value: float) -> str:
                    return f"{value / 1e12:.2f} TOPs"

                def fmt_ai(value: float) -> str:
                    return f"{value:.2f} OPs/Byte"

                bits = roofline.get('precision_bits', {})
                params = roofline.get('params', {})
                prefill_r = roofline.get('prefill', {})
                decode_r = roofline.get('decode', {})
                overall_r = roofline.get('overall', {})
                f.write("ä¸€ã€Roofline é¢„æµ‹æŒ‡æ ‡\n")
                f.write(f" ç¡¬ä»¶: {roofline.get('hardware', 'unknown')}\n")
                f.write(
                    f" ç²¾åº¦: W{bits.get('w', 'N/A')} / A{bits.get('a', 'N/A')} / KV{bits.get('kv', 'N/A')}\n"
                )
                f.write(
                    f" Prefill é˜¶æ®µ: å¼ºåº¦ {fmt_ai(prefill_r.get('arithmetic_intensity', 0.0))}, "
                    f"å—é™ç±»å‹ {prefill_r.get('bound', 'N/A')}, "
                    f"æ€§èƒ½ {fmt_perf(prefill_r.get('performance', 0.0))}, "
                    f"é¢„è®¡æ—¶é•¿ {fmt_time(prefill_r.get('inference_time'))}, "
                    f"å†…å­˜è®¿é—® {fmt_mem(prefill_r.get('memory_access', 0.0))}\n"
                )
                decode_total_time = decode_r.get('total_time')
                f.write(
                    f" Decode å•token: å¼ºåº¦ {fmt_ai(decode_r.get('arithmetic_intensity', 0.0))}, "
                    f"å—é™ç±»å‹ {decode_r.get('bound', 'N/A')}, "
                    f"æ€§èƒ½ {fmt_perf(decode_r.get('performance', 0.0))}, "
                    f"å•tokenæ—¶é•¿ {fmt_time(decode_r.get('inference_time'))}, "
                    f"æ€»æ—¶é•¿(@è¾“å‡º{params.get('output_len', 0)} token) {fmt_time(decode_total_time)}, "
                    f"æ€»å†…å­˜ {fmt_mem(decode_r.get('total_memory', 0.0))}\n"
                )
                f.write(
                    f" æ€»ä½“ä¼°è®¡: å¼ºåº¦ {fmt_ai(overall_r.get('arithmetic_intensity', 0.0))}, "
                    f"å—é™ç±»å‹ {overall_r.get('bound', 'N/A')}, "
                    f"æ€§èƒ½ {fmt_perf(overall_r.get('performance', 0.0))}, "
                    f"é¢„è®¡æ€»æ—¶é•¿ {fmt_time(overall_r.get('inference_time'))}, "
                    f"æ€»ä½“OPs {fmt_ops(overall_r.get('total_ops', 0.0))}\n\n"
                )
            # nsysæ¦‚è§ˆï¼ˆä¼˜å…ˆå±•ç¤º layer_kernelsï¼‰
            f.write("äºŒã€Nsys å…¨å±€æ€§èƒ½æ¦‚è§ˆ\n")
            nsys_overview = comprehensive_results.get('nsys_overview', {})
            # layer_rows = nsys_overview.get('layer_kernels_rows', [])
            # src_hint = nsys_overview.get('layer_kernels_source')
            # if layer_rows:
            #     if src_hint:
            #         f.write(f"- æ¥æºï¼š{src_hint}\n")
            #     else:
            #         f.write("- æ¥æºï¼šNVTX_EVENTS + StringIds + CUPTI_ACTIVITY_KIND_KERNEL ä¸‰è¡¨å…³è”\n")
            #     f.write("- æ˜ç»†è§: layer_kernels_run2_hot.json\n\n")
            #     preview = layer_rows[:20]
            #     for r in preview:
            #         f.write(f"- {str(r.get('name',''))[:80]} | {r.get('dur_ms',0)} ms\n")
            #     f.write("\n")
            # # æ¦‚è§ˆ
            kernel_stats = nsys_overview.get('kernel_analysis')
            kernel_time = 0.0
            if kernel_stats:
                kernel_time = kernel_stats.get('total_kernel_time', 0.0)
                f.write(f" æ€»kernelsæ•°é‡: {kernel_stats.get('total_kernels', 0)}\n")
                f.write(f" æ€»kernelæ‰§è¡Œæ—¶é—´: {kernel_time:.2f} ms\n")
                # f.write(f"- å¹³å‡kernelæ‰§è¡Œæ—¶é—´: {kernel_stats.get('avg_kernel_time', 0):.3f} ms\n")
            nvtx_ranges = nsys_overview.get('nvtx_ranges', {})
            for tag, span in nvtx_ranges.items():
                start_ns = span.get('start_ns')
                end_ns = span.get('end_ns')
                duration_ms = span.get('duration_ms')
                if duration_ms is None and isinstance(start_ns, (int, float)) and isinstance(end_ns, (int, float)):
                    duration_ms = (end_ns - start_ns) / 1_000_000.0
                if duration_ms is None:
                    continue
                idle_ratio = 0.0 if not duration_ms else max(duration_ms - kernel_time, 0.0) / duration_ms * 100.0
                f.write(
                    f" {tag} èŒƒå›´æŒç»­æ—¶é—´: {duration_ms:.2f} ms, ç©ºæ³¡ç‡ {idle_ratio:.2f} %\n"
                )
            f.write("\n")
            # çƒ­ç‚¹kernelsï¼ˆä¿æŒé¡ºåºï¼Œä¸å»é‡ï¼Œåªæ˜¾ç¤ºæ¯æ¡çš„ dur_msï¼‰
            # f.write(f"## ğŸ”¥ Run[2] Kernelsï¼ˆæŒ‰å‡ºç°é¡ºåºï¼‰\n\n")
            total_kernel_time_ms = sum(k.get('dur_ms', 0.0) for k in self.hot_kernels)
            for i, kernel in enumerate(self.hot_kernels, 1):
                display_name = str(kernel.get('name',''))
                dur = float(kernel.get('dur_ms', 0.0))
                percent = (dur / total_kernel_time_ms * 100.0) if total_kernel_time_ms > 0 else 0.0
                f.write(f"{i}. {display_name}\n")
                f.write(f"   - æ‰§è¡Œæ—¶é—´: {dur:.3f} ms\n")
                f.write(f"   - æ—¶é—´å æ¯”: {percent:.2f}%\n\n")
            
            # NCU æ·±åº¦åˆ†æ
            f.write("ä¸‰ã€ NCU æ·±åº¦åˆ†æç»“æœ\n\n")
            ncu_analysis = comprehensive_results.get('ncu_detailed_analysis', {})
            focus_analysis = comprehensive_results.get('ncu_focus_analysis', {})
            
            # å†™å…¥é€ kernel è¯¦ç»†
            items = []
            for kernel_name, analysis in ncu_analysis.items():
                m = re.match(r'^ncu_kernel_(\d+)_', kernel_name)
                idx = int(m.group(1)) if m else 10**9  # æ— å‰ç¼€çš„æ”¾åœ¨æœ€å
                items.append((idx, kernel_name, analysis))
            items.sort(key=lambda x: x[0])

            for i, (idx, kernel_name, analysis) in enumerate(items, 1):
                # æ ‡é¢˜æ”¹ä¸ºç¼–å·è¡Œ
                f.write(f"{i}. {kernel_name}\n\n")
                # åŸºæœ¬ç»Ÿè®¡
                f.write(f"   - è¯†åˆ«ç“¶é¢ˆæ•°: {analysis.get('bottlenecks_found', 0)}\n")
                # GPU åˆ©ç”¨ç‡
                gu = analysis.get('gpu_utilization', {})
                if gu:
                    f.write(f"   - å¹³å‡SMæ•ˆç‡: {gu.get('average_sm_efficiency', 'N/A')}\n")
                    f.write(f"   - æœ€é«˜SMæ•ˆç‡: {gu.get('max_sm_efficiency', 'N/A')}\n")
                    f.write(f"   - æœ€ä½SMæ•ˆç‡: {gu.get('min_sm_efficiency', 'N/A')}\n")
                    f.write(f"   - ä½äº50%æ•°é‡: {gu.get('kernels_below_50_percent', 0)} / {gu.get('total_kernels', 0)}\n")
                # å†…å­˜åˆ†æ
                mem = analysis.get('memory_analysis', {})
                bw = mem.get('bandwidth_stats', {})
                if bw:
                    f.write(f"   - å¹³å‡å¸¦å®½: {bw.get('average_bandwidth', 'N/A')} GB/s\n")
                    f.write(f"   - æœ€é«˜å¸¦å®½: {bw.get('max_bandwidth', 'N/A')} GB/s\n")
                    f.write(f"   - æœ€ä½å¸¦å®½: {bw.get('min_bandwidth', 'N/A')} GB/s\n")
                l2 = mem.get('l2_cache_stats', {})
                if l2:
                    f.write(f"   - å¹³å‡L2å‘½ä¸­ç‡: {l2.get('average_l2_hit_rate', 'N/A')}\n")
                    f.write(f"   - ä½L2å‘½ä¸­ç‡kernelæ•°: {l2.get('kernels_low_l2_hit_rate', 0)}\n")
                l1 = mem.get('l1_cache_stats', {})
                if l1:
                    f.write(f"   - å¹³å‡L1å‘½ä¸­ç‡: {l1.get('average_l1_hit_rate', 'N/A')}\n")
                    f.write(f"   - ä½L1å‘½ä¸­ç‡kernelæ•°: {l1.get('kernels_low_l1_hit_rate', 0)}\n")
                # ç“¶é¢ˆ
                bsum = analysis.get('bottleneck_summary', [])
                if bsum:
                    f.write("   - ä¸»è¦ç“¶é¢ˆ:\n")
                    for b in bsum:
                        f.write(f"     - {b.get('description','')} ({b.get('severity','')})\n")
                f.write("\n")

            # å†™å…¥ç„¦ç‚¹èšåˆï¼ˆè‹¥æœ‰ï¼‰
            if focus_analysis:
                f.write("## ğŸ¯ ç„¦ç‚¹å†…æ ¸èšåˆæŒ‡æ ‡ (å…¨é‡é‡‡é›†æå–)\n\n")
                for kname, fan in focus_analysis.items():
                    f.write(f"### {kname}\n\n")
                    gu = fan.get('gpu_utilization', {})
                    if gu:
                        f.write(f"- å¹³å‡SMæ•ˆç‡: {gu.get('average_sm_efficiency','N/A')}\n")
                        f.write(f"- Occupancy: {gu.get('achieved_occupancy','N/A')}\n")
                    mem = fan.get('memory_analysis', {}).get('bandwidth_stats', {})
                    if mem:
                        f.write(f"- å¹³å‡å¸¦å®½: {mem.get('average_bandwidth','N/A')} GB/s\n")
                        f.write(f"- L2å‘½ä¸­ç‡: {mem.get('l2_hit_rate','N/A')}%\n")
                    bsum = fan.get('bottleneck_summary', [])
                    if bsum:
                        f.write("- ä¸»è¦ç“¶é¢ˆ:\n")
                        for b in bsum:
                            f.write(f"  - {b.get('description','')} ({b.get('severity','')})\n")
                    f.write("\n")
                

        print(f"ğŸ“„ æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return str(report_file)

def create_sglang_analysis_workflow():
    """åˆ›å»ºSGlangä¸“ç”¨çš„åˆ†æå·¥ä½œæµ"""
    DEFAULT_MODEL_DIR = os.getenv('SGLANG_MODEL_PATH') or os.getenv('MODEL_PATH') or '/workspace/models/'

    def run_sglang_integrated_analysis(model_path: Optional[str] = None, 
                                      batch_size: int = 1,
                                      input_len: int = 128, 
                                      output_len: int = 1,
                                      disable_chunked_prefill: bool = True,
                                      gpu_ids: Optional[List[str]] = None,
                                      hardware: Optional[str] = None,
                                      w_bit: int = 16,
                                      a_bit: int = 16,
                                      kv_bit: Optional[int] = None,
                                      use_flashattention: bool = False):
        print(f"[DEBUG] run_sglang_integrated_analysis entered: bs={batch_size}, in={input_len}, out={output_len}")
        """è¿è¡ŒSGlangçš„é›†æˆåˆ†æï¼ˆå›ºå®šè®¾ç½®ï¼šbs=1, in=128, out=1ï¼‰ï¼Œå¯åœ¨å¤šä¸ªGPUä¸Šé¡ºåºæ‰§è¡Œ"""
        if not model_path:
            model_path = DEFAULT_MODEL_DIR.rstrip('/')
            print(f"â„¹ï¸ æœªæä¾› model_pathï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {model_path}")

        gpu_list = gpu_ids or ["0", "1"]
        base_env = os.environ.copy()
        base_env['SGLANG_ENABLE_CHUNKED_PREFILL'] = '0'

        resolved_hardware = hardware or os.getenv('ROOFLINE_HARDWARE') or 'nvidia_H800_SXM5_80G'
        outputs: List[Dict[str, str]] = []
        for gpu_id in gpu_list:
            env = base_env.copy()
            env['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
            print(f"ğŸ” åœ¨ GPU {gpu_id} ä¸Šè¿è¡Œ nsys/ncu æµç¨‹ ...")
            sglang_cmd = [
                'python', '-m', 'sglang.bench_one_batch',
                '--model-path', model_path,
                '--batch-size', str(batch_size),
                '--input-len', str(input_len),
                '--output-len', str(output_len),
                '--load-format', 'dummy',
                '--chunked-prefill-size', '0',
                '--disable-cuda-graph'
            ]

            analyzer = NSysToNCUAnalyzer(
                f"sglang_analysis_b{batch_size}_i{input_len}_o{output_len}_gpu{gpu_id}",
                env=env,
            )
            try:
                roofline = compute_roofline(
                    Path(model_path),
                    resolved_hardware,
                    batch_size,
                    input_len,
                    output_len,
                    w_bit=w_bit,
                    a_bit=a_bit,
                    kv_bit=kv_bit,
                    use_flashattention=use_flashattention,
                )
                analyzer.roofline_estimate = roofline
                try:
                    estimate_path = analyzer.output_dir / "roofline_estimate.json"
                    with open(estimate_path, "w", encoding="utf-8") as rf:
                        json.dump(roofline, rf, ensure_ascii=False, indent=2)
                    print(f"ğŸ“ Roofline é¢„æµ‹å·²å†™å…¥: {estimate_path}")
                except Exception as write_exc:
                    print(f"âš ï¸ Roofline ç»“æœå†™å…¥å¤±è´¥: {write_exc}")
                print("ğŸ“ Roofline é¢„æµ‹å·²å®Œæˆï¼Œç»“æœå°†åœ¨æœ€ç»ˆæŠ¥å‘Šä¸­å±•ç¤ºã€‚")
            except Exception as exc:
                print(f"âš ï¸ Roofline é¢„æµ‹å¤±è´¥: {exc}")
            nsys_file = analyzer.step1_nsys_analysis(sglang_cmd, "sglang_overview")
            hot = analyzer.step2_extract_hot_kernels(nsys_file, top_k=8)
            if not hot:
                print("âŒ æœªå‘ç°çƒ­ç‚¹kernelsï¼Œåˆ†æç»ˆæ­¢");
                continue
            ncu_files = analyzer.step3_ncu_targeted_analysis(sglang_cmd, hot, max_kernels=len(hot))
            results = analyzer.step4_comprehensive_analysis(ncu_files)
            report_file = analyzer.generate_final_report(results)
            outputs.append({
                "gpu": str(gpu_id),
                "dir": str(analyzer.output_dir)
            })
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {analyzer.output_dir}\nğŸ“„ æŠ¥å‘Š: {report_file}")

        return outputs
    return run_sglang_integrated_analysis

# --- è¾…åŠ©å‡½æ•°: å°†é«˜é˜¶æŠ¥å‘Š Markdown ç²—ç•¥ç»“æ„åŒ–ä¸º JSON ---
def _extract_advanced_json(md_text: str) -> Dict[str, Any]:  # type: ignore
    sections: Dict[str, Any] = {}
    current = None
    for line in md_text.splitlines():
        if line.startswith('#'):
            # è·å–æ ‡é¢˜
            title = line.strip('# ').strip()
            current = title
            sections[current] = []
        else:
            if current is not None:
                sections[current].append(line)
    # ç®€å•æŠ½å–ä»»åŠ¡åˆ—è¡¨ä¸åˆ†ç±»
    tasks = []
    for k, v in sections.items():
        if 'ä»»åŠ¡åˆ—è¡¨' in k or 'ç»†ç²’åº¦' in k:
            tasks.extend([ln for ln in v if ln.strip().startswith('- ')])
    summary = sections.get('6. æ€»ç»“ (Summary)', [])
    return {
        'sections': list(sections.keys()),
        'tasks_lines': tasks,
        'summary': '\n'.join(summary[:10]),
        'raw_length': len(md_text)
    }

def main():
    import argparse, os
    parser = argparse.ArgumentParser(description='é›†æˆ nsys å’Œ ncu çš„æ€§èƒ½åˆ†æå·¥å…·')
    # SGlang å‚æ•°
    parser.add_argument('--sglang-model', type=str, default=os.getenv('SGLANG_MODEL_PATH') or os.getenv('MODEL_PATH'),
                        help='SGlangæ¨¡å‹è·¯å¾„')
    parser.add_argument('--sglang-batch', type=int, default=1, help='SGlangæ‰¹æ¬¡å¤§å°')
    parser.add_argument('--sglang-input-len', type=int, default=128, help='SGlangè¾“å…¥é•¿åº¦')
    parser.add_argument('--sglang-output-len', type=int, default=1, help='SGlangè¾“å‡ºé•¿åº¦')
    # å…¼å®¹ï¼šä¿ç•™æœªçŸ¥å‚æ•°ä½†ä¸ä½¿ç”¨
    known_args, unknown_tail = parser.parse_known_args()
    # å¿½ç•¥ unknown_tailï¼Œç»Ÿä¸€èµ°å·¥ä½œæµ
    if unknown_tail:
        print(f"[WARN] å¿½ç•¥åŸå§‹ç›®æ ‡å‘½ä»¤ï¼ˆunknown_tailï¼‰ï¼š{' '.join(unknown_tail)}")

    try:
        run_workflow = create_sglang_analysis_workflow()
        print("[DEBUG] è°ƒç”¨ create_sglang_analysis_workflow()")
        run_workflow(
            model_path=known_args.sglang_model,
            batch_size=known_args.sglang_batch,
            input_len=known_args.sglang_input_len,
            output_len=known_args.sglang_output_len,
            disable_chunked_prefill=True
        )
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­åˆ†æ")
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback; traceback.print_exc()

if __name__ == "__main__":
    main()

