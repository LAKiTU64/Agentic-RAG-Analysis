#!/usr/bin/env python3
"""
NVIDIA Nsight Systems (nsys) è¾“å‡ºæ–‡ä»¶è‡ªåŠ¨åŒ–è§£æå·¥å…·

æ”¯æŒè§£æå¤šç§ nsys è¾“å‡ºæ ¼å¼ï¼š
- SQLite æ•°æ®åº“æ–‡ä»¶ (.sqlite)
- CSV å¯¼å‡ºæ–‡ä»¶
- JSON å¯¼å‡ºæ–‡ä»¶
- è‡ªåŠ¨è°ƒç”¨ nsys å¯¼å‡ºå·¥å…·

ä½œè€…: AIåŠ©æ‰‹
ç‰ˆæœ¬: 1.0
"""

import os
import sys
import sqlite3
import json
import csv
import subprocess
import argparse
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Optional, Union, Tuple
from dataclasses import dataclass
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

@dataclass
class KernelInfo:
    """CUDA Kernel æ‰§è¡Œä¿¡æ¯"""
    name: str
    start_time: float
    duration: float
    grid_size: Optional[Tuple[int, int, int]] = None
    block_size: Optional[Tuple[int, int, int]] = None
    registers_per_thread: Optional[int] = None
    shared_memory: Optional[int] = None

@dataclass
class MemoryTransfer:
    """å†…å­˜ä¼ è¾“ä¿¡æ¯"""
    kind: str  # H2D, D2H, D2D
    size: int
    start_time: float
    duration: float
    bandwidth: Optional[float] = None

@dataclass
class APICall:
    """API è°ƒç”¨ä¿¡æ¯"""
    name: str
    start_time: float
    duration: float
    thread_id: int

class NsysParser:
    """Nsys è¾“å‡ºæ–‡ä»¶è§£æå™¨"""
    
    def __init__(self, input_file: str):
        self.input_file = Path(input_file)
        self.kernels: List[KernelInfo] = []
        self.memory_transfers: List[MemoryTransfer] = []
        self.api_calls: List[APICall] = []
        self.metadata: Dict = {}
        
        if not self.input_file.exists():
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
    
    def parse(self) -> None:
        """è§£æè¾“å…¥æ–‡ä»¶"""
        suffix = self.input_file.suffix.lower()
        
        if suffix == '.nsys-rep':
            self._parse_nsys_rep()
        elif suffix in ['.db', '.sqlite', '.sqlite3']:
            self._parse_sqlite()
        elif suffix == '.csv':
            self._parse_csv()
        elif suffix == '.json':
            self._parse_json()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")
    
    def _parse_nsys_rep(self) -> None:
        """è§£æ .nsys-rep æ–‡ä»¶ï¼ˆéœ€è¦å…ˆå¯¼å‡ºä¸ºSQLiteï¼‰"""
        print("ğŸ“‹ æ£€æµ‹åˆ° .nsys-rep æ–‡ä»¶ï¼Œæ­£åœ¨å¯¼å‡ºä¸ºSQLiteæ ¼å¼...")
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        sqlite_file = self.input_file.with_suffix('.sqlite')
        
        # è°ƒç”¨ nsys å¯¼å‡ºå‘½ä»¤
        cmd = [
            'nsys', 'export', 
            '--type=sqlite',
            '--output', str(sqlite_file),
            str(self.input_file)
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            print(f"âœ… å¯¼å‡ºæˆåŠŸ: {sqlite_file}")
            
            # è§£æå¯¼å‡ºçš„SQLiteæ–‡ä»¶
            self._parse_sqlite(sqlite_file)
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ nsyså¯¼å‡ºå¤±è´¥: {e.stderr}")
            print("è¯·ç¡®ä¿ nsys å·¥å…·å·²æ­£ç¡®å®‰è£…å¹¶åœ¨PATHä¸­")
            raise
        except FileNotFoundError:
            print("âŒ æœªæ‰¾åˆ° nsys å‘½ä»¤")
            print("è¯·å®‰è£… NVIDIA Nsight Systems å¹¶ç¡®ä¿ nsys åœ¨PATHä¸­")
            raise
    
    def _parse_sqlite(self, sqlite_file: Optional[Path] = None) -> None:
        """è§£æ SQLite æ•°æ®åº“æ–‡ä»¶"""
        db_file = sqlite_file or self.input_file
        
        print(f"ğŸ“Š æ­£åœ¨è§£æSQLiteæ–‡ä»¶: {db_file}")
        
        conn = sqlite3.connect(db_file)
        
        try:
            # è·å–è¡¨ä¿¡æ¯
            tables = self._get_table_names(conn)
            print(f"ğŸ” å‘ç°è¡¨: {', '.join(tables)}")
            
            # è§£æCUDA kernels
            if 'CUPTI_ACTIVITY_KIND_KERNEL' in tables:
                self._parse_cuda_kernels(conn)
            
            # è§£æå†…å­˜ä¼ è¾“
            if 'CUPTI_ACTIVITY_KIND_MEMCPY' in tables:
                self._parse_memory_transfers(conn)
            
            # è§£æAPIè°ƒç”¨
            if 'CUPTI_ACTIVITY_KIND_RUNTIME' in tables:
                self._parse_api_calls(conn)
            
            # è·å–å…ƒæ•°æ®
            self._parse_metadata(conn)
            
        finally:
            conn.close()
    
    def _get_table_names(self, conn: sqlite3.Connection) -> List[str]:
        """è·å–æ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨å"""
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        return [row[0] for row in cursor.fetchall()]
    
    def _parse_cuda_kernels(self, conn: sqlite3.Connection) -> None:
        """è§£æCUDA kernelä¿¡æ¯"""
        query = """
        SELECT 
            demangledName,
            start,
            end,
            gridX, gridY, gridZ,
            blockX, blockY, blockZ,
            registersPerThread,
            sharedMemoryExecuted
        FROM CUPTI_ACTIVITY_KIND_KERNEL
        ORDER BY start
        """
        
        cursor = conn.cursor()
        cursor.execute(query)
        
        for row in cursor.fetchall():
            kernel = KernelInfo(
                name=row[0] or "Unknown Kernel",
                start_time=row[1] / 1e9,  # è½¬æ¢ä¸ºç§’
                duration=(row[2] - row[1]) / 1e9,  # è½¬æ¢ä¸ºç§’
                grid_size=(row[3], row[4], row[5]) if row[3] else None,
                block_size=(row[6], row[7], row[8]) if row[6] else None,
                registers_per_thread=row[9],
                shared_memory=row[10]
            )
            self.kernels.append(kernel)
        
        print(f"ğŸ”¥ è§£æåˆ° {len(self.kernels)} ä¸ªCUDA kernels")
    
    def _parse_memory_transfers(self, conn: sqlite3.Connection) -> None:
        """è§£æå†…å­˜ä¼ è¾“ä¿¡æ¯"""
        query = """
        SELECT 
            copyKind,
            bytes,
            start,
            end
        FROM CUPTI_ACTIVITY_KIND_MEMCPY
        ORDER BY start
        """
        
        cursor = conn.cursor()
        cursor.execute(query)
        
        kind_map = {
            1: "H2D",  # Host to Device
            2: "D2H",  # Device to Host
            3: "D2D",  # Device to Device
        }
        
        for row in cursor.fetchall():
            duration_ns = row[3] - row[2]
            duration_s = duration_ns / 1e9
            bandwidth = (row[1] / (1024**3)) / duration_s if duration_s > 0 else 0  # GB/s
            
            transfer = MemoryTransfer(
                kind=kind_map.get(row[0], f"Kind_{row[0]}"),
                size=row[1],
                start_time=row[2] / 1e9,
                duration=duration_s,
                bandwidth=bandwidth
            )
            self.memory_transfers.append(transfer)
        
        print(f"ğŸ’¾ è§£æåˆ° {len(self.memory_transfers)} ä¸ªå†…å­˜ä¼ è¾“")
    
    def _parse_api_calls(self, conn: sqlite3.Connection) -> None:
        """è§£æAPIè°ƒç”¨ä¿¡æ¯"""
        query = """
        SELECT 
            nameId,
            start,
            end,
            threadId
        FROM CUPTI_ACTIVITY_KIND_RUNTIME
        ORDER BY start
        LIMIT 10000  -- é™åˆ¶æ•°é‡é¿å…è¿‡å¤šæ•°æ®
        """
        
        cursor = conn.cursor()
        cursor.execute(query)
        
        for row in cursor.fetchall():
            api_call = APICall(
                name=f"API_{row[0]}",
                start_time=row[1] / 1e9,
                duration=(row[2] - row[1]) / 1e9,
                thread_id=row[3]
            )
            self.api_calls.append(api_call)
        
        print(f"ğŸ”§ è§£æåˆ° {len(self.api_calls)} ä¸ªAPIè°ƒç”¨")
    
    def _parse_metadata(self, conn: sqlite3.Connection) -> None:
        """è§£æå…ƒæ•°æ®ä¿¡æ¯"""
        self.metadata = {
            'total_kernels': len(self.kernels),
            'total_memory_transfers': len(self.memory_transfers),
            'total_api_calls': len(self.api_calls),
            'parse_time': datetime.now().isoformat()
        }
        
        if self.kernels:
            total_time = max(k.start_time + k.duration for k in self.kernels) - min(k.start_time for k in self.kernels)
            self.metadata['total_execution_time'] = total_time
    
    def _parse_csv(self) -> None:
        """è§£æCSVæ–‡ä»¶"""
        print(f"ğŸ“‹ æ­£åœ¨è§£æCSVæ–‡ä»¶: {self.input_file}")
        # CSVè§£æé€»è¾‘ï¼ˆæ ¹æ®å…·ä½“CSVæ ¼å¼å®ç°ï¼‰
        pass
    
    def _parse_json(self) -> None:
        """è§£æJSONæ–‡ä»¶"""
        print(f"ğŸ“‹ æ­£åœ¨è§£æJSONæ–‡ä»¶: {self.input_file}")
        with open(self.input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        # JSONè§£æé€»è¾‘ï¼ˆæ ¹æ®å…·ä½“JSONæ ¼å¼å®ç°ï¼‰
        pass

class NsysAnalyzer:
    """Nsys æ•°æ®åˆ†æå™¨"""
    
    def __init__(self, parser: NsysParser):
        self.parser = parser
        self.stats = {}
    
    def analyze(self) -> Dict:
        """æ‰§è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸ” å¼€å§‹æ€§èƒ½åˆ†æ...")
        
        self.stats = {
            'kernel_analysis': self._analyze_kernels(),
            'memory_analysis': self._analyze_memory(),
            'timeline_analysis': self._analyze_timeline(),
            'bottleneck_analysis': self._analyze_bottlenecks()
        }
        
        return self.stats
    
    def _analyze_kernels(self) -> Dict:
        """åˆ†æCUDA kernelæ€§èƒ½"""
        if not self.parser.kernels:
            return {}
        
        kernels_df = pd.DataFrame([
            {
                'name': k.name,
                'duration': k.duration * 1000,  # è½¬æ¢ä¸ºæ¯«ç§’
                'start_time': k.start_time
            }
            for k in self.parser.kernels
        ])
        
        kernel_stats = kernels_df.groupby('name').agg({
            'duration': ['count', 'mean', 'std', 'min', 'max', 'sum']
        }).round(4)
        
        return {
            'total_kernels': len(self.parser.kernels),
            'unique_kernels': kernels_df['name'].nunique(),
            'total_kernel_time': kernels_df['duration'].sum(),
            'avg_kernel_time': kernels_df['duration'].mean(),
            'top_kernels': kernel_stats.sort_values(('duration', 'sum'), ascending=False).head(10),
            'kernel_distribution': kernels_df.groupby('name').size().sort_values(ascending=False)
        }
    
    def _analyze_memory(self) -> Dict:
        """åˆ†æå†…å­˜ä¼ è¾“"""
        if not self.parser.memory_transfers:
            return {}
        
        memory_df = pd.DataFrame([
            {
                'kind': m.kind,
                'size_mb': m.size / (1024 * 1024),
                'duration': m.duration * 1000,
                'bandwidth': m.bandwidth
            }
            for m in self.parser.memory_transfers
        ])
        
        return {
            'total_transfers': len(self.parser.memory_transfers),
            'total_data_mb': memory_df['size_mb'].sum(),
            'avg_bandwidth': memory_df['bandwidth'].mean(),
            'transfer_breakdown': memory_df.groupby('kind').agg({
                'size_mb': ['count', 'sum', 'mean'],
                'bandwidth': 'mean'
            }).round(4)
        }
    
    def _analyze_timeline(self) -> Dict:
        """åˆ†ææ—¶é—´çº¿"""
        all_events = []
        
        for k in self.parser.kernels:
            all_events.append(('kernel', k.start_time, k.duration))
        
        for m in self.parser.memory_transfers:
            all_events.append(('memory', m.start_time, m.duration))
        
        if not all_events:
            return {}
        
        all_events.sort(key=lambda x: x[1])  # æŒ‰å¼€å§‹æ—¶é—´æ’åº
        
        return {
            'total_events': len(all_events),
            'execution_span': max(e[1] + e[2] for e in all_events) - min(e[1] for e in all_events),
            'first_event_time': min(e[1] for e in all_events),
            'last_event_time': max(e[1] + e[2] for e in all_events)
        }
    
    def _analyze_bottlenecks(self) -> Dict:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        # åˆ†ælongest running kernels
        if self.parser.kernels:
            kernel_times = [(k.name, k.duration) for k in self.parser.kernels]
            kernel_times.sort(key=lambda x: x[1], reverse=True)
            bottlenecks.append({
                'type': 'longest_kernels',
                'data': kernel_times[:5]
            })
        
        # åˆ†æå†…å­˜å¸¦å®½åˆ©ç”¨ç‡
        if self.parser.memory_transfers:
            low_bandwidth = [
                (m.kind, m.bandwidth, m.size / (1024*1024))
                for m in self.parser.memory_transfers 
                if m.bandwidth and m.bandwidth < 100  # < 100 GB/s
            ]
            if low_bandwidth:
                bottlenecks.append({
                    'type': 'low_bandwidth_transfers',
                    'data': low_bandwidth
                })
        
        return {'identified_bottlenecks': bottlenecks}

class NsysVisualizer:
    """Nsys æ•°æ®å¯è§†åŒ–"""
    
    def __init__(self, parser: NsysParser, analyzer: NsysAnalyzer):
        self.parser = parser
        self.analyzer = analyzer
        self.output_dir = Path("nsys_analysis_output")
        self.output_dir.mkdir(exist_ok=True)
    
    def create_visualizations(self) -> None:
        """åˆ›å»ºæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨"""
        print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        if self.parser.kernels:
            self._plot_kernel_timeline()
            self._plot_kernel_duration_distribution()
            self._plot_top_kernels()
        
        if self.parser.memory_transfers:
            self._plot_memory_transfers()
            self._plot_bandwidth_analysis()
        
        print(f"ğŸ“ å›¾è¡¨å·²ä¿å­˜åˆ°: {self.output_dir}")
    
    def _plot_kernel_timeline(self) -> None:
        """ç»˜åˆ¶kernelæ‰§è¡Œæ—¶é—´çº¿"""
        fig, ax = plt.subplots(figsize=(15, 8))
        
        for i, kernel in enumerate(self.parser.kernels[:50]):  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
            ax.barh(i, kernel.duration * 1000, left=kernel.start_time * 1000, height=0.8)
        
        ax.set_xlabel('æ—¶é—´ (æ¯«ç§’)')
        ax.set_ylabel('Kernel ç´¢å¼•')
        ax.set_title('CUDA Kernel æ‰§è¡Œæ—¶é—´çº¿')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'kernel_timeline.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_kernel_duration_distribution(self) -> None:
        """ç»˜åˆ¶kernelæ‰§è¡Œæ—¶é—´åˆ†å¸ƒ"""
        durations = [k.duration * 1000 for k in self.parser.kernels]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # ç›´æ–¹å›¾
        ax1.hist(durations, bins=50, alpha=0.7, color='skyblue')
        ax1.set_xlabel('æ‰§è¡Œæ—¶é—´ (æ¯«ç§’)')
        ax1.set_ylabel('é¢‘æ¬¡')
        ax1.set_title('Kernel æ‰§è¡Œæ—¶é—´åˆ†å¸ƒ')
        ax1.grid(True, alpha=0.3)
        
        # ç®±çº¿å›¾
        ax2.boxplot(durations)
        ax2.set_ylabel('æ‰§è¡Œæ—¶é—´ (æ¯«ç§’)')
        ax2.set_title('Kernel æ‰§è¡Œæ—¶é—´ç®±çº¿å›¾')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'kernel_duration_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_top_kernels(self) -> None:
        """ç»˜åˆ¶è€—æ—¶æœ€é•¿çš„kernels"""
        kernel_stats = {}
        for kernel in self.parser.kernels:
            if kernel.name not in kernel_stats:
                kernel_stats[kernel.name] = {'count': 0, 'total_time': 0}
            kernel_stats[kernel.name]['count'] += 1
            kernel_stats[kernel.name]['total_time'] += kernel.duration * 1000
        
        # æŒ‰æ€»æ‰§è¡Œæ—¶é—´æ’åº
        sorted_kernels = sorted(kernel_stats.items(), 
                              key=lambda x: x[1]['total_time'], reverse=True)[:10]
        
        names = [item[0][:30] + '...' if len(item[0]) > 30 else item[0] for item, _ in sorted_kernels]
        times = [stats['total_time'] for _, stats in sorted_kernels]
        
        fig, ax = plt.subplots(figsize=(12, 8))
        bars = ax.barh(names, times, color='lightcoral')
        
        ax.set_xlabel('æ€»æ‰§è¡Œæ—¶é—´ (æ¯«ç§’)')
        ax.set_title('è€—æ—¶æœ€é•¿çš„ Top 10 Kernels')
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            width = bar.get_width()
            ax.text(width, bar.get_y() + bar.get_height()/2, 
                   f'{width:.2f}ms', ha='left', va='center')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'top_kernels.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_memory_transfers(self) -> None:
        """ç»˜åˆ¶å†…å­˜ä¼ è¾“åˆ†æ"""
        if not self.parser.memory_transfers:
            return
        
        transfer_data = {}
        for transfer in self.parser.memory_transfers:
            if transfer.kind not in transfer_data:
                transfer_data[transfer.kind] = {'count': 0, 'total_size': 0, 'total_time': 0}
            transfer_data[transfer.kind]['count'] += 1
            transfer_data[transfer.kind]['total_size'] += transfer.size / (1024 * 1024)  # MB
            transfer_data[transfer.kind]['total_time'] += transfer.duration * 1000  # ms
        
        kinds = list(transfer_data.keys())
        sizes = [transfer_data[kind]['total_size'] for kind in kinds]
        times = [transfer_data[kind]['total_time'] for kind in kinds]
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # ä¼ è¾“æ•°æ®é‡
        ax1.pie(sizes, labels=kinds, autopct='%1.1f%%', startangle=90)
        ax1.set_title('å†…å­˜ä¼ è¾“æ•°æ®é‡åˆ†å¸ƒ (MB)')
        
        # ä¼ è¾“æ—¶é—´
        ax2.bar(kinds, times, color=['#ff9999', '#66b3ff', '#99ff99'])
        ax2.set_ylabel('æ€»ä¼ è¾“æ—¶é—´ (æ¯«ç§’)')
        ax2.set_title('å†…å­˜ä¼ è¾“æ—¶é—´ç»Ÿè®¡')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'memory_transfers.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_bandwidth_analysis(self) -> None:
        """ç»˜åˆ¶å¸¦å®½åˆ†æ"""
        if not self.parser.memory_transfers:
            return
        
        bandwidths = [m.bandwidth for m in self.parser.memory_transfers if m.bandwidth]
        if not bandwidths:
            return
        
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.hist(bandwidths, bins=30, alpha=0.7, color='lightgreen')
        ax.set_xlabel('å¸¦å®½ (GB/s)')
        ax.set_ylabel('é¢‘æ¬¡')
        ax.set_title('å†…å­˜ä¼ è¾“å¸¦å®½åˆ†å¸ƒ')
        ax.axvline(x=sum(bandwidths)/len(bandwidths), color='red', linestyle='--', 
                  label=f'å¹³å‡å¸¦å®½: {sum(bandwidths)/len(bandwidths):.2f} GB/s')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'bandwidth_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()

class NsysReporter:
    """Nsys åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, parser: NsysParser, analyzer: NsysAnalyzer):
        self.parser = parser
        self.analyzer = analyzer
        self.output_dir = Path("nsys_analysis_output")
        self.output_dir.mkdir(exist_ok=True)
    
    def generate_report(self) -> None:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("ğŸ“„ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        report_path = self.output_dir / "analysis_report.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(self._generate_header())
            f.write(self._generate_summary())
            f.write(self._generate_kernel_analysis())
            f.write(self._generate_memory_analysis())
            f.write(self._generate_bottleneck_analysis())
            f.write(self._generate_recommendations())
        
        print(f"ğŸ“‹ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        
        # åŒæ—¶ç”ŸæˆJSONæ ¼å¼çš„è¯¦ç»†æ•°æ®
        json_path = self.output_dir / "analysis_data.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.analyzer.stats, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“Š è¯¦ç»†æ•°æ®å·²ä¿å­˜: {json_path}")
    
    def _generate_header(self) -> str:
        """ç”ŸæˆæŠ¥å‘Šå¤´éƒ¨"""
        return f"""
{'='*80}
NVIDIA Nsight Systems æ€§èƒ½åˆ†ææŠ¥å‘Š
{'='*80}
åˆ†ææ–‡ä»¶: {self.parser.input_file}
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
{'='*80}

"""
    
    def _generate_summary(self) -> str:
        """ç”Ÿæˆæ‘˜è¦"""
        return f"""
ğŸ“Š æ€§èƒ½æ‘˜è¦
{'-'*40}
â€¢ æ€» CUDA Kernels: {len(self.parser.kernels)}
â€¢ æ€»å†…å­˜ä¼ è¾“: {len(self.parser.memory_transfers)}
â€¢ æ€» API è°ƒç”¨: {len(self.parser.api_calls)}

"""
    
    def _generate_kernel_analysis(self) -> str:
        """ç”Ÿæˆkernelåˆ†æ"""
        if not self.parser.kernels:
            return "ğŸ”¥ CUDA Kernel åˆ†æ\n" + "-"*40 + "\næ—  kernel æ•°æ®\n\n"
        
        stats = self.analyzer.stats.get('kernel_analysis', {})
        
        return f"""
ğŸ”¥ CUDA Kernel åˆ†æ
{'-'*40}
â€¢ æ€»æ‰§è¡Œæ—¶é—´: {stats.get('total_kernel_time', 0):.2f} ms
â€¢ å¹³å‡kernelæ—¶é—´: {stats.get('avg_kernel_time', 0):.2f} ms
â€¢ å”¯ä¸€kernelæ•°é‡: {stats.get('unique_kernels', 0)}

è€—æ—¶æœ€é•¿çš„ Kernels:
"""
    
    def _generate_memory_analysis(self) -> str:
        """ç”Ÿæˆå†…å­˜åˆ†æ"""
        if not self.parser.memory_transfers:
            return "ğŸ’¾ å†…å­˜ä¼ è¾“åˆ†æ\n" + "-"*40 + "\næ— å†…å­˜ä¼ è¾“æ•°æ®\n\n"
        
        stats = self.analyzer.stats.get('memory_analysis', {})
        
        return f"""
ğŸ’¾ å†…å­˜ä¼ è¾“åˆ†æ
{'-'*40}
â€¢ æ€»æ•°æ®ä¼ è¾“: {stats.get('total_data_mb', 0):.2f} MB
â€¢ å¹³å‡å¸¦å®½: {stats.get('avg_bandwidth', 0):.2f} GB/s
â€¢ ä¼ è¾“æ¬¡æ•°: {stats.get('total_transfers', 0)}

"""
    
    def _generate_bottleneck_analysis(self) -> str:
        """ç”Ÿæˆç“¶é¢ˆåˆ†æ"""
        bottlenecks = self.analyzer.stats.get('bottleneck_analysis', {}).get('identified_bottlenecks', [])
        
        if not bottlenecks:
            return "ğŸš« æ€§èƒ½ç“¶é¢ˆåˆ†æ\n" + "-"*40 + "\næœªå‘ç°æ˜æ˜¾ç“¶é¢ˆ\n\n"
        
        result = f"""
ğŸš« æ€§èƒ½ç“¶é¢ˆåˆ†æ
{'-'*40}
"""
        
        for bottleneck in bottlenecks:
            if bottleneck['type'] == 'longest_kernels':
                result += "â€¢ è€—æ—¶æœ€é•¿çš„ kernels å¯èƒ½æ˜¯ç“¶é¢ˆ\n"
            elif bottleneck['type'] == 'low_bandwidth_transfers':
                result += "â€¢ æ£€æµ‹åˆ°ä½å¸¦å®½å†…å­˜ä¼ è¾“\n"
        
        return result + "\n"
    
    def _generate_recommendations(self) -> str:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        return f"""
ğŸ’¡ ä¼˜åŒ–å»ºè®®
{'-'*40}
â€¢ åˆ†æè€—æ—¶æœ€é•¿çš„ kernelsï¼Œè€ƒè™‘ç®—æ³•ä¼˜åŒ–
â€¢ æ£€æŸ¥å†…å­˜ä¼ è¾“æ•ˆç‡ï¼Œå‡å°‘ä¸å¿…è¦çš„ä¼ è¾“
â€¢ è€ƒè™‘ä½¿ç”¨å¼‚æ­¥ä¼ è¾“å’Œè®¡ç®—é‡å 
â€¢ ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼ä»¥æé«˜å¸¦å®½åˆ©ç”¨ç‡

{'='*80}
"""

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='NVIDIA Nsight Systems è¾“å‡ºæ–‡ä»¶è‡ªåŠ¨åŒ–è§£æå·¥å…·')
    parser.add_argument('input_file', help='è¾“å…¥æ–‡ä»¶è·¯å¾„ (.nsys-rep, .sqlite, .csv, .json)')
    parser.add_argument('--no-viz', action='store_true', help='ä¸ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨')
    parser.add_argument('--no-report', action='store_true', help='ä¸ç”Ÿæˆåˆ†ææŠ¥å‘Š')
    parser.add_argument('--output-dir', default='nsys_analysis_output', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    try:
        # è§£ææ–‡ä»¶
        print(f"ğŸš€ å¼€å§‹è§£ææ–‡ä»¶: {args.input_file}")
        nsys_parser = NsysParser(args.input_file)
        nsys_parser.parse()
        
        # åˆ†ææ•°æ®
        analyzer = NsysAnalyzer(nsys_parser)
        analyzer.analyze()
        
        # ç”Ÿæˆå¯è§†åŒ–
        if not args.no_viz:
            visualizer = NsysVisualizer(nsys_parser, analyzer)
            visualizer.output_dir = Path(args.output_dir)
            visualizer.create_visualizations()
        
        # ç”ŸæˆæŠ¥å‘Š
        if not args.no_report:
            reporter = NsysReporter(nsys_parser, analyzer)
            reporter.output_dir = Path(args.output_dir)
            reporter.generate_report()
        
        print(f"\nâœ… åˆ†æå®Œæˆ! ç»“æœä¿å­˜åœ¨: {args.output_dir}")
        print(f"ğŸ“Š è§£æäº† {len(nsys_parser.kernels)} ä¸ªkernels, {len(nsys_parser.memory_transfers)} ä¸ªå†…å­˜ä¼ è¾“")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()


