"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description","Estimated Speedup Type","Estimated Speedup"

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.58",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","12,721",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.35",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.09",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","8.03",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","6.61",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","11.76",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8,195.42",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","26.31",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full waves across all SMs. Look at Launch Statistics for more details.","",""

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","33.55",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.53",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.02",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","39.42",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.58",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","39.42",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (31.0%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","136.64",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","8.35",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.90",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","36.55",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","66.54",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","16.05",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 24.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 68.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.022"

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","39.18",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.39",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","60.82",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.14",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.91",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.14 active warps per scheduler, but only an average of 0.91 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","60.82"

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","18.22",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","18.78",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","32",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.47",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 31.2% of the total average of 18.2 cycles between issuing two instructions.","global","31.21"

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3,134.06",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1,654,784",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3,230.69",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1,705,803",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1,024",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","36",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","32.77",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","131,072",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.65",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","12",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","32",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","48",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","75",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","45.03",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","28.82",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (75.0%) and measured achieved occupancy (45.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","39.95"

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","TheoreticalOccupancy","OPT","The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required registers.","global","25"

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","857.40",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","839,168",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8,195.42",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1,620,890",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7,843.14",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1,087,840",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8,195.42",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1,620,890",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8,246.14",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","6,483,560",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 7.54% above the average, while the minimum instance value is 7.04% below the average.","global","5.066"

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 11.47% above the average, while the minimum instance value is 9.30% below the average.","global","6.614"

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","180,224",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","100",

"0","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.58",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","12,071",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","9.15",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.31",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.62",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","6.39",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","14.00",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8,479.90",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","26.41",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full waves across all SMs. Look at Launch Statistics for more details.","",""

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","4.19",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.48",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.02",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","38.11",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.52",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","38.11",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (30.0%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","144.07",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","9.15",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","7.05",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","36.37",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","67.77",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","16.09",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 24.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 68.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.407"

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","39.23",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.39",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","60.77",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.25",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.92",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.25 active warps per scheduler, but only an average of 0.92 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","60.77"

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","18.48",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","19.06",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","32",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.47",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 5.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 30.5% of the total average of 18.5 cycles between issuing two instructions.","global","30.53"

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3,134.06",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1,654,784",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3,231.36",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1,706,157",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1,024",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","36",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","32.77",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","131,072",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.65",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","12",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","32",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","48",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","75",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","45.21",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","28.93",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (75.0%) and measured achieved occupancy (45.2%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","39.72"

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","TheoreticalOccupancy","OPT","The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required registers.","global","25"

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","857.20",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","794,880",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8,479.90",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1,615,242",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7,643.75",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1,032,080",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8,479.90",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1,615,242",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8,237.88",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","6,460,968",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 13.94% above the average, while the minimum instance value is 11.13% below the average.","global","8.26"

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","180,224",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","100",

"1","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.61",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.58",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","12,625",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.29",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.13",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.97",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","6.49",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","11.72",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8,370.61",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","26.36",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full waves across all SMs. Look at Launch Statistics for more details.","",""

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","33.55",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.50",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.02",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","38.60",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.54",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","38.60",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (30.4%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","137.73",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","8.29",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","6.87",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","36.36",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","66.95",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","16.07",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 24.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 68.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.014"

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","38.66",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.39",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","61.34",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.35",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.90",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.35 active warps per scheduler, but only an average of 0.90 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","61.34"

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","19.02",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","19.60",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","32",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.47",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3,134.06",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1,654,784",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3,230.93",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1,705,932",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1,024",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","36",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","32.77",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","131,072",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.65",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","12",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","32",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","48",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","75",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","44.99",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","28.79",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (75.0%) and measured achieved occupancy (45.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","40.01"

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","TheoreticalOccupancy","OPT","The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required registers.","global","25"

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","857.40",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","830,592",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8,370.61",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1,618,052",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7,925.82",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1,080,240",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8,370.61",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1,618,052",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8,357.45",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","6,472,208",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 11.82% above the average, while the minimum instance value is 10.57% below the average.","global","6.937"

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","180,224",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","100",

"2","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.62",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.59",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","12,147",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","9.05",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.28",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","7.65",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","6.40",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","13.83",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","8,463.04",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","26.22",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full waves across all SMs. Look at Launch Statistics for more details.","",""

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","33.55",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.48",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.02",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","38.18",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.53",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","38.18",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","ALU is the highest-utilized pipeline (30.1%) based on active cycles, taking into account the rates of its different instructions. It executes integer and logic operations. It is well-utilized, but should not be a bottleneck.","",""

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","143.46",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","9.05",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","7.12",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","36.47",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","67.34",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","15.98",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads from L2 might not be optimal. On average, only 24.0 of the 32 bytes transmitted per sector are utilized by each thread. This applies to the 68.8% of sectors missed in L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced global loads.","global","2.377"

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","39.24",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.39",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","60.76",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.26",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.93",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.26 active warps per scheduler, but only an average of 0.93 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","60.76"

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","18.51",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","19.09",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","32",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.47",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","3,134.06",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1,654,784",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","3,231.19",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1,706,067",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Block Size","","128",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Grid Size","","1,024",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","36",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","32.77",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Threads","thread","131,072",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.65",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","12",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","32",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","16",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","48",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","75",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","45.52",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","29.13",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (75.0%) and measured achieved occupancy (45.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","39.31"

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Occupancy","","","","TheoreticalOccupancy","OPT","The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required registers.","global","25"

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","857.20",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","801,024",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","8,463.04",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1,626,606",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7,856.46",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1,040,240",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","8,463.04",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1,626,606",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","8,234.65",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","6,506,424",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much higher number of active cycles than the average number of active cycles. Additionally, other SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 7.98% above the average, while the minimum instance value is 6.86% below the average.","global","5.334"

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Additionally, other L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 12.62% above the average, while the minimum instance value is 11.02% below the average.","global","7.624"

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.11",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","180,224",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","100",

"3","916414","python3.12","127.0.0.1","void index_elementwise_kernel<128, 4, void gpu_index_kernel<void index_put_kernel_impl<OpaqueType<2>>(TensorIterator &, ArrayRef<long>, ArrayRef<long>)::[lambda(char *, const char *, long) (instance 1)]>(TensorIteratorBase &, ArrayRef<long>, ArrayRef<long>, const T1 &, bool)::[lambda(int) (instance 1)]>(long, T3)","1","7","(128, 1, 1)","(1024, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0",
