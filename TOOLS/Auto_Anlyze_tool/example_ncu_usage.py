#!/usr/bin/env python3
"""
NCU Parser ä½¿ç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ ncu_parser.py åˆ†æž NVIDIA Nsight Compute è¾“å‡ºæ–‡ä»¶
"""

import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from ncu_parser import NCUParser, NCUAnalyzer, NCUVisualizer, NCUReporter

def example_basic_usage():
    """åŸºç¡€ä½¿ç”¨ç¤ºä¾‹"""
    print("="*60)
    print("NCU Parser åŸºç¡€ä½¿ç”¨ç¤ºä¾‹")
    print("="*60)
    
    # ç¤ºä¾‹è¾“å…¥æ–‡ä»¶è·¯å¾„ (è¯·æ ¹æ®å®žé™…æƒ…å†µä¿®æ”¹)
    input_files = [
        "sample_profile.ncu-rep",  # NCU report æ–‡ä»¶
        "kernel_metrics.csv",     # CSV å¯¼å‡ºæ–‡ä»¶
        "profile_data.json"       # JSON å¯¼å‡ºæ–‡ä»¶
    ]
    
    for input_file in input_files:
        if Path(input_file).exists():
            print(f"\nðŸ“ å¤„ç†æ–‡ä»¶: {input_file}")
            
            try:
                # 1. è§£æžNCUæ–‡ä»¶
                parser = NCUParser(input_file)
                parser.parse()
                
                # 2. åˆ†æžæ€§èƒ½æ•°æ®
                analyzer = NCUAnalyzer(parser)
                stats = analyzer.analyze()
                
                # 3. ç”Ÿæˆå¯è§†åŒ–
                visualizer = NCUVisualizer(parser, analyzer)
                visualizer.create_visualizations()
                
                # 4. ç”Ÿæˆåˆ†æžæŠ¥å‘Š
                reporter = NCUReporter(parser, analyzer)
                reporter.generate_report()
                
                print(f"âœ… åˆ†æžå®Œæˆ! è§£æžäº† {len(parser.kernels)} ä¸ªkernels")
                
            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        else:
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")

def example_advanced_analysis():
    """é«˜çº§åˆ†æžç¤ºä¾‹"""
    print("\n" + "="*60)
    print("NCU Parser é«˜çº§åˆ†æžç¤ºä¾‹")
    print("="*60)
    
    # å‡è®¾æœ‰ä¸€ä¸ªç¤ºä¾‹æ–‡ä»¶
    input_file = "advanced_profile.ncu-rep"
    
    if not Path(input_file).exists():
        print(f"âš ï¸  ç¤ºä¾‹æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        print("åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º...")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        create_sample_data()
        input_file = "sample_data.json"
    
    try:
        # è§£æžå’Œåˆ†æž
        parser = NCUParser(input_file)
        parser.parse()
        
        analyzer = NCUAnalyzer(parser)
        stats = analyzer.analyze()
        
        # è¯¦ç»†åˆ†æžç»“æžœ
        print(f"\nðŸ“Š åˆ†æžç»“æžœæ‘˜è¦:")
        print(f"â€¢ æ€»kernelæ•°: {len(parser.kernels)}")
        
        # GPUåˆ©ç”¨çŽ‡åˆ†æž
        gpu_stats = stats.get('gpu_utilization', {})
        if 'average_sm_efficiency' in gpu_stats:
            print(f"â€¢ å¹³å‡SMæ•ˆçŽ‡: {gpu_stats['average_sm_efficiency']:.1f}%")
            print(f"â€¢ ä½Žæ•ˆçŽ‡kernelæ•°: {gpu_stats.get('kernels_below_50_percent', 0)}")
        
        # å†…å­˜æ€§èƒ½åˆ†æž
        memory_stats = stats.get('memory_analysis', {})
        if 'bandwidth_stats' in memory_stats:
            bandwidth = memory_stats['bandwidth_stats']
            print(f"â€¢ å¹³å‡DRAMå¸¦å®½: {bandwidth.get('average_bandwidth', 0):.1f} GB/s")
        
        # ç“¶é¢ˆåˆ†æž
        bottleneck_stats = stats.get('bottleneck_analysis', {})
        print(f"â€¢ è¯†åˆ«ç“¶é¢ˆæ•°: {bottleneck_stats.get('total_bottlenecks', 0)}")
        
        if 'top_issues' in bottleneck_stats:
            print(f"\nðŸš« ä¸»è¦æ€§èƒ½é—®é¢˜:")
            for i, issue in enumerate(bottleneck_stats['top_issues'][:3], 1):
                print(f"  {i}. {issue['description']} ({issue['severity']})")
        
        # ç”ŸæˆæŠ¥å‘Šå’Œå¯è§†åŒ–
        visualizer = NCUVisualizer(parser, analyzer)
        visualizer.create_visualizations()
        
        reporter = NCUReporter(parser, analyzer)
        reporter.generate_report()
        
        print(f"\nâœ… é«˜çº§åˆ†æžå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ é«˜çº§åˆ†æžå¤±è´¥: {e}")

def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºŽæ¼”ç¤º"""
    import json
    
    sample_data = [
        {
            "name": "sample_kernel_1",
            "smEfficiency": 85.2,
            "achievedOccupancy": 75.6,
            "theoreticalOccupancy": 87.3,
            "dramBandwidth": 650.4,
            "l2HitRate": 82.1,
            "l1HitRate": 78.5,
            "tensorActive": 12.3,
            "warpExecutionEfficiency": 91.7,
            "duration": 2.34,
            "registersPerThread": 32
        },
        {
            "name": "sample_kernel_2", 
            "smEfficiency": 45.1,
            "achievedOccupancy": 34.2,
            "theoreticalOccupancy": 62.8,
            "dramBandwidth": 285.7,
            "l2HitRate": 45.3,
            "l1HitRate": 67.2,
            "tensorActive": 0.0,
            "warpExecutionEfficiency": 67.4,
            "duration": 5.67,
            "registersPerThread": 48
        },
        {
            "name": "sample_kernel_3",
            "smEfficiency": 72.8,
            "achievedOccupancy": 68.4,
            "theoreticalOccupancy": 75.2,
            "dramBandwidth": 478.3,
            "l2HitRate": 88.7,
            "l1HitRate": 92.1,
            "tensorActive": 85.6,
            "warpExecutionEfficiency": 89.3,
            "duration": 1.23,
            "registersPerThread": 24
        }
    ]
    
    with open("sample_data.json", "w", encoding='utf-8') as f:
        json.dump(sample_data, f, indent=2)
    
    print("âœ… åˆ›å»ºç¤ºä¾‹æ•°æ®: sample_data.json")

def example_custom_analysis():
    """è‡ªå®šä¹‰åˆ†æžç¤ºä¾‹"""
    print("\n" + "="*60)
    print("NCU Parser è‡ªå®šä¹‰åˆ†æžç¤ºä¾‹")
    print("="*60)
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    create_sample_data()
    
    try:
        parser = NCUParser("sample_data.json")
        parser.parse()
        
        print(f"\nðŸ” è‡ªå®šä¹‰åˆ†æž:")
        
        # è‡ªå®šä¹‰æŒ‡æ ‡åˆ†æž
        high_efficiency_kernels = []
        low_efficiency_kernels = []
        tensor_core_kernels = []
        
        for kernel in parser.kernels:
            # åˆ†ç±»é«˜æ•ˆå’Œä½Žæ•ˆkernel
            if kernel.sm_efficiency and kernel.sm_efficiency > 70:
                high_efficiency_kernels.append(kernel)
            elif kernel.sm_efficiency and kernel.sm_efficiency < 50:
                low_efficiency_kernels.append(kernel)
            
            # ä½¿ç”¨Tensor Coreçš„kernel
            if kernel.tensor_active and kernel.tensor_active > 10:
                tensor_core_kernels.append(kernel)
        
        print(f"â€¢ é«˜æ•ˆçŽ‡kernels (>70%): {len(high_efficiency_kernels)}")
        for k in high_efficiency_kernels:
            print(f"  - {k.name}: SMæ•ˆçŽ‡ {k.sm_efficiency:.1f}%")
        
        print(f"â€¢ ä½Žæ•ˆçŽ‡kernels (<50%): {len(low_efficiency_kernels)}")
        for k in low_efficiency_kernels:
            print(f"  - {k.name}: SMæ•ˆçŽ‡ {k.sm_efficiency:.1f}%")
        
        print(f"â€¢ ä½¿ç”¨Tensor Coreçš„kernels: {len(tensor_core_kernels)}")
        for k in tensor_core_kernels:
            print(f"  - {k.name}: Tensoræ´»è·ƒåº¦ {k.tensor_active:.1f}%")
        
        # è‡ªå®šä¹‰ä¼˜åŒ–å»ºè®®
        print(f"\nðŸ’¡ è‡ªå®šä¹‰ä¼˜åŒ–å»ºè®®:")
        
        if low_efficiency_kernels:
            print("â€¢ å¯¹äºŽä½Žæ•ˆçŽ‡kernels:")
            print("  - æ£€æŸ¥ç®—æ³•å¤æ‚åº¦å’Œå·¥ä½œè´Ÿè½½åˆ†å¸ƒ")
            print("  - è€ƒè™‘å¢žåŠ æ¯ä¸ªçº¿ç¨‹çš„è®¡ç®—é‡")
            print("  - æ£€æŸ¥æ˜¯å¦å­˜åœ¨åˆ†æ”¯åˆ†æ­§")
        
        if not tensor_core_kernels:
            print("â€¢ æœªæ£€æµ‹åˆ°Tensor Coreä½¿ç”¨:")
            print("  - è€ƒè™‘å°†é€‚åˆçš„æ“ä½œè¿ç§»åˆ°Tensor Core")
            print("  - ä½¿ç”¨åŠç²¾åº¦æˆ–æ··åˆç²¾åº¦è®¡ç®—")
        
        # å†…å­˜ä¼˜åŒ–å»ºè®®
        low_bandwidth_kernels = [k for k in parser.kernels 
                               if k.dram_bandwidth and k.dram_bandwidth < 400]
        if low_bandwidth_kernels:
            print("â€¢ å¯¹äºŽä½Žå¸¦å®½åˆ©ç”¨çŽ‡kernels:")
            print("  - ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼")
            print("  - è€ƒè™‘ä½¿ç”¨å…±äº«å†…å­˜")
            print("  - æ£€æŸ¥å†…å­˜åˆå¹¶è®¿é—®")
        
    except Exception as e:
        print(f"âŒ è‡ªå®šä¹‰åˆ†æžå¤±è´¥: {e}")

def cleanup_sample_files():
    """æ¸…ç†ç¤ºä¾‹æ–‡ä»¶"""
    sample_files = ["sample_data.json"]
    
    for file in sample_files:
        if Path(file).exists():
            Path(file).unlink()
            print(f"ðŸ—‘ï¸  æ¸…ç†æ–‡ä»¶: {file}")

def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ðŸš€ NCU Parser ä½¿ç”¨ç¤ºä¾‹")
    print("è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨NCUåˆ†æžå·¥å…·")
    
    try:
        # åŸºç¡€ä½¿ç”¨ç¤ºä¾‹
        example_basic_usage()
        
        # é«˜çº§åˆ†æžç¤ºä¾‹  
        example_advanced_analysis()
        
        # è‡ªå®šä¹‰åˆ†æžç¤ºä¾‹
        example_custom_analysis()
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
    finally:
        # æ¸…ç†ç¤ºä¾‹æ–‡ä»¶
        cleanup_sample_files()
        print(f"\nâœ… ç¤ºä¾‹æ‰§è¡Œå®Œæˆ!")

if __name__ == "__main__":
    main()

