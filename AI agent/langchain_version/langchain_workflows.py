#!/usr/bin/env python3
"""
LangChainå·¥ä½œæµç¨‹é“¾

å®šä¹‰å¤æ‚çš„LLMæ€§èƒ½åˆ†æå·¥ä½œæµç¨‹ï¼Œä½¿ç”¨LangChainçš„é“¾å¼è°ƒç”¨
"""

from typing import Dict, List, Any, Optional
import json
from datetime import datetime

# LangChain imports
from langchain.chains.base import Chain
from langchain.chains import LLMChain, SequentialChain
from langchain.prompts import PromptTemplate
from langchain.schema import BaseOutputParser
from langchain.callbacks.manager import CallbackManagerForChainRun
from pydantic import BaseModel, Field

class AnalysisWorkflowInput(BaseModel):
    """åˆ†æå·¥ä½œæµè¾“å…¥"""
    user_request: str = Field(description="ç”¨æˆ·è¯·æ±‚")
    config_data: Optional[Dict] = Field(default=None, description="é…ç½®æ•°æ®")
    context: Optional[Dict] = Field(default=None, description="ä¸Šä¸‹æ–‡ä¿¡æ¯")

class AnalysisWorkflowOutput(BaseModel):
    """åˆ†æå·¥ä½œæµè¾“å‡º"""
    analysis_plan: Dict = Field(description="åˆ†æè®¡åˆ’")
    execution_steps: List[Dict] = Field(description="æ‰§è¡Œæ­¥éª¤")
    recommendations: List[str] = Field(description="å»ºè®®")
    estimated_time: str = Field(description="é¢„ä¼°æ—¶é—´")

class ModelAnalysisChain(Chain):
    """æ¨¡å‹åˆ†æé“¾ - è´Ÿè´£åˆ†ææ¨¡å‹ç‰¹æ€§å’Œéœ€æ±‚"""
    
    input_key: str = "user_request"
    output_key: str = "model_analysis"
    
    def _call(
        self,
        inputs: Dict[str, Any],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> Dict[str, Any]:
        """æ‰§è¡Œæ¨¡å‹åˆ†æ"""
        
        user_request = inputs[self.input_key]
        
        # åˆ†æç”¨æˆ·è¯·æ±‚ä¸­çš„æ¨¡å‹ä¿¡æ¯
        analysis = self._analyze_model_requirements(user_request)
        
        return {self.output_key: analysis}
    
    def _analyze_model_requirements(self, request: str) -> Dict:
        """åˆ†ææ¨¡å‹éœ€æ±‚"""
        
        # æå–æ¨¡å‹ä¿¡æ¯
        model_info = {
            "detected_model": None,
            "model_size": None,
            "analysis_type": "auto",
            "priority_metrics": []
        }
        
        request_lower = request.lower()
        
        # æ£€æµ‹æ¨¡å‹åç§°
        model_patterns = {
            "llama-7b": {"size": "7B", "type": "decoder", "memory": "~13GB"},
            "llama-13b": {"size": "13B", "type": "decoder", "memory": "~26GB"},
            "qwen-7b": {"size": "7B", "type": "decoder", "memory": "~13GB"},
            "qwen-14b": {"size": "14B", "type": "decoder", "memory": "~28GB"},
            "chatglm-6b": {"size": "6B", "type": "encoder-decoder", "memory": "~12GB"},
            "baichuan-7b": {"size": "7B", "type": "decoder", "memory": "~13GB"},
            "baichuan-13b": {"size": "13B", "type": "decoder", "memory": "~26GB"}
        }
        
        for model_name, info in model_patterns.items():
            if model_name.replace("-", "") in request_lower.replace("-", ""):
                model_info["detected_model"] = model_name
                model_info["model_size"] = info["size"]
                model_info["architecture"] = info["type"]
                model_info["estimated_memory"] = info["memory"]
                break
        
        # æ£€æµ‹åˆ†æç±»å‹
        if "nsys" in request_lower:
            model_info["analysis_type"] = "nsys"
            model_info["priority_metrics"] = ["timeline", "kernel_distribution", "memory_transfers"]
        elif "ncu" in request_lower:
            model_info["analysis_type"] = "ncu"
            model_info["priority_metrics"] = ["sm_efficiency", "occupancy", "memory_bandwidth"]
        elif "æ·±åº¦" in request_lower or "kernel" in request_lower:
            model_info["analysis_type"] = "ncu"
            model_info["priority_metrics"] = ["kernel_analysis", "bottleneck_detection"]
        elif "å…¨å±€" in request_lower or "timeline" in request_lower:
            model_info["analysis_type"] = "nsys"
            model_info["priority_metrics"] = ["global_timeline", "api_analysis"]
        else:
            model_info["analysis_type"] = "auto"
            model_info["priority_metrics"] = ["comprehensive_analysis"]
        
        return model_info
    
    @property
    def _chain_type(self) -> str:
        return "model_analysis"

class ConfigOptimizationChain(Chain):
    """é…ç½®ä¼˜åŒ–é“¾ - åŸºäºæ¨¡å‹åˆ†æä¼˜åŒ–é…ç½®"""
    
    input_key: str = "model_analysis"
    output_key: str = "optimized_config"
    
    def _call(
        self,
        inputs: Dict[str, Any],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> Dict[str, Any]:
        """æ‰§è¡Œé…ç½®ä¼˜åŒ–"""
        
        model_analysis = inputs[self.input_key]
        user_config = inputs.get("config_data", {})
        
        # åŸºäºæ¨¡å‹åˆ†æä¼˜åŒ–é…ç½®
        optimized = self._optimize_config(model_analysis, user_config)
        
        return {self.output_key: optimized}
    
    def _optimize_config(self, model_analysis: Dict, user_config: Dict) -> Dict:
        """ä¼˜åŒ–é…ç½®å‚æ•°"""
        
        optimized_config = {
            "batch_size": [1, 4, 8],
            "input_len": [512, 1024],
            "output_len": [64, 128],
            "analysis_params": {}
        }
        
        detected_model = model_analysis.get("detected_model", "")
        model_size = model_analysis.get("model_size", "")
        
        # åŸºäºæ¨¡å‹å¤§å°ä¼˜åŒ–batch_size
        if "7B" in model_size or "6B" in model_size:
            optimized_config["batch_size"] = [1, 4, 8, 16]
            optimized_config["recommended_batch"] = 8
        elif "13B" in model_size or "14B" in model_size:
            optimized_config["batch_size"] = [1, 2, 4, 8]
            optimized_config["recommended_batch"] = 4
        elif "70B" in model_size:
            optimized_config["batch_size"] = [1, 2]
            optimized_config["recommended_batch"] = 1
        
        # åŸºäºåˆ†æç±»å‹ä¼˜åŒ–
        analysis_type = model_analysis.get("analysis_type", "auto")
        if analysis_type == "ncu":
            optimized_config["analysis_params"] = {
                "profile_steps": 5,
                "detailed_metrics": True,
                "kernel_filter": "top_10"
            }
        elif analysis_type == "nsys":
            optimized_config["analysis_params"] = {
                "trace_apis": ["cuda", "nvtx", "osrt"],
                "memory_usage": True,
                "timeline_detail": "high"
            }
        
        # åˆå¹¶ç”¨æˆ·é…ç½®
        if user_config:
            for key, value in user_config.items():
                if key in optimized_config:
                    optimized_config[key] = value
        
        return optimized_config
    
    @property
    def _chain_type(self) -> str:
        return "config_optimization"

class ExecutionPlanChain(Chain):
    """æ‰§è¡Œè®¡åˆ’é“¾ - ç”Ÿæˆè¯¦ç»†çš„æ‰§è¡Œæ­¥éª¤"""
    
    input_key: str = "optimized_config"
    output_key: str = "execution_plan"
    
    def _call(
        self,
        inputs: Dict[str, Any],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰§è¡Œè®¡åˆ’"""
        
        optimized_config = inputs[self.input_key]
        model_analysis = inputs.get("model_analysis", {})
        
        # ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
        plan = self._generate_execution_plan(model_analysis, optimized_config)
        
        return {self.output_key: plan}
    
    def _generate_execution_plan(self, model_analysis: Dict, config: Dict) -> Dict:
        """ç”Ÿæˆè¯¦ç»†æ‰§è¡Œè®¡åˆ’"""
        
        analysis_type = model_analysis.get("analysis_type", "auto")
        detected_model = model_analysis.get("detected_model", "unknown")
        
        base_steps = [
            {
                "step": 1,
                "name": "ç¯å¢ƒæ£€æŸ¥",
                "description": "æ£€æŸ¥GPUç¯å¢ƒå’Œå·¥å…·å¯ç”¨æ€§",
                "estimated_time": "30ç§’",
                "command": "nvidia-smi && nsys --version"
            },
            {
                "step": 2,
                "name": "æ¨¡å‹é…ç½®",
                "description": f"é…ç½®{detected_model}æ¨¡å‹å‚æ•°",
                "estimated_time": "1åˆ†é’Ÿ",
                "parameters": config
            }
        ]
        
        # æ ¹æ®åˆ†æç±»å‹æ·»åŠ æ­¥éª¤
        if analysis_type == "nsys":
            base_steps.extend([
                {
                    "step": 3,
                    "name": "NSyså…¨å±€åˆ†æ",
                    "description": "è¿è¡ŒNsight Systemsè¿›è¡Œå…¨å±€æ€§èƒ½åˆ†æ",
                    "estimated_time": "3-5åˆ†é’Ÿ",
                    "command": f"nsys profile -o {detected_model}_profile"
                },
                {
                    "step": 4,
                    "name": "ç»“æœè§£æ",
                    "description": "è§£ænsysè¾“å‡ºå¹¶ç”ŸæˆæŠ¥å‘Š",
                    "estimated_time": "1-2åˆ†é’Ÿ"
                }
            ])
        elif analysis_type == "ncu":
            base_steps.extend([
                {
                    "step": 3,
                    "name": "çƒ­ç‚¹è¯†åˆ«",
                    "description": "å…ˆç”¨nsysè¯†åˆ«çƒ­ç‚¹kernels",
                    "estimated_time": "2-3åˆ†é’Ÿ"
                },
                {
                    "step": 4,
                    "name": "NCUæ·±åº¦åˆ†æ",
                    "description": "ä½¿ç”¨Nsight Computeåˆ†æçƒ­ç‚¹kernels",
                    "estimated_time": "5-10åˆ†é’Ÿ",
                    "command": "ncu --set full -o kernel_analysis"
                },
                {
                    "step": 5,
                    "name": "ç“¶é¢ˆåˆ†æ",
                    "description": "åˆ†ækernelæ€§èƒ½ç“¶é¢ˆ",
                    "estimated_time": "1-2åˆ†é’Ÿ"
                }
            ])
        else:  # auto
            base_steps.extend([
                {
                    "step": 3,
                    "name": "é›†æˆåˆ†æç¬¬ä¸€é˜¶æ®µ",
                    "description": "NSyså…¨å±€åˆ†æè¯†åˆ«çƒ­ç‚¹",
                    "estimated_time": "3-5åˆ†é’Ÿ"
                },
                {
                    "step": 4,
                    "name": "é›†æˆåˆ†æç¬¬äºŒé˜¶æ®µ", 
                    "description": "NCUæ·±åº¦åˆ†æçƒ­ç‚¹kernels",
                    "estimated_time": "5-8åˆ†é’Ÿ"
                },
                {
                    "step": 5,
                    "name": "ç»¼åˆæŠ¥å‘Šç”Ÿæˆ",
                    "description": "ç”Ÿæˆç»¼åˆæ€§èƒ½åˆ†ææŠ¥å‘Š",
                    "estimated_time": "1-2åˆ†é’Ÿ"
                }
            ])
        
        # æ·»åŠ æœ€ç»ˆæ­¥éª¤
        base_steps.append({
            "step": len(base_steps) + 1,
            "name": "æŠ¥å‘Šç”Ÿæˆ",
            "description": "ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å’Œä¼˜åŒ–å»ºè®®",
            "estimated_time": "30ç§’"
        })
        
        total_time = sum([self._parse_time(step.get("estimated_time", "1åˆ†é’Ÿ")) 
                         for step in base_steps])
        
        return {
            "steps": base_steps,
            "total_steps": len(base_steps),
            "estimated_total_time": f"{total_time}åˆ†é’Ÿ",
            "analysis_type": analysis_type,
            "priority": "normal" if total_time <= 10 else "high"
        }
    
    def _parse_time(self, time_str: str) -> float:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²ä¸ºåˆ†é’Ÿ"""
        if "ç§’" in time_str:
            return float(time_str.replace("ç§’", "")) / 60
        elif "åˆ†é’Ÿ" in time_str:
            parts = time_str.replace("åˆ†é’Ÿ", "").split("-")
            if len(parts) == 2:
                return (float(parts[0]) + float(parts[1])) / 2
            else:
                return float(parts[0])
        return 1.0
    
    @property
    def _chain_type(self) -> str:
        return "execution_plan"

class RecommendationChain(Chain):
    """å»ºè®®ç”Ÿæˆé“¾ - åŸºäºåˆ†æè®¡åˆ’ç”Ÿæˆå»ºè®®"""
    
    input_key: str = "execution_plan"
    output_key: str = "recommendations"
    
    def _call(
        self,
        inputs: Dict[str, Any],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå»ºè®®"""
        
        execution_plan = inputs[self.input_key]
        model_analysis = inputs.get("model_analysis", {})
        
        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(model_analysis, execution_plan)
        
        return {self.output_key: recommendations}
    
    def _generate_recommendations(self, model_analysis: Dict, execution_plan: Dict) -> List[str]:
        """ç”Ÿæˆæ™ºèƒ½å»ºè®®"""
        
        recommendations = []
        
        detected_model = model_analysis.get("detected_model", "")
        analysis_type = execution_plan.get("analysis_type", "auto")
        total_time = execution_plan.get("estimated_total_time", "")
        
        # åŸºäºæ¨¡å‹çš„å»ºè®®
        if "7b" in detected_model.lower():
            recommendations.extend([
                "ğŸ¯ 7Bæ¨¡å‹ä¼˜åŒ–å»ºè®®ï¼šæ¨èä½¿ç”¨FP16ç²¾åº¦ä»¥å‡å°‘å†…å­˜å ç”¨",
                "ğŸ“Š batch_sizeå»ºè®®ï¼š8-16ä¸ºæœ€ä½³èŒƒå›´ï¼Œå¹³è¡¡ååé‡å’Œå»¶è¿Ÿ",
                "ğŸš€ åŠ é€Ÿå»ºè®®ï¼šè€ƒè™‘ä½¿ç”¨FlashAttentionä¼˜åŒ–æ³¨æ„åŠ›è®¡ç®—"
            ])
        elif "13b" in detected_model.lower() or "14b" in detected_model.lower():
            recommendations.extend([
                "ğŸ¯ 13B/14Bæ¨¡å‹ä¼˜åŒ–å»ºè®®ï¼šå»ºè®®ä½¿ç”¨Tensorå¹¶è¡Œæé«˜æ€§èƒ½",
                "ğŸ“Š batch_sizeå»ºè®®ï¼š4-8ä¸ºæ¨èèŒƒå›´ï¼Œé¿å…å†…å­˜æº¢å‡º",
                "ğŸ’¾ å†…å­˜ä¼˜åŒ–ï¼šå¯ç”¨gradient checkpointingèŠ‚çœå†…å­˜"
            ])
        
        # åŸºäºåˆ†æç±»å‹çš„å»ºè®®
        if analysis_type == "nsys":
            recommendations.extend([
                "ğŸ“ˆ NSysåˆ†æé‡ç‚¹ï¼šå…³æ³¨kernelæ‰§è¡Œtimelineå’Œå¹¶è¡Œåº¦",
                "ğŸ” ä¼˜åŒ–æ–¹å‘ï¼šè¯†åˆ«kernelé—´çš„ç©ºéš™æ—¶é—´ï¼Œæé«˜GPUåˆ©ç”¨ç‡",
                "ğŸ“Š ç›‘æ§æŒ‡æ ‡ï¼šé‡ç‚¹å…³æ³¨CUDA APIè°ƒç”¨å¼€é”€"
            ])
        elif analysis_type == "ncu":
            recommendations.extend([
                "ğŸ”¬ NCUåˆ†æé‡ç‚¹ï¼šæ·±åº¦åˆ†æSMæ•ˆç‡å’Œå†…å­˜å¸¦å®½",
                "âš¡ ä¼˜åŒ–æ–¹å‘ï¼šæé«˜occupancyå’Œå‡å°‘warpåœé¡¿",
                "ğŸ¯ å…³é”®æŒ‡æ ‡ï¼šSM efficiency, Memory bandwidth, Tensor Coreåˆ©ç”¨ç‡"
            ])
        
        # åŸºäºæ‰§è¡Œæ—¶é—´çš„å»ºè®®
        if "10" in total_time or "15" in total_time:
            recommendations.append("â° åˆ†ææ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®åœ¨éé«˜å³°æ—¶é—´è¿è¡Œ")
        
        # é€šç”¨å»ºè®®
        recommendations.extend([
            "ğŸ’¡ å»ºè®®å…ˆè¿è¡Œå¿«é€Ÿåˆ†æéªŒè¯ç¯å¢ƒé…ç½®",
            "ğŸ“ åˆ†ææœŸé—´ä¿æŒGPUè´Ÿè½½ç¨³å®šï¼Œé¿å…å…¶ä»–ä»»åŠ¡å¹²æ‰°",
            "ğŸ”„ å»ºè®®å¯¹æ¯”å¤šä¸ªbatch_sizeçš„æ€§èƒ½è¡¨ç°",
            "ğŸ“Š å…³æ³¨åˆ†æç»“æœä¸­çš„æ€§èƒ½ç“¶é¢ˆè¯†åˆ«å’Œä¼˜åŒ–å»ºè®®"
        ])
        
        return recommendations
    
    @property
    def _chain_type(self) -> str:
        return "recommendation"

class PerformanceAnalysisWorkflow:
    """æ€§èƒ½åˆ†æå·¥ä½œæµç¨‹ç®¡ç†å™¨"""
    
    def __init__(self):
        # åˆ›å»ºå·¥ä½œæµç¨‹é“¾
        self.model_chain = ModelAnalysisChain()
        self.config_chain = ConfigOptimizationChain()
        self.execution_chain = ExecutionPlanChain()
        self.recommendation_chain = RecommendationChain()
        
        # ç»„åˆæˆé¡ºåºé“¾
        self.workflow_chain = SequentialChain(
            chains=[
                self.model_chain,
                self.config_chain, 
                self.execution_chain,
                self.recommendation_chain
            ],
            input_variables=["user_request", "config_data"],
            output_variables=["model_analysis", "optimized_config", "execution_plan", "recommendations"],
            verbose=True
        )
    
    async def run_workflow(self, user_request: str, config_data: Dict = None) -> Dict:
        """è¿è¡Œå®Œæ•´å·¥ä½œæµç¨‹"""
        
        try:
            inputs = {
                "user_request": user_request,
                "config_data": config_data or {}
            }
            
            # æ‰§è¡Œå·¥ä½œæµç¨‹
            results = self.workflow_chain(inputs)
            
            # æ•´ç†è¾“å‡º
            workflow_output = {
                "status": "success",
                "timestamp": datetime.now().isoformat(),
                "workflow_results": {
                    "model_analysis": results["model_analysis"],
                    "optimized_config": results["optimized_config"],
                    "execution_plan": results["execution_plan"],
                    "recommendations": results["recommendations"]
                },
                "summary": self._generate_summary(results)
            }
            
            return workflow_output
            
        except Exception as e:
            return {
                "status": "error",
                "message": f"å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}",
                "timestamp": datetime.now().isoformat()
            }
    
    def _generate_summary(self, results: Dict) -> Dict:
        """ç”Ÿæˆå·¥ä½œæµç¨‹æ‘˜è¦"""
        
        model_analysis = results.get("model_analysis", {})
        execution_plan = results.get("execution_plan", {})
        recommendations = results.get("recommendations", [])
        
        return {
            "detected_model": model_analysis.get("detected_model", "unknown"),
            "analysis_type": model_analysis.get("analysis_type", "auto"),
            "total_steps": execution_plan.get("total_steps", 0),
            "estimated_time": execution_plan.get("estimated_total_time", "æœªçŸ¥"),
            "recommendations_count": len(recommendations),
            "workflow_complexity": "simple" if execution_plan.get("total_steps", 0) <= 4 else "complex"
        }

# ä½¿ç”¨ç¤ºä¾‹
async def test_workflow():
    """æµ‹è¯•å·¥ä½œæµç¨‹"""
    
    print("ğŸ§ª æµ‹è¯•LangChainå·¥ä½œæµç¨‹...")
    
    workflow = PerformanceAnalysisWorkflow()
    
    test_requests = [
        "åˆ†æ llama-7b æ¨¡å‹ï¼Œä½¿ç”¨ nsys è¿›è¡Œå…¨å±€æ€§èƒ½åˆ†æ",
        "å¯¹ qwen-14b è¿›è¡Œ ncu æ·±åº¦kernelåˆ†æï¼Œbatch_size=4,8", 
        "ç»¼åˆåˆ†æ chatglm-6b çš„æ€§èƒ½ç“¶é¢ˆ"
    ]
    
    for request in test_requests:
        print(f"\nğŸ‘¤ è¯·æ±‚: {request}")
        
        result = await workflow.run_workflow(request)
        
        if result["status"] == "success":
            summary = result["summary"]
            print(f"ğŸ¤– æ£€æµ‹æ¨¡å‹: {summary['detected_model']}")
            print(f"ğŸ“Š åˆ†æç±»å‹: {summary['analysis_type']}")
            print(f"â±ï¸ é¢„ä¼°æ—¶é—´: {summary['estimated_time']}")
            print(f"ğŸ’¡ å»ºè®®æ•°é‡: {summary['recommendations_count']}")
        else:
            print(f"âŒ é”™è¯¯: {result['message']}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_workflow())


