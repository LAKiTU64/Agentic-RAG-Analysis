"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description","Estimated Speedup Type","Estimated Speedup"

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.57",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","10,201",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","26.24",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","26.24",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","6.46",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.75",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","41.41",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,104.97",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","19.59",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full waves across all SMs. Look at Launch Statistics for more details.","",""

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","8.39",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.33",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.76",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","34.13",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.37",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","34.13",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","80.77"

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","872.99",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","19.93",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","26.24",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","21.63",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","44.20",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","3.87",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","35.22",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.35",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","64.78",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.44",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.77",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.44 active warps per scheduler, but only an average of 0.77 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","64.78"

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","21.12",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","21.76",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","32",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.14",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 50.1% of the total average of 21.1 cycles between issuing two instructions.","global","50.1"

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2,023.27",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1,068,288",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2,083.86",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1,100,279",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 0 fused and 221184 non-fused FP32 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its current performance). Check the Source page to identify where this kernel executes FP32 instructions.","global","5.96"

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","1,024",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","31",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","8.19",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","131,072",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.48",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","3.03"

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","8",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","2",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","64",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","100",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","43.88",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","28.09",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (100.0%) and measured achieved occupancy (43.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","56.12"

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4,408.60",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","672,000",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","6,104.97",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1,404,456",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7,067.57",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","873,680",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","6,104.97",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1,404,456",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","5,917.47",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5,617,824",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.50% above the average, while the minimum instance value is 100.00% below the average.","global","7.748"

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 18.04% above the average, while the minimum instance value is 100.00% below the average.","global","10.03"

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 13.50% above the average, while the minimum instance value is 100.00% below the average.","global","7.748"

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 10.04% above the average, while the minimum instance value is 8.00% below the average.","global","6.495"

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.04",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","41,344",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","100",

"0","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","2.60",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.58",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","12,638",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.22",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","DRAM Throughput","%","21.22",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Duration","us","8",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","16.99",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","33.33",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","6,018.66",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","18.98",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full waves across all SMs. Look at Launch Statistics for more details.","",""

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 2% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","PM Sampling","Maximum Buffer Size","Mbyte","2.10",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","PM Sampling","Dropped Samples","sample","0",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","PM Sampling","Maximum Sampling Interval","us","1.50",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","PM Sampling","# Pass Groups","","2",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","PmSampling","","","","PMSamplingData","WRN","Sampling interval is larger than 10% of the workload duration, which likely results in very few collected samples. For better results, use the --pm-sampling-interval option to reduce the sampling interval. Use --pm-sampling-buffer-size to increase the sampling buffer size for the smaller interval, or don't set a fixed buffer size and let the tool adjust it automatically.","",""

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.34",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.74",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Compute Workload Analysis","Issue Slots Busy","%","34.63",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.39",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Compute Workload Analysis","SM Busy","%","34.63",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","80.5"

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Memory Workload Analysis","Memory Throughput","Gbyte/s","705.44",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Memory Workload Analysis","Mem Busy","%","16.07",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Memory Workload Analysis","Max Bandwidth","%","21.22",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Memory Workload Analysis","L1/TEX Hit Rate","%","21.62",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Memory Workload Analysis","L2 Compression Ratio","","0",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Memory Workload Analysis","L2 Hit Rate","%","44.14",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Memory Workload Analysis","Mem Pipes Busy","%","3.75",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Scheduler Statistics","One or More Eligible","%","35.85",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.36",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Scheduler Statistics","No Eligible","%","64.15",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Scheduler Statistics","Active Warps Per Scheduler","warp","7.29",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.79",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 7.29 active warps per scheduler, but only an average of 0.79 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","64.15"

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","20.34",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","20.95",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Warp State Statistics","Avg. Active Threads Per Warp","","32",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","31.14",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 10.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 51.1% of the total average of 20.3 cycles between issuing two instructions.","global","51.05"

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","2,023.27",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Instruction Statistics","Executed Instructions","inst","1,068,288",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","2,083.99",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Instruction Statistics","Issued Instructions","inst","1,100,348",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 0 fused and 221184 non-fused FP32 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its current performance). Check the Source page to identify where this kernel executes FP32 instructions.","global","6.045"

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Block Size","","1,024",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Cluster Size","","0",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Grid Size","","128",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Registers Per Thread","register/thread","31",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","8.19",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","# SMs","SM","132",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Threads","thread","131,072",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Uses Green Context","","0",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Launch Statistics","Waves Per SM","","0.48",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","LaunchStats","","","","LaunchConfiguration","OPT","The grid for this launch is configured to execute only 128 blocks, which is less than the GPU's 132 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","3.03"

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Active Clusters","cluster","0",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Max Cluster Size","block","8",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Overall GPU Occupancy","%","0",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Cluster Occupancy","%","0",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Barriers","block","32",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit SM","block","32",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Registers","block","2",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Shared Mem","block","8",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Block Limit Warps","block","2",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Active Warps per SM","warp","64",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Theoretical Occupancy","%","100",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Occupancy","%","43.53",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","Achieved Active Warps Per SM","warp","27.86",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (100.0%) and measured achieved occupancy (43.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","56.47"

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","4,409",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","831,104",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","6,018.66",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","1,449,292",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","7,193.25",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","1,083,760",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","6,018.66",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","1,449,292",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","5,813.07",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","5,797,168",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 16.73% above the average, while the minimum instance value is 100.00% below the average.","global","9.172"

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 23.24% above the average, while the minimum instance value is 100.00% below the average.","global","12.3"

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 16.73% above the average, while the minimum instance value is 100.00% below the average.","global","9.172"

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much higher number of active cycles than the average number of active cycles. Maximum instance value is 10.94% above the average, while the minimum instance value is 7.73% below the average.","global","5.81"

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Source Counters","Branch Instructions Ratio","%","0.04",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Source Counters","Branch Instructions","inst","41,344",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Source Counters","Branch Efficiency","%","100",

"1","957663","python3.12","127.0.0.1","void act_and_mul_kernel<__half, &silu<float>>(T1 *, const T1 *, int)","1","7","(1024, 1, 1)","(128, 1, 1)","0","9.0","Source Counters","Avg. Divergent Branches","","0",
